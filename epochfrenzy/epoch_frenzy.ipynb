{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.214312600Z",
     "start_time": "2023-06-06T01:26:31.183323500Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'C:\\\\Users\\mmont\\HW2\\\\03.jsonl'\n",
    "TEST_PATH = 'C:\\\\Users\\mmont\\HW2\\\\test.jsonl'\n",
    "VAL_PATH = 'C:\\\\Users\\mmont\\HW2\\\\val.jsonl'\n",
    "MODEL_PATH = 'C:\\\\Users\\mmont\\HW2' #folder with generation.py, model.py, and tokenizer.py\n",
    "TRAINED_SPM_PATH = 'C:\\\\Users\\mmont\\HW2\\\\tokenizer.model' #downloaded from Ed post"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Init**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.254191500Z",
     "start_time": "2023-06-06T01:26:31.201276Z"
    }
   },
   "outputs": [],
   "source": [
    "#Make sure INGESTED_SAMPLE_CNT % MAX_BSZ == 0; assymetric batches break something somewhere\n",
    "#MAX_SEQ_LEN = 2048\n",
    "MAX_SEQ_LEN = 256\n",
    "INGESTED_SAMPLE_CNT = 2500\n",
    "MAX_BSZ = 10\n",
    "MINI_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.255161500Z",
     "start_time": "2023-06-06T01:26:31.218259900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(MODEL_PATH)\n",
    "\n",
    "import time\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from model_train import ModelArgs, Transformer #no cuda, no inference mode, no output projection (all sequences),\\\n",
    "                                               #no init_method=lambda x: x (fairscale default),\\\n",
    "                                               #remove self.cache; not for training,\n",
    "                                               #CONSIDER: fairscale -> torch.nn replacement (not needed; good practice)\n",
    "from tokenizer_zeropad import Tokenizer #override default padding to 0 to shut tok_embed up about indexing\\\n",
    "                                        #Consider eos() as token\n",
    "\n",
    "#to to-do: model checkpoints (torch.save)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.255161500Z",
     "start_time": "2023-06-06T01:26:31.232265200Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.341039100Z",
     "start_time": "2023-06-06T01:26:31.249176700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'list'>\n",
      "Length: 2500\n",
      "First element type: <class 'dict'>\n",
      "Keys: dict_keys(['text', 'meta'])\n"
     ]
    }
   ],
   "source": [
    "def examine_data(data):\n",
    "    '''debugging func'''\n",
    "    print('Input type: {}'.format(type(data)))\n",
    "    print('Length: {}'.format(len(data)))\n",
    "    print('First element type: {}'.format(type(data[0])))\n",
    "    if type(data[0])==dict:\n",
    "        print('Keys: {}'.format(data[0].keys()))\n",
    "    return\n",
    "\n",
    "def make_data_list(filepath:str, maxiter:int): #no much better way; maybe webtext for bonus point\n",
    "    '''ingests JSON into list (with tripwire parameter to prevent computer from crashing)'''\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= maxiter:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "train_data_raw = make_data_list(TRAIN_PATH, INGESTED_SAMPLE_CNT)\n",
    "val_data_raw = make_data_list(VAL_PATH, INGESTED_SAMPLE_CNT/5)\n",
    "examine_data(train_data_raw) #meta might be a useful extra points and/or ablation (Mitchell doesn't reccommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.386355200Z",
     "start_time": "2023-06-06T01:26:31.342035500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'list'>\n",
      "Length: 2500\n",
      "First element type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "def extract_texts(data_list): #list of strings; BAD!\n",
    "    '''gets rid of the metadata'''\n",
    "    return [item['text'] for item in data_list]\n",
    "\n",
    "train_texts = extract_texts(train_data_raw)\n",
    "val_texts = extract_texts(val_data_raw)\n",
    "examine_data(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.395331Z",
     "start_time": "2023-06-06T01:26:31.362384900Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        encoded_text = self.tokenizer.encode(text, bos=False, eos=False)\n",
    "\n",
    "        #Truncate the sequence to max_seq_len if it's too long\n",
    "        if len(encoded_text) > MAX_SEQ_LEN - 1:  #We subtract 2 to account for the BOS and EOS tokens\\\n",
    "                                                 #small modification: -1 instead\n",
    "            encoded_text = encoded_text[:MAX_SEQ_LEN - 1]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text, dtype=torch.long),\n",
    "            'target_ids': torch.tensor(encoded_text + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'target_ids': target_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.404308Z",
     "start_time": "2023-06-06T01:26:31.386355200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.data:\n",
      "Input type: <class 'list'>\n",
      "Length: 2500\n",
      "First element type: <class 'dict'>\n",
      "Keys: dict_keys(['text', 'meta'])\n",
      "Tokenizer: <tokenizer_zeropad.Tokenizer object at 0x000001EB8F322F40>\n",
      "self.data:\n",
      "Input type: <class 'list'>\n",
      "Length: 50\n",
      "First element type: <class 'dict'>\n",
      "Keys: dict_keys(['text', 'meta'])\n",
      "Tokenizer: <tokenizer_zeropad.Tokenizer object at 0x000001EB8F322F40>\n"
     ]
    }
   ],
   "source": [
    "def examine_dataset(dataset):\n",
    "    print('self.data:')\n",
    "    examine_data(dataset.data)\n",
    "    print('Tokenizer: {}'.format(dataset.tokenizer))\n",
    "    return\n",
    "\n",
    "tokenizer = Tokenizer(TRAINED_SPM_PATH) #pretrained spm\n",
    "train_dataset = TextDataset(train_data_raw, tokenizer) #Dataset obj\n",
    "val_dataset = TextDataset(val_data_raw, tokenizer)\n",
    "examine_dataset(train_dataset)\n",
    "examine_dataset(val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.449336700Z",
     "start_time": "2023-06-06T01:26:31.406327600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=localhost\n",
      "env: MASTER_PORT=0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "#torch.distributed.init_process_group(backend='gloo')\n",
    "#fs_init.initialize_model_parallel(1) #1 worker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.806686700Z",
     "start_time": "2023-06-06T01:26:31.423314Z"
    }
   },
   "outputs": [],
   "source": [
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "mini_args = ModelArgs(\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ, #only works for 32; no idea why\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "if MINI_MODEL: #global var (2nd cell)\n",
    "    model = Transformer(mini_args)\n",
    "else:\n",
    "    model = Transformer(model_args)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=model_args.max_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=model_args.max_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = torch.optim.AdamW(model.parameters()) #Ablation idea: \n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  #ignores padding token for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:31.807957800Z",
     "start_time": "2023-06-06T01:26:31.806686700Z"
    }
   },
   "outputs": [],
   "source": [
    "#debugging tools\n",
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print('TENSOR OVERVIEW\\n'\n",
    "          'Type: {}\\n'\n",
    "          'Data Type: {}\\n'\n",
    "          'Shape: {}\\n'\n",
    "          'Number of Dimensions: {}\\n'\n",
    "          'Device: {}\\n'\n",
    "          'Requires Grad: {}\\n'\n",
    "          'Gradient: {}\\n'.format(tensor.type(), tensor.dtype, tensor.shape, tensor.ndim, tensor.device,\\\n",
    "                                  tensor.requires_grad, tensor.grad))\n",
    "    return\n",
    "\n",
    "def flag(msg='unspecified'):\n",
    "    print('FLAG: {}'.format(msg))\n",
    "    return\n",
    "\n",
    "def loop_summary(titles:tuple, tensors:tuple):\n",
    "    for i in range(len(titles)):\n",
    "        flag(titles[i])\n",
    "        examine_tensor(tensors[i])\n",
    "    return\n",
    "\n",
    "def examine_dataloader(dataloader):\n",
    "    '''debugging function'''\n",
    "    print('DATALOADER OVERVIEW\\n'\n",
    "          'Number of Batches: {}\\n'\n",
    "          'Batch Size: {}\\n'\n",
    "          'Shuffle: {}\\n'\n",
    "          'Number of Workers: {}\\n'.format(\n",
    "              len(dataloader), \n",
    "              dataloader.batch_size,\n",
    "              dataloader.num_workers))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T01:26:32.292402700Z",
     "start_time": "2023-06-06T01:26:31.810944600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATALOADER OVERVIEW\n",
      "Number of Batches: 250\n",
      "Batch Size: 10\n",
      "\n",
      "First batch overview:\n",
      "Keys:  dict_keys(['input_ids', 'target_ids'])\n",
      "Shape of input_ids tensor: torch.Size([10, 256])\n",
      "Shape of target_ids tensor: torch.Size([10, 256])\n",
      "\n",
      "Dataset overview:\n",
      "Length of Dataset:  2500\n",
      "Example item:  {'input_ids': tensor([    1,  9041,   558,  5921,   478,  1308,   550, 26049, 22873,  3614,\n",
      "          304,   263,  2532,  1974, 27822,   297,  7660, 29892,   360, 29889,\n",
      "        29907, 29889,   304, 15905,  9279,   363,  4857,  1747,   478, 29909,\n",
      "         9045,  2562, 29892, 17231,  1009,  9554,  7014,   723,  5401,  2304,\n",
      "         1192,   322,  5220,   292,   785,   363,  5314,  4822,   278, 14311,\n",
      "        29889,    13,    13,    13,    13, 29911,  5086,   263,   274,   434,\n",
      "          515,   278, 16417,  1510,  1383,   935,   323,   804, 29892,   278,\n",
      "          478, 29909,   512, 13715,   362, 27819,  8373,  6296,  1438, 19001,\n",
      "        29899, 14056,   558,  1600,  1295,  1434,   263,  9451,   310,   478,\n",
      "        29909, 20251, 29892,  3704,   478, 29909,  7634, 19024,   653,   363,\n",
      "        15202,  4942, 29889,  4699,  1383,   352,  9089, 29892,  1058,  1497,\n",
      "          278,  7306,   310,   278,  1741,   471,   304, 12141, 11104,   393,\n",
      "         1033,   664,   363,   278,  1788,   472,  2919, 29889,    13,    13,\n",
      "           13,    13, 29908,  1349,   852,   310,   502,  1058,   505,   263,\n",
      "         8825,   304,  6493,  1422,   478,  2887,  4822,  1065,   964, 13925,\n",
      "         1058,   505,  1935, 29878,   928,  7014,   322,   526,  1985,   304,\n",
      "         1207, 12080,  2253,   363, 25808,   550, 29892,   541,  1906,  7014,\n",
      "        10331,   304,  7952,  2629,  1438, 23968,   478,  2887,  1699,  1383,\n",
      "          352,  9089,  1497, 29889,   376,  1576,  1900,  2655,   304,   437,\n",
      "          471,   304,  7101,  1438,  1900, 23274,   322, 29892,  2748,   591,\n",
      "         4658,   896,   526,  3907,   263,  4328, 29892,  1207,  1854,   896,\n",
      "          526,  5718,  2705,  7436,  1213,    13,    13,    13,    13, 23036,\n",
      "          540,   471, 10658,   304,   670,  2602,   263,  1629,  8020, 29892,\n",
      "         1383,   352,  9089,   756, 18365,   304, 27391, 23440,  1600,   332,\n",
      "         3527,  2629,   278, 27871, 29889,  5290,  4926,   445,  1629, 29892,\n",
      "          478, 29909,  7841,   263,   716,  7817]), 'target_ids': tensor([ 9041,   558,  5921,   478,  1308,   550, 26049, 22873,  3614,   304,\n",
      "          263,  2532,  1974, 27822,   297,  7660, 29892,   360, 29889, 29907,\n",
      "        29889,   304, 15905,  9279,   363,  4857,  1747,   478, 29909,  9045,\n",
      "         2562, 29892, 17231,  1009,  9554,  7014,   723,  5401,  2304,  1192,\n",
      "          322,  5220,   292,   785,   363,  5314,  4822,   278, 14311, 29889,\n",
      "           13,    13,    13,    13, 29911,  5086,   263,   274,   434,   515,\n",
      "          278, 16417,  1510,  1383,   935,   323,   804, 29892,   278,   478,\n",
      "        29909,   512, 13715,   362, 27819,  8373,  6296,  1438, 19001, 29899,\n",
      "        14056,   558,  1600,  1295,  1434,   263,  9451,   310,   478, 29909,\n",
      "        20251, 29892,  3704,   478, 29909,  7634, 19024,   653,   363, 15202,\n",
      "         4942, 29889,  4699,  1383,   352,  9089, 29892,  1058,  1497,   278,\n",
      "         7306,   310,   278,  1741,   471,   304, 12141, 11104,   393,  1033,\n",
      "          664,   363,   278,  1788,   472,  2919, 29889,    13,    13,    13,\n",
      "           13, 29908,  1349,   852,   310,   502,  1058,   505,   263,  8825,\n",
      "          304,  6493,  1422,   478,  2887,  4822,  1065,   964, 13925,  1058,\n",
      "          505,  1935, 29878,   928,  7014,   322,   526,  1985,   304,  1207,\n",
      "        12080,  2253,   363, 25808,   550, 29892,   541,  1906,  7014, 10331,\n",
      "          304,  7952,  2629,  1438, 23968,   478,  2887,  1699,  1383,   352,\n",
      "         9089,  1497, 29889,   376,  1576,  1900,  2655,   304,   437,   471,\n",
      "          304,  7101,  1438,  1900, 23274,   322, 29892,  2748,   591,  4658,\n",
      "          896,   526,  3907,   263,  4328, 29892,  1207,  1854,   896,   526,\n",
      "         5718,  2705,  7436,  1213,    13,    13,    13,    13, 23036,   540,\n",
      "          471, 10658,   304,   670,  2602,   263,  1629,  8020, 29892,  1383,\n",
      "          352,  9089,   756, 18365,   304, 27391, 23440,  1600,   332,  3527,\n",
      "         2629,   278, 27871, 29889,  5290,  4926,   445,  1629, 29892,   478,\n",
      "        29909,  7841,   263,   716,  7817,     2])}\n",
      "DATALOADER OVERVIEW\n",
      "Number of Batches: 5\n",
      "Batch Size: 10\n",
      "\n",
      "First batch overview:\n",
      "Keys:  dict_keys(['input_ids', 'target_ids'])\n",
      "Shape of input_ids tensor: torch.Size([10, 256])\n",
      "Shape of target_ids tensor: torch.Size([10, 256])\n",
      "\n",
      "Dataset overview:\n",
      "Length of Dataset:  50\n",
      "Example item:  {'input_ids': tensor([    1, 11732,  6405,  8271, 29901, 12129,   335,  2310,   609,  5717,\n",
      "          363,  5969,  2039,   411, 13616,  8042,  3726, 14385, 29871, 29906,\n",
      "        29906,  5846, 29871, 29906, 29900, 29896, 29955,  6376,   630,  7488,\n",
      "         1199, 11732,  6405, 21820, 10021, 29879,    13,    13,  3027,  3509,\n",
      "         1266,   830,   329,   414,  1967,  5777,   683,   376, 10454,   338,\n",
      "          278,   931,   363,  7928,   434,  1699,  1497,  1704,   793, 12129,\n",
      "          335,  2310,   609,    13,    13, 29907,  2075,  6405, 29915, 29879,\n",
      "         2123, 16470, 11822, 29892,  1704,   793, 12129,   335,  2310,   609,\n",
      "        29892,   756,  2000,   363,   716,  5969,  2039,   411, 13616,  1156,\n",
      "         2903,   271,   391, 13973,  2113,   263,  2243,   326, 13638,   297,\n",
      "          263, 14014,  8271, 29889,    13,    13,  3868,  1497,   540,  5131,\n",
      "          278, 27214,   800,   297,  1771,  1558,  1379, 29892,   988,   540,\n",
      "          338,  8471,   297,  1583, 29899,   326,  4752,   429,   488, 29892,\n",
      "          470,  1790, 19007,  4234, 29889,    13,    13,  5592,   475, 29915,\n",
      "        29879, 15512,  7668,  1085,  3328,   390,  7069, 29891,  2678,  7470,\n",
      "          304, 12560,   278,  2969, 29889,    13,    13,  3868,  1497,   540,\n",
      "          723,  4808,  5969,  2039,   411,   278,  2343,   310,   278,   716,\n",
      "        11732,   273,  5874,   541,   393, 11822,   723,   505,   304,  2125,\n",
      "          701,  1009,  1400,   297, 11732,  6405,  3528, 29889,    13,    13,\n",
      "         3868, 28305, 22006,  3237, 12129,   335,  2310,   609, 29892,  4417,\n",
      "          393,   278, 19576,   310,   498,  1295,  3250, 29915, 29879,  8271,\n",
      "          471,   512,   743,   826,  5632,  3922, 29892,   278, 11822,   310,\n",
      "          278, 21353,   466,   575,  6263, 29892,   607, 10753, 11732,  6405,\n",
      "          304,  3933,   263, 12647, 29899,  1300,  4917,   681,   760,   310,\n",
      "        13616, 29889,    13,    13,  1576, 21353,   466,   575,  6263,   338,\n",
      "         1286,   278,  5120, 29915, 29879, 24842]), 'target_ids': tensor([11732,  6405,  8271, 29901, 12129,   335,  2310,   609,  5717,   363,\n",
      "         5969,  2039,   411, 13616,  8042,  3726, 14385, 29871, 29906, 29906,\n",
      "         5846, 29871, 29906, 29900, 29896, 29955,  6376,   630,  7488,  1199,\n",
      "        11732,  6405, 21820, 10021, 29879,    13,    13,  3027,  3509,  1266,\n",
      "          830,   329,   414,  1967,  5777,   683,   376, 10454,   338,   278,\n",
      "          931,   363,  7928,   434,  1699,  1497,  1704,   793, 12129,   335,\n",
      "         2310,   609,    13,    13, 29907,  2075,  6405, 29915, 29879,  2123,\n",
      "        16470, 11822, 29892,  1704,   793, 12129,   335,  2310,   609, 29892,\n",
      "          756,  2000,   363,   716,  5969,  2039,   411, 13616,  1156,  2903,\n",
      "          271,   391, 13973,  2113,   263,  2243,   326, 13638,   297,   263,\n",
      "        14014,  8271, 29889,    13,    13,  3868,  1497,   540,  5131,   278,\n",
      "        27214,   800,   297,  1771,  1558,  1379, 29892,   988,   540,   338,\n",
      "         8471,   297,  1583, 29899,   326,  4752,   429,   488, 29892,   470,\n",
      "         1790, 19007,  4234, 29889,    13,    13,  5592,   475, 29915, 29879,\n",
      "        15512,  7668,  1085,  3328,   390,  7069, 29891,  2678,  7470,   304,\n",
      "        12560,   278,  2969, 29889,    13,    13,  3868,  1497,   540,   723,\n",
      "         4808,  5969,  2039,   411,   278,  2343,   310,   278,   716, 11732,\n",
      "          273,  5874,   541,   393, 11822,   723,   505,   304,  2125,   701,\n",
      "         1009,  1400,   297, 11732,  6405,  3528, 29889,    13,    13,  3868,\n",
      "        28305, 22006,  3237, 12129,   335,  2310,   609, 29892,  4417,   393,\n",
      "          278, 19576,   310,   498,  1295,  3250, 29915, 29879,  8271,   471,\n",
      "          512,   743,   826,  5632,  3922, 29892,   278, 11822,   310,   278,\n",
      "        21353,   466,   575,  6263, 29892,   607, 10753, 11732,  6405,   304,\n",
      "         3933,   263, 12647, 29899,  1300,  4917,   681,   760,   310, 13616,\n",
      "        29889,    13,    13,  1576, 21353,   466,   575,  6263,   338,  1286,\n",
      "          278,  5120, 29915, 29879, 24842,     2])}\n"
     ]
    }
   ],
   "source": [
    "def examine_dataloader(dataloader):\n",
    "    '''debugging function'''\n",
    "    print('DATALOADER OVERVIEW\\n'\n",
    "          'Number of Batches: {}\\n'\n",
    "          'Batch Size: {}\\n'.format(len(dataloader), dataloader.batch_size))\n",
    "\n",
    "    # Examine the first batch in the dataloader\n",
    "    first_batch = next(iter(dataloader))\n",
    "    print('First batch overview:')\n",
    "    print('Keys: ', first_batch.keys())\n",
    "    \n",
    "    for key in first_batch.keys():\n",
    "        print('Shape of {} tensor: {}'.format(key, first_batch[key].shape))\n",
    "    \n",
    "    # Examine the dataset\n",
    "    print('\\nDataset overview:')\n",
    "    print('Length of Dataset: ', len(dataloader.dataset))\n",
    "\n",
    "    # Try getting an item from the dataset\n",
    "    try:\n",
    "        print('Example item: ', dataloader.dataset[0])\n",
    "    except Exception as e:\n",
    "        print('Could not retrieve item from dataset: ', str(e))\n",
    "        \n",
    "    return\n",
    "\n",
    "examine_dataloader(train_dataloader)\n",
    "examine_dataloader(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T02:20:12.605684400Z",
     "start_time": "2023-06-06T01:26:32.291404200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loop 1\n",
      "10.387748718261719\n",
      "10.188249588012695\n",
      "Loop 2\n",
      "10.18424129486084\n",
      "9.693785095214844\n",
      "Loop 3\n",
      "9.71471881866455\n",
      "9.287421035766602\n",
      "Loop 4\n",
      "9.31380558013916\n",
      "8.922862434387207\n",
      "Loop 5\n",
      "9.003082275390625\n",
      "8.608777236938476\n",
      "Loop 6\n",
      "8.740099906921387\n",
      "8.349223709106445\n",
      "Loop 7\n",
      "8.569924354553223\n",
      "8.153986358642578\n",
      "Loop 8\n",
      "8.31257438659668\n",
      "8.008385467529298\n",
      "Loop 9\n",
      "8.079536437988281\n",
      "7.914016628265381\n",
      "Loop 10\n",
      "7.903517723083496\n",
      "7.857735347747803\n",
      "Loop 11\n",
      "7.899935722351074\n",
      "7.837017250061035\n",
      "Loop 12\n",
      "7.9402337074279785\n",
      "7.822018241882324\n",
      "Loop 13\n",
      "7.844095706939697\n",
      "7.815658187866211\n",
      "Loop 14\n",
      "7.902446269989014\n",
      "7.774166488647461\n",
      "Loop 15\n",
      "7.800807952880859\n",
      "7.735138034820556\n",
      "Loop 16\n",
      "7.682217121124268\n",
      "7.76634635925293\n",
      "Loop 17\n",
      "7.945694923400879\n",
      "7.723428344726562\n",
      "Loop 18\n",
      "7.956690788269043\n",
      "7.702315998077393\n",
      "Loop 19\n",
      "7.314047336578369\n",
      "7.716390323638916\n",
      "Loop 20\n",
      "7.783333778381348\n",
      "7.68515043258667\n",
      "Loop 21\n",
      "7.953357696533203\n",
      "7.632419109344482\n",
      "Loop 22\n",
      "7.376928329467773\n",
      "7.639599800109863\n",
      "Loop 23\n",
      "7.5660624504089355\n",
      "7.603056621551514\n",
      "Loop 24\n",
      "7.614349365234375\n",
      "7.589583396911621\n",
      "Loop 25\n",
      "7.873660087585449\n",
      "7.577462863922119\n",
      "Loop 26\n",
      "7.632771968841553\n",
      "7.546137046813965\n",
      "Loop 27\n",
      "7.870786190032959\n",
      "7.584250640869141\n",
      "Loop 28\n",
      "7.75644588470459\n",
      "7.52421236038208\n",
      "Loop 29\n",
      "7.514676570892334\n",
      "7.520423507690429\n",
      "Loop 30\n",
      "7.8371901512146\n",
      "7.528990936279297\n",
      "Loop 31\n",
      "7.735001564025879\n",
      "7.499631309509278\n",
      "Loop 32\n",
      "7.639279365539551\n",
      "7.489838886260986\n",
      "Loop 33\n",
      "7.684872627258301\n",
      "7.457561683654785\n",
      "Loop 34\n",
      "7.418467044830322\n",
      "7.453095436096191\n",
      "Loop 35\n",
      "7.546231269836426\n",
      "7.439811515808105\n",
      "Loop 36\n",
      "7.655471324920654\n",
      "7.423029613494873\n",
      "Loop 37\n",
      "7.076757907867432\n",
      "7.400914001464844\n",
      "Loop 38\n",
      "7.108198642730713\n",
      "7.42478084564209\n",
      "Loop 39\n",
      "7.412491321563721\n",
      "7.401418399810791\n",
      "Loop 40\n",
      "7.411794185638428\n",
      "7.371311187744141\n",
      "Loop 41\n",
      "7.454147815704346\n",
      "7.3819232940673825\n",
      "Loop 42\n",
      "7.567276954650879\n",
      "7.367146396636963\n",
      "Loop 43\n",
      "7.517008304595947\n",
      "7.341305541992187\n",
      "Loop 44\n",
      "7.450911998748779\n",
      "7.3353118896484375\n",
      "Loop 45\n",
      "7.724617004394531\n",
      "7.31302433013916\n",
      "Loop 46\n",
      "7.582661151885986\n",
      "7.314223384857177\n",
      "Loop 47\n",
      "8.023207664489746\n",
      "7.33552360534668\n",
      "Loop 48\n",
      "7.345098495483398\n",
      "7.307844066619873\n",
      "Loop 49\n",
      "7.653456211090088\n",
      "7.309585189819336\n",
      "Loop 50\n",
      "7.351099491119385\n",
      "7.304422283172608\n",
      "Loop 51\n",
      "7.146638870239258\n",
      "7.2820737838745115\n",
      "Loop 52\n",
      "7.241291522979736\n",
      "7.267046070098877\n",
      "Loop 53\n",
      "7.460752010345459\n",
      "7.262349605560303\n",
      "Loop 54\n",
      "7.407205581665039\n",
      "7.248696994781494\n",
      "Loop 55\n",
      "7.753894329071045\n",
      "7.234056377410889\n",
      "Loop 56\n",
      "7.423930644989014\n",
      "7.229628276824951\n",
      "Loop 57\n",
      "7.268893718719482\n",
      "7.224506378173828\n",
      "Loop 58\n",
      "7.361207485198975\n",
      "7.227353858947754\n",
      "Loop 59\n",
      "7.546494007110596\n",
      "7.202724838256836\n",
      "Loop 60\n",
      "7.36464262008667\n",
      "7.199482345581055\n",
      "Loop 61\n",
      "7.183856964111328\n",
      "7.196749114990235\n",
      "Loop 62\n",
      "7.457675933837891\n",
      "7.200509262084961\n",
      "Loop 63\n",
      "7.050553321838379\n",
      "7.178741931915283\n",
      "Loop 64\n",
      "7.109186172485352\n",
      "7.172739505767822\n",
      "Loop 65\n",
      "6.986373424530029\n",
      "7.1694135665893555\n",
      "Loop 66\n",
      "7.723372459411621\n",
      "7.159287166595459\n",
      "Loop 67\n",
      "7.355098247528076\n",
      "7.166206645965576\n",
      "Loop 68\n",
      "7.298780918121338\n",
      "7.153592491149903\n",
      "Loop 69\n",
      "7.239645004272461\n",
      "7.144723224639892\n",
      "Loop 70\n",
      "7.421321868896484\n",
      "7.1390830993652346\n",
      "Loop 71\n",
      "7.514534950256348\n",
      "7.147367477416992\n",
      "Loop 72\n",
      "7.366951942443848\n",
      "7.124099349975586\n",
      "Loop 73\n",
      "6.762233257293701\n",
      "7.129343128204345\n",
      "Loop 74\n",
      "7.292393207550049\n",
      "7.107024765014648\n",
      "Loop 75\n",
      "7.260188579559326\n",
      "7.12556734085083\n",
      "Loop 76\n",
      "7.259960174560547\n",
      "7.108302879333496\n",
      "Loop 77\n",
      "7.33195161819458\n",
      "7.100645446777344\n",
      "Loop 78\n",
      "6.884968280792236\n",
      "7.097314739227295\n",
      "Loop 79\n",
      "7.182342052459717\n",
      "7.08843240737915\n",
      "Loop 80\n",
      "7.487429618835449\n",
      "7.071472549438477\n",
      "Loop 81\n",
      "7.467100620269775\n",
      "7.089085578918457\n",
      "Loop 82\n",
      "7.2719902992248535\n",
      "7.077383804321289\n",
      "Loop 83\n",
      "7.196268558502197\n",
      "7.051894950866699\n",
      "Loop 84\n",
      "7.21038818359375\n",
      "7.051268005371094\n",
      "Loop 85\n",
      "6.765312194824219\n",
      "7.064350032806397\n",
      "Loop 86\n",
      "7.320430278778076\n",
      "7.048154735565186\n",
      "Loop 87\n",
      "7.48861837387085\n",
      "7.041941738128662\n",
      "Loop 88\n",
      "7.324838638305664\n",
      "7.05207052230835\n",
      "Loop 89\n",
      "7.170585632324219\n",
      "7.024982166290283\n",
      "Loop 90\n",
      "7.29534387588501\n",
      "6.987197017669677\n",
      "Loop 91\n",
      "7.042814254760742\n",
      "7.006428813934326\n",
      "Loop 92\n",
      "7.345874786376953\n",
      "6.999651050567627\n",
      "Loop 93\n",
      "7.147005558013916\n",
      "6.978259944915772\n",
      "Loop 94\n",
      "7.118575096130371\n",
      "6.976434421539307\n",
      "Loop 95\n",
      "7.077216148376465\n",
      "6.960698795318604\n",
      "Loop 96\n",
      "7.097134590148926\n",
      "6.951544189453125\n",
      "Loop 97\n",
      "7.165811538696289\n",
      "6.940457725524903\n",
      "Loop 98\n",
      "6.940135478973389\n",
      "6.939610195159912\n",
      "Loop 99\n",
      "7.811671733856201\n",
      "6.939388942718506\n",
      "Loop 100\n",
      "7.016388893127441\n",
      "6.941662693023682\n",
      "Loop 101\n",
      "7.009260654449463\n",
      "6.934841823577881\n",
      "Loop 102\n",
      "7.052393436431885\n",
      "6.913364124298096\n",
      "Loop 103\n",
      "6.846856594085693\n",
      "6.906752395629883\n",
      "Loop 104\n",
      "7.3663811683654785\n",
      "6.912042236328125\n",
      "Loop 105\n",
      "7.080555438995361\n",
      "6.882252979278564\n",
      "Loop 106\n",
      "6.731743335723877\n",
      "6.892785167694091\n",
      "Loop 107\n",
      "6.9650774002075195\n",
      "6.8862457275390625\n",
      "Loop 108\n",
      "7.107893466949463\n",
      "6.882989597320557\n",
      "Loop 109\n",
      "7.1161956787109375\n",
      "6.868627738952637\n",
      "Loop 110\n",
      "6.932437896728516\n",
      "6.863509654998779\n",
      "Loop 111\n",
      "7.0478339195251465\n",
      "6.880196762084961\n",
      "Loop 112\n",
      "7.3589348793029785\n",
      "6.881088638305664\n",
      "Loop 113\n",
      "6.979812145233154\n",
      "6.848692607879639\n",
      "Loop 114\n",
      "6.844578266143799\n",
      "6.851497840881348\n",
      "Loop 115\n",
      "7.453021049499512\n",
      "6.847968101501465\n",
      "Loop 116\n",
      "6.958643436431885\n",
      "6.846203327178955\n",
      "Loop 117\n",
      "6.690760612487793\n",
      "6.839109230041504\n",
      "Loop 118\n",
      "6.611561298370361\n",
      "6.842634963989258\n",
      "Loop 119\n",
      "7.558321952819824\n",
      "6.840779113769531\n",
      "Loop 120\n",
      "6.782777309417725\n",
      "6.82127799987793\n",
      "Loop 121\n",
      "6.920801639556885\n",
      "6.817838478088379\n",
      "Loop 122\n",
      "7.034475326538086\n",
      "6.827977180480957\n",
      "Loop 123\n",
      "7.1675124168396\n",
      "6.825128650665283\n",
      "Loop 124\n",
      "7.262110710144043\n",
      "6.827672672271729\n",
      "Loop 125\n",
      "6.982704162597656\n",
      "6.824989700317383\n",
      "Loop 126\n",
      "6.723402500152588\n",
      "6.797423267364502\n",
      "Loop 127\n",
      "6.6686320304870605\n",
      "6.785497665405273\n",
      "Loop 128\n",
      "6.68791389465332\n",
      "6.781651592254638\n",
      "Loop 129\n",
      "6.889280796051025\n",
      "6.7806275367736815\n",
      "Loop 130\n",
      "6.892923355102539\n",
      "6.773550701141358\n",
      "Loop 131\n",
      "6.855121612548828\n",
      "6.767480850219727\n",
      "Loop 132\n",
      "6.918131351470947\n",
      "6.770956134796142\n",
      "Loop 133\n",
      "7.06488037109375\n",
      "6.75491943359375\n",
      "Loop 134\n",
      "7.066934108734131\n",
      "6.742902851104736\n",
      "Loop 135\n",
      "6.845611095428467\n",
      "6.750117778778076\n",
      "Loop 136\n",
      "7.00763463973999\n",
      "6.7299723625183105\n",
      "Loop 137\n",
      "7.319960594177246\n",
      "6.741606616973877\n",
      "Loop 138\n",
      "6.8795952796936035\n",
      "6.754462337493896\n",
      "Loop 139\n",
      "6.947157382965088\n",
      "6.737066745758057\n",
      "Loop 140\n",
      "7.2752299308776855\n",
      "6.728943347930908\n",
      "Loop 141\n",
      "6.762803554534912\n",
      "6.731685638427734\n",
      "Loop 142\n",
      "6.95855712890625\n",
      "6.718876457214355\n",
      "Loop 143\n",
      "6.8048787117004395\n",
      "6.732712173461914\n",
      "Loop 144\n",
      "6.91322135925293\n",
      "6.72985200881958\n",
      "Loop 145\n",
      "6.849847793579102\n",
      "6.711399841308594\n",
      "Loop 146\n",
      "6.907800197601318\n",
      "6.695877647399902\n",
      "Loop 147\n",
      "6.943079471588135\n",
      "6.700513553619385\n",
      "Loop 148\n",
      "6.78768253326416\n",
      "6.6910382270812985\n",
      "Loop 149\n",
      "6.96674919128418\n",
      "6.675622940063477\n",
      "Loop 150\n",
      "6.910109996795654\n",
      "6.6676959037780765\n",
      "Loop 151\n",
      "6.857237815856934\n",
      "6.66014461517334\n",
      "Loop 152\n",
      "6.6685028076171875\n",
      "6.660742473602295\n",
      "Loop 153\n",
      "6.816871166229248\n",
      "6.659582805633545\n",
      "Loop 154\n",
      "6.745975971221924\n",
      "6.661025333404541\n",
      "Loop 155\n",
      "6.941910266876221\n",
      "6.661404323577881\n",
      "Loop 156\n",
      "6.911056995391846\n",
      "6.654965209960937\n",
      "Loop 157\n",
      "6.980887413024902\n",
      "6.642840576171875\n",
      "Loop 158\n",
      "6.7429022789001465\n",
      "6.649847507476807\n",
      "Loop 159\n",
      "6.698193073272705\n",
      "6.638826656341553\n",
      "Loop 160\n",
      "6.803749084472656\n",
      "6.628417301177978\n",
      "Loop 161\n",
      "6.472748756408691\n",
      "6.63391227722168\n",
      "Loop 162\n",
      "6.780388832092285\n",
      "6.629405212402344\n",
      "Loop 163\n",
      "6.7090630531311035\n",
      "6.604348182678223\n",
      "Loop 164\n",
      "6.441755294799805\n",
      "6.604572391510009\n",
      "Loop 165\n",
      "6.909298896789551\n",
      "6.6021778106689455\n",
      "Loop 166\n",
      "6.8370466232299805\n",
      "6.602830791473389\n",
      "Loop 167\n",
      "6.574759006500244\n",
      "6.593287467956543\n",
      "Loop 168\n",
      "6.501461982727051\n",
      "6.583691215515136\n",
      "Loop 169\n",
      "7.014054775238037\n",
      "6.5695219993591305\n",
      "Loop 170\n",
      "6.7778239250183105\n",
      "6.57300910949707\n",
      "Loop 171\n",
      "6.74894380569458\n",
      "6.5761090278625485\n",
      "Loop 172\n",
      "6.4749650955200195\n",
      "6.56106595993042\n",
      "Loop 173\n",
      "7.270999431610107\n",
      "6.575765132904053\n",
      "Loop 174\n",
      "6.813547134399414\n",
      "6.57826862335205\n",
      "Loop 175\n",
      "6.971388816833496\n",
      "6.5732804298400875\n",
      "Loop 176\n",
      "6.553513526916504\n",
      "6.563718318939209\n",
      "Loop 177\n",
      "6.306122303009033\n",
      "6.638341331481934\n",
      "Loop 178\n",
      "6.9574971199035645\n",
      "6.6545000076293945\n",
      "Loop 179\n",
      "6.487910270690918\n",
      "6.620306301116943\n",
      "Loop 180\n",
      "6.8333587646484375\n",
      "6.589346027374267\n",
      "Loop 181\n",
      "6.714561462402344\n",
      "6.586866188049316\n",
      "Loop 182\n",
      "6.698678493499756\n",
      "6.594580936431885\n",
      "Loop 183\n",
      "6.73006534576416\n",
      "6.584845447540284\n",
      "Loop 184\n",
      "6.814645767211914\n",
      "6.589692878723144\n",
      "Loop 185\n",
      "6.774621486663818\n",
      "6.567687320709228\n",
      "Loop 186\n",
      "6.817592144012451\n",
      "6.572800827026367\n",
      "Loop 187\n",
      "6.934935092926025\n",
      "6.544416713714599\n",
      "Loop 188\n",
      "7.1664652824401855\n",
      "6.540329265594482\n",
      "Loop 189\n",
      "6.81019401550293\n",
      "6.537065219879151\n",
      "Loop 190\n",
      "6.703952312469482\n",
      "6.52507905960083\n",
      "Loop 191\n",
      "6.549755573272705\n",
      "6.52608814239502\n",
      "Loop 192\n",
      "6.866549968719482\n",
      "6.5233917236328125\n",
      "Loop 193\n",
      "6.800349712371826\n",
      "6.506398773193359\n",
      "Loop 194\n",
      "6.726321220397949\n",
      "6.496811389923096\n",
      "Loop 195\n",
      "6.539862632751465\n",
      "6.503420066833496\n",
      "Loop 196\n",
      "6.635403633117676\n",
      "6.495178604125977\n",
      "Loop 197\n",
      "6.801381587982178\n",
      "6.506404304504395\n",
      "Loop 198\n",
      "7.24919319152832\n",
      "6.515662384033203\n",
      "Loop 199\n",
      "7.134087562561035\n",
      "6.544608211517334\n",
      "Loop 200\n",
      "6.646509647369385\n",
      "6.549154663085938\n",
      "Loop 201\n",
      "6.569154262542725\n",
      "6.55446720123291\n",
      "Loop 202\n",
      "6.702385425567627\n",
      "6.53985595703125\n",
      "Loop 203\n",
      "7.0683512687683105\n",
      "6.519607353210449\n",
      "Loop 204\n",
      "6.809769153594971\n",
      "6.508541774749756\n",
      "Loop 205\n",
      "6.956058025360107\n",
      "6.522523498535156\n",
      "Loop 206\n",
      "6.688512325286865\n",
      "6.519727420806885\n",
      "Loop 207\n",
      "7.0926032066345215\n",
      "6.506961250305176\n",
      "Loop 208\n",
      "6.64526891708374\n",
      "6.493081092834473\n",
      "Loop 209\n",
      "6.749946594238281\n",
      "6.489570140838623\n",
      "Loop 210\n",
      "6.8852949142456055\n",
      "6.499421501159668\n",
      "Loop 211\n",
      "6.545717716217041\n",
      "6.50189733505249\n",
      "Loop 212\n",
      "6.86885929107666\n",
      "6.482814598083496\n",
      "Loop 213\n",
      "6.795717716217041\n",
      "6.475592136383057\n",
      "Loop 214\n",
      "6.367510795593262\n",
      "6.4722906112670895\n",
      "Loop 215\n",
      "7.120753288269043\n",
      "6.4763578414917\n",
      "Loop 216\n",
      "7.123012542724609\n",
      "6.475140953063965\n",
      "Loop 217\n",
      "6.699652194976807\n",
      "6.470525074005127\n",
      "Loop 218\n",
      "6.8368940353393555\n",
      "6.457097434997559\n",
      "Loop 219\n",
      "6.488475799560547\n",
      "6.443027591705322\n",
      "Loop 220\n",
      "6.491392612457275\n",
      "6.451721858978272\n",
      "Loop 221\n",
      "6.501124382019043\n",
      "6.447572898864746\n",
      "Loop 222\n",
      "6.403278827667236\n",
      "6.430724239349365\n",
      "Loop 223\n",
      "6.561166286468506\n",
      "6.440858364105225\n",
      "Loop 224\n",
      "6.154508590698242\n",
      "6.426453018188477\n",
      "Loop 225\n",
      "6.703047752380371\n",
      "6.438637924194336\n",
      "Loop 226\n",
      "6.6659746170043945\n",
      "6.415767669677734\n",
      "Loop 227\n",
      "6.97479248046875\n",
      "6.41760368347168\n",
      "Loop 228\n",
      "6.272299289703369\n",
      "6.414847087860108\n",
      "Loop 229\n",
      "6.55290412902832\n",
      "6.40858039855957\n",
      "Loop 230\n",
      "6.745766639709473\n",
      "6.404047298431396\n",
      "Loop 231\n",
      "6.657716274261475\n",
      "6.398991680145263\n",
      "Loop 232\n",
      "6.747437953948975\n",
      "6.398759555816651\n",
      "Loop 233\n",
      "6.506125450134277\n",
      "6.39217357635498\n",
      "Loop 234\n",
      "6.953062534332275\n",
      "6.402131366729736\n",
      "Loop 235\n",
      "6.552299976348877\n",
      "6.42002649307251\n",
      "Loop 236\n",
      "6.374748706817627\n",
      "6.410401344299316\n",
      "Loop 237\n",
      "6.690585136413574\n",
      "6.3885650634765625\n",
      "Loop 238\n",
      "6.6540422439575195\n",
      "6.385654067993164\n",
      "Loop 239\n",
      "6.709710597991943\n",
      "6.380940532684326\n",
      "Loop 240\n",
      "6.098241806030273\n",
      "6.373199939727783\n",
      "Loop 241\n",
      "6.81404972076416\n",
      "6.3851419448852536\n",
      "Loop 242\n",
      "6.5264482498168945\n",
      "6.391466045379639\n",
      "Loop 243\n",
      "6.234744071960449\n",
      "6.391498947143555\n",
      "Loop 244\n",
      "6.3949127197265625\n",
      "6.375881671905518\n",
      "Loop 245\n",
      "6.806403636932373\n",
      "6.372153568267822\n",
      "Loop 246\n",
      "7.00022029876709\n",
      "6.372259330749512\n",
      "Loop 247\n",
      "6.893230438232422\n",
      "6.391413021087646\n",
      "Loop 248\n",
      "6.596784591674805\n",
      "6.414650726318359\n",
      "Loop 249\n",
      "6.6768293380737305\n",
      "6.404372596740723\n",
      "Loop 250\n",
      "6.51732873916626\n",
      "6.393680095672607\n",
      "Average training loss: 7.12\n",
      "Average validation loss: 6.39\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dir = './ckpts'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    data = np.ndarray(shape=(251, 2))\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    model.train()\n",
    "    counter = 0 #debug feature\n",
    "    start = time.time()\n",
    "    for epoch in range(0, int(2500/INGESTED_SAMPLE_CNT)):\n",
    "        total_loss = 0\n",
    "\n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            counter += 1\n",
    "            if counter==0:\n",
    "                break\n",
    "            print('Loop {}'.format(counter))\n",
    "\n",
    "            inputs = batch['input_ids'] #bsz x seq_len\n",
    "\n",
    "            targets = batch['target_ids'] #bsz x seq_len\n",
    "\n",
    "            outputs = model(inputs, start_pos=0) #bsz x sel_len x vocab_size\n",
    "\n",
    "            flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "\n",
    "            flat_targets = targets.view(-1) #(bsz*seq_len)\n",
    "\n",
    "            loss = loss_func(flat_outputs, flat_targets) #flattening confirmed by TA\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            print(loss.item())\n",
    "            data[counter][0] = loss.item()\n",
    "\n",
    "            #this would be out of the loop for per-epoch validation loss\n",
    "            total_val_loss = 0\n",
    "            for batch in val_dataloader:\n",
    "\n",
    "                inputs = batch['input_ids'] #bsz x seq_len\n",
    "\n",
    "                targets = batch['target_ids'] #bsz x seq_len\n",
    "\n",
    "                outputs = model(inputs, start_pos=0) #bsz x sel_len x vocab_size\n",
    "\n",
    "                flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "\n",
    "                flat_targets = targets.view(-1) #(bsz*seq_len)\n",
    "\n",
    "                valloss = loss_func(flat_outputs, flat_targets) #flattening confirmed by TA\n",
    "                total_val_loss += valloss.item()\n",
    "            data[counter][1] = total_val_loss / len(val_dataloader)\n",
    "            print(data[counter][1])\n",
    "\n",
    "\n",
    "\n",
    "        #store data from this epoch\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        # commented out to account for storing data from batches instead\n",
    "        # data[epoch][0] = avg_train_loss\n",
    "        # data[epoch][1] = avg_val_loss\n",
    "        print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"Average validation loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "\n",
    "    # output data to csv file\n",
    "    df = pandas.DataFrame(data)\n",
    "    filepath = Path('out.csv');\n",
    "    df.to_csv(filepath)\n",
    "\n",
    "    # save final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, \"ckpts/model\" + str(INGESTED_SAMPLE_CNT) + \"pts.pth\")\n",
    "\n",
    "    json_params = json.dumps({\n",
    "        \"dim\": model_args.dim,\n",
    "        \"n_layers\": model_args.n_layers,\n",
    "        \"n_heads\": model_args.n_heads,\n",
    "        \"vocab_size\": model_args.vocab_size,\n",
    "        \"multiple_of\": model_args.multiple_of,\n",
    "        \"norm_eps\": model_args.norm_eps,\n",
    "    }, indent=4)\n",
    "\n",
    "    with open(dir + \"/params\" + str(INGESTED_SAMPLE_CNT) + \"pts.json\", \"w\") as outfile:\n",
    "         outfile.write(json_params)\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T02:20:12.619647400Z",
     "start_time": "2023-06-06T02:20:12.606681600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
