{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/'\n",
    "# TEST_PATH = '/home/patrick/Documents/CSE_599/HW/2/testset/test.jsonl'\n",
    "# VAL_PATH = '/home/patrick/Documents/CSE_599/HW/2/valset/val.jsonl'\n",
    "MODEL_PATH = '/Users/anderson/Desktop/Project/LLaMA-From-Inference-to-Training/' #folder with generation.py, model.py, and tokenizer.py\n",
    "TRAINED_SPM_PATH = './tokenizer.model' #downloaded from Ed post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Make sure INGESTED_SAMPLE_CNT % MAX_BSZ == 0; assymetric batches break something somewhere\n",
    "# MAX_SEQ_LEN = 2048\n",
    "MAX_SEQ_LEN = 512\n",
    "INGESTED_SAMPLE_CNT = 8\n",
    "MAX_BSZ = 16\n",
    "MINI_MODEL = False\n",
    "RESUME = True \n",
    "PRETRAINED_PATH = \"./ckpts/best_ckpt/model.pth\"\n",
    "PARAMS_JSON_PATH = \"./ckpts/best_ckpt/params.json\"\n",
    "DATA_PATH = \"/Users/anderson/Downloads/11.jsonl.zst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(MODEL_PATH)\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from generation import LLaMA\n",
    "from llama.model_train import ModelArgs, Transformer #ctrl+f and comment out cuda, all else same\n",
    "#from model import ModelArgs, Transformer #use this one if you have NVIDIA GPU\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextDataset.__init__() missing 1 required positional argument: 'max_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m# train_texts, val_texts, test_texts = train_test_split(\"../data/02_10000_entries.txt\", 25000, 100, 2000)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m train_texts, val_texts, test_texts \u001b[39m=\u001b[39m train_test_split(read_file(DATA_PATH, entries\u001b[39m=\u001b[39m\u001b[39m30000\u001b[39m), \u001b[39m27000\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m train_dataset \u001b[39m=\u001b[39m TextDataset(train_texts, tokenizer)\n\u001b[1;32m     99\u001b[0m val_dataset \u001b[39m=\u001b[39m TextDataset(val_texts, tokenizer)\n\u001b[1;32m    100\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mMAX_BSZ, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n",
      "\u001b[0;31mTypeError\u001b[0m: TextDataset.__init__() missing 1 required positional argument: 'max_seq_len'"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np \n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zstandard as zstd\n",
    "import io\n",
    "\n",
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)\n",
    "\n",
    "def read_line(line):\n",
    "    line = line.strip()\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return data['text']\n",
    "\n",
    "def read_file(file_path, gbs=1.0, entries=10000):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        decompressor = zstd.ZstdDecompressor()\n",
    "        stream_reader = decompressor.stream_reader(file)\n",
    "        stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "\n",
    "        lines = []\n",
    "        for line in stream:\n",
    "            line = read_line(line)\n",
    "            if line is not None:\n",
    "                lines.append(line)\n",
    "            if len(lines) == entries:\n",
    "                break\n",
    "    print(len(lines))\n",
    "    return lines\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len, random_mask=False, n_mask=1):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_mask = random_mask\n",
    "        self.n_mask = n_mask\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text, bos=False, eos=False)\n",
    "        if not self.random_mask: \n",
    "            #Truncate the sequence to max_seq_len if it's too long\n",
    "            if len(encoded_text) > self.max_seq_len - 1:  #We subtract 2 to account for the BOS and EOS tokens\\\n",
    "                                                    #small modification: -1 instead\n",
    "                encoded_text = encoded_text[:self.max_seq_len - 1]           \n",
    "            return {\n",
    "                'input_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text[:-1], dtype=torch.long),\n",
    "                'target_ids': torch.tensor(encoded_text[1:] + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "            }\n",
    "        else: \n",
    "            if len(encoded_text) > self.max_seq_len - 2:\n",
    "                encoded_text = encoded_text[:self.max_seq_len - 2]\n",
    "            return {\n",
    "                'input_ids': torch.tensor([self.tokenizer.bos_id] + self.mask(encoded_text) + [self.tokenizer.eos_id], dtype=torch.long),\n",
    "                'target_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    def mask(self, encoded_text): \n",
    "        idxs = np.random.choice(range(len(encoded_text)), size=self.n_mask)\n",
    "        encoded_text[idxs] = self.tokenizer.pad_id\n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'target_ids': target_ids\n",
    "    }\n",
    "\n",
    "def train_test_split(lines, train_n, valid_n, test_n): \n",
    "    train_texts = []\n",
    "    val_texts = []\n",
    "    test_texts = []\n",
    "    for idx, line in enumerate(lines): \n",
    "        if idx < train_n: \n",
    "            train_texts.append(line)\n",
    "        elif idx < train_n + valid_n: \n",
    "            val_texts.append(line)\n",
    "        elif idx < train_n + valid_n + test_n: \n",
    "            test_texts.append(line) \n",
    "        else: \n",
    "            break \n",
    "    return train_texts, val_texts, test_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=localhost\n",
      "env: MASTER_PORT=0\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo')\n",
    "fs_init.initialize_model_parallel(1) #1 worker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "dict_keys(['model_state_dict', 'optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "mini_args = ModelArgs(\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ, #only works for 32; no idea why\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "if RESUME: \n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(PRETRAINED_PATH, map_location=\"cpu\")\n",
    "    print(checkpoint.keys())\n",
    "    with open(PARAMS_JSON_PATH, \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=MAX_SEQ_LEN, max_batch_size=MAX_BSZ, **params\n",
    "    )\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    model = Transformer(model_args)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "elif MINI_MODEL: #global var (2nd cell)\n",
    "    model = Transformer(mini_args)\n",
    "else:\n",
    "    model = Transformer(model_args)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  # ignores padding token for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, train_dataloader, val_dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        print(f\"Batch {i+1} / {len(train_dataloader)}\")\n",
    "        inputs = batch['input_ids'] #bsz x seq_len \n",
    "        targets = batch['target_ids'] #bsz x seq_len\n",
    "        # examine_tensor(targets)\n",
    "\n",
    "        outputs = model(inputs, start_pos=0) #bsz x sel_len x vocab_size\n",
    "\n",
    "        flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "\n",
    "        flat_targets = targets.view(-1) #(bsz*seq_len)\n",
    "\n",
    "        loss = loss_func(flat_outputs, flat_targets) #might be incorrect to flatten, idk\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        val_loss = 0\n",
    "\n",
    "        for val_batch in val_dataloader: \n",
    "            val_inputs = val_batch['input_ids']\n",
    "            val_targets = val_batch['target_ids']\n",
    "\n",
    "            outputs = model(val_inputs, start_pos=0)\n",
    "            flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "            flat_targets = val_targets.view(-1) #(bsz*seq_len)\n",
    "            val_loss += loss_func(flat_outputs, flat_targets).item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"training loss = {loss.item()}\")\n",
    "        print(f\"validation loss = {val_loss}\")\n",
    "        total_loss += loss.item()\n",
    "        train_history.append(loss.item())\n",
    "        val_history.append(val_loss)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    return train_history, val_history "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.369975090026855\n",
      "validation loss = 10.321795112208315\n",
      "Batch 2 / 157\n",
      "training loss = 10.321008682250977\n",
      "validation loss = 10.24862339622096\n",
      "Batch 3 / 157\n",
      "training loss = 10.282401084899902\n",
      "validation loss = 10.156404696012798\n",
      "Batch 4 / 157\n",
      "training loss = 10.16823959350586\n",
      "validation loss = 10.054638109709087\n",
      "Batch 5 / 157\n",
      "training loss = 10.060935974121094\n",
      "validation loss = 9.95194881840756\n",
      "Batch 6 / 157\n",
      "training loss = 9.9651460647583\n",
      "validation loss = 9.847370448865389\n",
      "Batch 7 / 157\n",
      "training loss = 9.858600616455078\n",
      "validation loss = 9.741851505480314\n",
      "Batch 8 / 157\n",
      "training loss = 9.705316543579102\n",
      "validation loss = 9.63182228489926\n",
      "Batch 9 / 157\n",
      "training loss = 9.566237449645996\n",
      "validation loss = 9.519994033010382\n",
      "Batch 10 / 157\n",
      "training loss = 9.591535568237305\n",
      "validation loss = 9.40741955606561\n",
      "Batch 11 / 157\n",
      "training loss = 9.371626853942871\n",
      "validation loss = 9.297244322927375\n",
      "Batch 12 / 157\n",
      "training loss = 9.272883415222168\n",
      "validation loss = 9.188379086946187\n",
      "Batch 13 / 157\n",
      "training loss = 9.240792274475098\n",
      "validation loss = 9.079821385835347\n",
      "Batch 14 / 157\n",
      "training loss = 8.945048332214355\n",
      "validation loss = 8.97465635600843\n",
      "Batch 15 / 157\n",
      "training loss = 8.949538230895996\n",
      "validation loss = 8.873608890332674\n",
      "Batch 16 / 157\n",
      "training loss = 8.982658386230469\n",
      "validation loss = 8.777450912877134\n",
      "Batch 17 / 157\n",
      "training loss = 8.786388397216797\n",
      "validation loss = 8.682862783733167\n",
      "Batch 18 / 157\n",
      "training loss = 8.723859786987305\n",
      "validation loss = 8.592621200963071\n",
      "Batch 19 / 157\n",
      "training loss = 8.70651912689209\n",
      "validation loss = 8.50806703065571\n",
      "Batch 20 / 157\n",
      "training loss = 8.486959457397461\n",
      "validation loss = 8.430512880024157\n",
      "Batch 21 / 157\n",
      "training loss = 8.32349681854248\n",
      "validation loss = 8.35395336151123\n",
      "Batch 22 / 157\n",
      "training loss = 8.517104148864746\n",
      "validation loss = 8.288212475023771\n",
      "Batch 23 / 157\n",
      "training loss = 8.282194137573242\n",
      "validation loss = 8.225418843721089\n",
      "Batch 24 / 157\n",
      "training loss = 8.44461727142334\n",
      "validation loss = 8.172042620809455\n",
      "Batch 25 / 157\n",
      "training loss = 8.065339088439941\n",
      "validation loss = 8.118596302835565\n",
      "Batch 26 / 157\n",
      "training loss = 8.257554054260254\n",
      "validation loss = 8.06516428997642\n",
      "Batch 27 / 157\n",
      "training loss = 8.216588020324707\n",
      "validation loss = 8.028403583325838\n",
      "Batch 28 / 157\n",
      "training loss = 8.012435913085938\n",
      "validation loss = 7.99357072930587\n",
      "Batch 29 / 157\n",
      "training loss = 7.775503635406494\n",
      "validation loss = 7.959789828250282\n",
      "Batch 30 / 157\n",
      "training loss = 7.787322521209717\n",
      "validation loss = 7.932762698123329\n",
      "Batch 31 / 157\n",
      "training loss = 8.011141777038574\n",
      "validation loss = 7.90283549459357\n",
      "Batch 32 / 157\n",
      "training loss = 7.799652576446533\n",
      "validation loss = 7.881370845593904\n",
      "Batch 33 / 157\n",
      "training loss = 7.731461048126221\n",
      "validation loss = 7.864706767232795\n",
      "Batch 34 / 157\n",
      "training loss = 7.728208541870117\n",
      "validation loss = 7.853802229228773\n",
      "Batch 35 / 157\n",
      "training loss = 7.818306922912598\n",
      "validation loss = 7.8408711583990796\n",
      "Batch 36 / 157\n",
      "training loss = 7.969033241271973\n",
      "validation loss = 7.835560924128482\n",
      "Batch 37 / 157\n",
      "training loss = 7.605493068695068\n",
      "validation loss = 7.830875823372288\n",
      "Batch 38 / 157\n",
      "training loss = 7.767307758331299\n",
      "validation loss = 7.825883363422594\n",
      "Batch 39 / 157\n",
      "training loss = 8.125747680664062\n",
      "validation loss = 7.819779471347206\n",
      "Batch 40 / 157\n",
      "training loss = 7.947385787963867\n",
      "validation loss = 7.813661675704153\n",
      "Batch 41 / 157\n",
      "training loss = 7.892749309539795\n",
      "validation loss = 7.810039972004137\n",
      "Batch 42 / 157\n",
      "training loss = 7.9137797355651855\n",
      "validation loss = 7.7969897671749715\n",
      "Batch 43 / 157\n",
      "training loss = 7.775132656097412\n",
      "validation loss = 7.78985229291414\n",
      "Batch 44 / 157\n",
      "training loss = 7.822071552276611\n",
      "validation loss = 7.781577712611148\n",
      "Batch 45 / 157\n",
      "training loss = 8.020630836486816\n",
      "validation loss = 7.765636544478567\n",
      "Batch 46 / 157\n",
      "training loss = 7.682088851928711\n",
      "validation loss = 7.759590324602629\n",
      "Batch 47 / 157\n",
      "training loss = 7.847867012023926\n",
      "validation loss = 7.763058712607936\n",
      "Batch 48 / 157\n",
      "training loss = 7.861452102661133\n",
      "validation loss = 7.760891086176822\n",
      "Batch 49 / 157\n",
      "training loss = 7.6346516609191895\n",
      "validation loss = 7.7555929234153345\n",
      "Batch 50 / 157\n",
      "training loss = 7.988622188568115\n",
      "validation loss = 7.745901910882247\n",
      "Batch 51 / 157\n",
      "training loss = 8.043867111206055\n",
      "validation loss = 7.730638278158088\n",
      "Batch 52 / 157\n",
      "training loss = 7.7665863037109375\n",
      "validation loss = 7.728237704226845\n",
      "Batch 53 / 157\n",
      "training loss = 7.375557899475098\n",
      "validation loss = 7.722823143005371\n",
      "Batch 54 / 157\n",
      "training loss = 7.621151924133301\n",
      "validation loss = 7.727436843671296\n",
      "Batch 55 / 157\n",
      "training loss = 7.170680999755859\n",
      "validation loss = 7.722227598491468\n",
      "Batch 56 / 157\n",
      "training loss = 7.370917320251465\n",
      "validation loss = 7.727542450553493\n",
      "Batch 57 / 157\n",
      "training loss = 7.9840874671936035\n",
      "validation loss = 7.72191532034623\n",
      "Batch 58 / 157\n",
      "training loss = 7.524554252624512\n",
      "validation loss = 7.709179426494398\n",
      "Batch 59 / 157\n",
      "training loss = 7.703084945678711\n",
      "validation loss = 7.6906970425655965\n",
      "Batch 60 / 157\n",
      "training loss = 7.73431921005249\n",
      "validation loss = 7.68406195389597\n",
      "Batch 61 / 157\n",
      "training loss = 7.647037982940674\n",
      "validation loss = 7.698187150453267\n",
      "Batch 62 / 157\n",
      "training loss = 7.609709739685059\n",
      "validation loss = 7.687677157552619\n",
      "Batch 63 / 157\n",
      "training loss = 7.436312198638916\n",
      "validation loss = 7.674510027232923\n",
      "Batch 64 / 157\n",
      "training loss = 7.584772109985352\n",
      "validation loss = 7.6766559701216845\n",
      "Batch 65 / 157\n",
      "training loss = 7.650679111480713\n",
      "validation loss = 7.6831275789361255\n",
      "Batch 66 / 157\n",
      "training loss = 7.864491939544678\n",
      "validation loss = 7.678132885380795\n",
      "Batch 67 / 157\n",
      "training loss = 7.491179466247559\n",
      "validation loss = 7.672111686907317\n",
      "Batch 68 / 157\n",
      "training loss = 7.457381248474121\n",
      "validation loss = 7.659711109964471\n",
      "Batch 69 / 157\n",
      "training loss = 7.465487957000732\n",
      "validation loss = 7.658360255391974\n",
      "Batch 70 / 157\n",
      "training loss = 7.4789958000183105\n",
      "validation loss = 7.648140204580207\n",
      "Batch 71 / 157\n",
      "training loss = 7.624626159667969\n",
      "validation loss = 7.6533812974628646\n",
      "Batch 72 / 157\n",
      "training loss = 7.551129341125488\n",
      "validation loss = 7.64548921585083\n",
      "Batch 73 / 157\n",
      "training loss = 7.8131561279296875\n",
      "validation loss = 7.632597772698653\n",
      "Batch 74 / 157\n",
      "training loss = 7.765821933746338\n",
      "validation loss = 7.635145438344855\n",
      "Batch 75 / 157\n",
      "training loss = 7.712657451629639\n",
      "validation loss = 7.627598938189055\n",
      "Batch 76 / 157\n",
      "training loss = 7.5503411293029785\n",
      "validation loss = 7.630414260061164\n",
      "Batch 77 / 157\n",
      "training loss = 7.640096187591553\n",
      "validation loss = 7.623320177981728\n",
      "Batch 78 / 157\n",
      "training loss = 7.40049409866333\n",
      "validation loss = 7.613052794807835\n",
      "Batch 79 / 157\n",
      "training loss = 7.999324321746826\n",
      "validation loss = 7.617491320559853\n",
      "Batch 80 / 157\n",
      "training loss = 7.254330158233643\n",
      "validation loss = 7.615316717248214\n",
      "Batch 81 / 157\n",
      "training loss = 7.478865146636963\n",
      "validation loss = 7.615195098676179\n",
      "Batch 82 / 157\n",
      "training loss = 7.28235387802124\n",
      "validation loss = 7.610891593130011\n",
      "Batch 83 / 157\n",
      "training loss = 7.471395015716553\n",
      "validation loss = 7.610035319077341\n",
      "Batch 84 / 157\n",
      "training loss = 7.579594612121582\n",
      "validation loss = 7.607987730126632\n",
      "Batch 85 / 157\n",
      "training loss = 7.443713188171387\n",
      "validation loss = 7.59694608889128\n",
      "Batch 86 / 157\n",
      "training loss = 8.006163597106934\n",
      "validation loss = 7.605857849121094\n",
      "Batch 87 / 157\n",
      "training loss = 7.631025314331055\n",
      "validation loss = 7.605059924878572\n",
      "Batch 88 / 157\n",
      "training loss = 7.918512344360352\n",
      "validation loss = 7.601997425681667\n",
      "Batch 89 / 157\n",
      "training loss = 7.611314296722412\n",
      "validation loss = 7.596637374476383\n",
      "Batch 90 / 157\n",
      "training loss = 7.566559791564941\n",
      "validation loss = 7.594630843714664\n",
      "Batch 91 / 157\n",
      "training loss = 7.631163597106934\n",
      "validation loss = 7.592030475014134\n",
      "Batch 92 / 157\n",
      "training loss = 7.749248027801514\n",
      "validation loss = 7.581593638972232\n",
      "Batch 93 / 157\n",
      "training loss = 7.229161262512207\n",
      "validation loss = 7.5828939488059595\n",
      "Batch 94 / 157\n",
      "training loss = 7.825662612915039\n",
      "validation loss = 7.575166677173815\n",
      "Batch 95 / 157\n",
      "training loss = 7.569953441619873\n",
      "validation loss = 7.573864736055073\n",
      "Batch 96 / 157\n",
      "training loss = 7.508046627044678\n",
      "validation loss = 7.5697376853541325\n",
      "Batch 97 / 157\n",
      "training loss = 7.561665058135986\n",
      "validation loss = 7.55787626065706\n",
      "Batch 98 / 157\n",
      "training loss = 7.408185005187988\n",
      "validation loss = 7.561310592450593\n",
      "Batch 99 / 157\n",
      "training loss = 7.504268169403076\n",
      "validation loss = 7.556940003445274\n",
      "Batch 100 / 157\n",
      "training loss = 7.998524188995361\n",
      "validation loss = 7.551867033305921\n",
      "Batch 101 / 157\n",
      "training loss = 7.319973945617676\n",
      "validation loss = 7.552639133051822\n",
      "Batch 102 / 157\n",
      "training loss = 7.252833843231201\n",
      "validation loss = 7.548433077962775\n",
      "Batch 103 / 157\n",
      "training loss = 7.4745306968688965\n",
      "validation loss = 7.541791464153089\n",
      "Batch 104 / 157\n",
      "training loss = 7.129267692565918\n",
      "validation loss = 7.545245245883339\n",
      "Batch 105 / 157\n",
      "training loss = 7.610878944396973\n",
      "validation loss = 7.53791133981002\n",
      "Batch 106 / 157\n",
      "training loss = 7.5051398277282715\n",
      "validation loss = 7.528092208661531\n",
      "Batch 107 / 157\n",
      "training loss = 7.573655128479004\n",
      "validation loss = 7.529278303447523\n",
      "Batch 108 / 157\n",
      "training loss = 7.625667095184326\n",
      "validation loss = 7.533382340481407\n",
      "Batch 109 / 157\n",
      "training loss = 7.146142482757568\n",
      "validation loss = 7.525652283116391\n",
      "Batch 110 / 157\n",
      "training loss = 7.0916619300842285\n",
      "validation loss = 7.518187246824565\n",
      "Batch 111 / 157\n",
      "training loss = 7.441328525543213\n",
      "validation loss = 7.524010833941008\n",
      "Batch 112 / 157\n",
      "training loss = 7.57261848449707\n",
      "validation loss = 7.520503445675499\n",
      "Batch 113 / 157\n",
      "training loss = 7.532913684844971\n",
      "validation loss = 7.521352391493948\n",
      "Batch 114 / 157\n",
      "training loss = 7.419236660003662\n",
      "validation loss = 7.512770903737922\n",
      "Batch 115 / 157\n",
      "training loss = 7.4140729904174805\n",
      "validation loss = 7.512585087826378\n",
      "Batch 116 / 157\n",
      "training loss = 7.595400333404541\n",
      "validation loss = 7.510210940712376\n",
      "Batch 117 / 157\n",
      "training loss = 7.493828773498535\n",
      "validation loss = 7.504085515674792\n",
      "Batch 118 / 157\n",
      "training loss = 7.330843925476074\n",
      "validation loss = 7.491807209817987\n",
      "Batch 119 / 157\n",
      "training loss = 7.640859603881836\n",
      "validation loss = 7.497130268498471\n",
      "Batch 120 / 157\n",
      "training loss = 7.6786065101623535\n",
      "validation loss = 7.489751389152126\n",
      "Batch 121 / 157\n",
      "training loss = 7.71401309967041\n",
      "validation loss = 7.493652745297081\n",
      "Batch 122 / 157\n",
      "training loss = 7.501253128051758\n",
      "validation loss = 7.4919775159735424\n",
      "Batch 123 / 157\n",
      "training loss = 7.366800308227539\n",
      "validation loss = 7.481160916780171\n",
      "Batch 124 / 157\n",
      "training loss = 7.9776997566223145\n",
      "validation loss = 7.476837082913048\n",
      "Batch 125 / 157\n",
      "training loss = 7.530839920043945\n",
      "validation loss = 7.4774574982492545\n",
      "Batch 126 / 157\n",
      "training loss = 7.530439853668213\n",
      "validation loss = 7.475789948513634\n",
      "Batch 127 / 157\n",
      "training loss = 7.6721343994140625\n",
      "validation loss = 7.476730622743306\n",
      "Batch 128 / 157\n",
      "training loss = 7.680319786071777\n",
      "validation loss = 7.467502794767681\n",
      "Batch 129 / 157\n",
      "training loss = 7.56921911239624\n",
      "validation loss = 7.4631938683359245\n",
      "Batch 130 / 157\n",
      "training loss = 7.26350212097168\n",
      "validation loss = 7.461863241697612\n",
      "Batch 131 / 157\n",
      "training loss = 7.500852108001709\n",
      "validation loss = 7.464423656463623\n",
      "Batch 132 / 157\n",
      "training loss = 7.235800743103027\n",
      "validation loss = 7.456796420247931\n",
      "Batch 133 / 157\n",
      "training loss = 7.457366943359375\n",
      "validation loss = 7.451931476593018\n",
      "Batch 134 / 157\n",
      "training loss = 7.29400110244751\n",
      "validation loss = 7.4456757746244735\n",
      "Batch 135 / 157\n",
      "training loss = 7.379937171936035\n",
      "validation loss = 7.43925646731728\n",
      "Batch 136 / 157\n",
      "training loss = 7.392223834991455\n",
      "validation loss = 7.437779376381322\n",
      "Batch 137 / 157\n",
      "training loss = 7.085019588470459\n",
      "validation loss = 7.434075506109941\n",
      "Batch 138 / 157\n",
      "training loss = 7.331793308258057\n",
      "validation loss = 7.4323963617023665\n",
      "Batch 139 / 157\n",
      "training loss = 7.369039535522461\n",
      "validation loss = 7.4410543943706315\n",
      "Batch 140 / 157\n",
      "training loss = 6.8690667152404785\n",
      "validation loss = 7.432403363679585\n",
      "Batch 141 / 157\n",
      "training loss = 7.252471923828125\n",
      "validation loss = 7.429765098973324\n",
      "Batch 142 / 157\n",
      "training loss = 7.291520595550537\n",
      "validation loss = 7.434394736039011\n",
      "Batch 143 / 157\n",
      "training loss = 7.542909145355225\n",
      "validation loss = 7.441299363186485\n",
      "Batch 144 / 157\n",
      "training loss = 7.683271408081055\n",
      "validation loss = 7.447241833335475\n",
      "Batch 145 / 157\n",
      "training loss = 7.541009902954102\n",
      "validation loss = 7.436443379050807\n",
      "Batch 146 / 157\n",
      "training loss = 7.6700897216796875\n",
      "validation loss = 7.418388517279374\n",
      "Batch 147 / 157\n",
      "training loss = 7.384189128875732\n",
      "validation loss = 7.4128289222717285\n",
      "Batch 148 / 157\n",
      "training loss = 7.151740550994873\n",
      "validation loss = 7.420726650639584\n",
      "Batch 149 / 157\n",
      "training loss = 7.1542463302612305\n",
      "validation loss = 7.424810083288896\n",
      "Batch 150 / 157\n",
      "training loss = 7.7732720375061035\n",
      "validation loss = 7.417982703761051\n",
      "Batch 151 / 157\n",
      "training loss = 7.499515056610107\n",
      "validation loss = 7.408609314968712\n",
      "Batch 152 / 157\n",
      "training loss = 7.626088619232178\n",
      "validation loss = 7.414829279247083\n",
      "Batch 153 / 157\n",
      "training loss = 7.542596817016602\n",
      "validation loss = 7.423737475746556\n",
      "Batch 154 / 157\n",
      "training loss = 7.172590732574463\n",
      "validation loss = 7.41114172182585\n",
      "Batch 155 / 157\n",
      "training loss = 7.515097618103027\n",
      "validation loss = 7.401727902261834\n",
      "Batch 156 / 157\n",
      "training loss = 7.176934719085693\n",
      "validation loss = 7.405023449345639\n",
      "Batch 157 / 157\n",
      "training loss = 7.452294826507568\n",
      "validation loss = 7.4051032066345215\n",
      "Average training loss: 7.85\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.377676963806152\n",
      "validation loss = 10.339271143863076\n",
      "Batch 2 / 157\n",
      "training loss = 10.333518981933594\n",
      "validation loss = 10.253609456514058\n",
      "Batch 3 / 157\n",
      "training loss = 10.26346492767334\n",
      "validation loss = 10.138412726552863\n",
      "Batch 4 / 157\n",
      "training loss = 10.135354995727539\n",
      "validation loss = 10.01791447087338\n",
      "Batch 5 / 157\n",
      "training loss = 10.051759719848633\n",
      "validation loss = 9.901399662620143\n",
      "Batch 6 / 157\n",
      "training loss = 9.941927909851074\n",
      "validation loss = 9.784721776058799\n",
      "Batch 7 / 157\n",
      "training loss = 9.792767524719238\n",
      "validation loss = 9.667581909581235\n",
      "Batch 8 / 157\n",
      "training loss = 9.65036392211914\n",
      "validation loss = 9.550645476893374\n",
      "Batch 9 / 157\n",
      "training loss = 9.543474197387695\n",
      "validation loss = 9.433658850820441\n",
      "Batch 10 / 157\n",
      "training loss = 9.448725700378418\n",
      "validation loss = 9.320491991545024\n",
      "Batch 11 / 157\n",
      "training loss = 9.342602729797363\n",
      "validation loss = 9.208853018911261\n",
      "Batch 12 / 157\n",
      "training loss = 9.196810722351074\n",
      "validation loss = 9.098686419035259\n",
      "Batch 13 / 157\n",
      "training loss = 9.033232688903809\n",
      "validation loss = 8.995840976112767\n",
      "Batch 14 / 157\n",
      "training loss = 8.951972961425781\n",
      "validation loss = 8.895751501384535\n",
      "Batch 15 / 157\n",
      "training loss = 8.926651000976562\n",
      "validation loss = 8.798380299618369\n",
      "Batch 16 / 157\n",
      "training loss = 8.768498420715332\n",
      "validation loss = 8.7069495351691\n",
      "Batch 17 / 157\n",
      "training loss = 8.688140869140625\n",
      "validation loss = 8.617576900281405\n",
      "Batch 18 / 157\n",
      "training loss = 8.651229858398438\n",
      "validation loss = 8.534265718962017\n",
      "Batch 19 / 157\n",
      "training loss = 8.463340759277344\n",
      "validation loss = 8.45400313327187\n",
      "Batch 20 / 157\n",
      "training loss = 8.451775550842285\n",
      "validation loss = 8.38231026498895\n",
      "Batch 21 / 157\n",
      "training loss = 8.547751426696777\n",
      "validation loss = 8.309449697795667\n",
      "Batch 22 / 157\n",
      "training loss = 8.409103393554688\n",
      "validation loss = 8.245098164207057\n",
      "Batch 23 / 157\n",
      "training loss = 8.153380393981934\n",
      "validation loss = 8.184141911958394\n",
      "Batch 24 / 157\n",
      "training loss = 8.324718475341797\n",
      "validation loss = 8.127292281702944\n",
      "Batch 25 / 157\n",
      "training loss = 8.13918399810791\n",
      "validation loss = 8.073278226350483\n",
      "Batch 26 / 157\n",
      "training loss = 7.996491432189941\n",
      "validation loss = 8.023083335474917\n",
      "Batch 27 / 157\n",
      "training loss = 7.974874973297119\n",
      "validation loss = 7.978479008925588\n",
      "Batch 28 / 157\n",
      "training loss = 7.802958965301514\n",
      "validation loss = 7.936306024852552\n",
      "Batch 29 / 157\n",
      "training loss = 7.93056583404541\n",
      "validation loss = 7.89644708131489\n",
      "Batch 30 / 157\n",
      "training loss = 7.806792736053467\n",
      "validation loss = 7.8624928123072575\n",
      "Batch 31 / 157\n",
      "training loss = 7.749220848083496\n",
      "validation loss = 7.827777109648052\n",
      "Batch 32 / 157\n",
      "training loss = 7.941877841949463\n",
      "validation loss = 7.803116698014109\n",
      "Batch 33 / 157\n",
      "training loss = 7.78887939453125\n",
      "validation loss = 7.776374942377994\n",
      "Batch 34 / 157\n",
      "training loss = 8.116619110107422\n",
      "validation loss = 7.754250526428223\n",
      "Batch 35 / 157\n",
      "training loss = 7.722935199737549\n",
      "validation loss = 7.731502482765599\n",
      "Batch 36 / 157\n",
      "training loss = 8.068275451660156\n",
      "validation loss = 7.716227205176103\n",
      "Batch 37 / 157\n",
      "training loss = 7.894804954528809\n",
      "validation loss = 7.696230587206389\n",
      "Batch 38 / 157\n",
      "training loss = 7.737509250640869\n",
      "validation loss = 7.688447952270508\n",
      "Batch 39 / 157\n",
      "training loss = 7.752294540405273\n",
      "validation loss = 7.680612940537302\n",
      "Batch 40 / 157\n",
      "training loss = 7.778726100921631\n",
      "validation loss = 7.668062912790399\n",
      "Batch 41 / 157\n",
      "training loss = 7.622219085693359\n",
      "validation loss = 7.6597372105247095\n",
      "Batch 42 / 157\n",
      "training loss = 7.59844446182251\n",
      "validation loss = 7.6574932148582056\n",
      "Batch 43 / 157\n",
      "training loss = 7.635672569274902\n",
      "validation loss = 7.6517794759649975\n",
      "Batch 44 / 157\n",
      "training loss = 7.559784889221191\n",
      "validation loss = 7.649735701711554\n",
      "Batch 45 / 157\n",
      "training loss = 7.639678001403809\n",
      "validation loss = 7.648217979230378\n",
      "Batch 46 / 157\n",
      "training loss = 7.7695770263671875\n",
      "validation loss = 7.642990664431923\n",
      "Batch 47 / 157\n",
      "training loss = 7.663854598999023\n",
      "validation loss = 7.635175805342825\n",
      "Batch 48 / 157\n",
      "training loss = 7.558372974395752\n",
      "validation loss = 7.63065064580817\n",
      "Batch 49 / 157\n",
      "training loss = 7.7114105224609375\n",
      "validation loss = 7.630570737939132\n",
      "Batch 50 / 157\n",
      "training loss = 7.608600616455078\n",
      "validation loss = 7.630239486694336\n",
      "Batch 51 / 157\n",
      "training loss = 7.696924686431885\n",
      "validation loss = 7.6313314939800065\n",
      "Batch 52 / 157\n",
      "training loss = 7.748081684112549\n",
      "validation loss = 7.628307493109452\n",
      "Batch 53 / 157\n",
      "training loss = 7.54619026184082\n",
      "validation loss = 7.623472163551732\n",
      "Batch 54 / 157\n",
      "training loss = 7.99406623840332\n",
      "validation loss = 7.619691246434262\n",
      "Batch 55 / 157\n",
      "training loss = 7.617521286010742\n",
      "validation loss = 7.617655327445583\n",
      "Batch 56 / 157\n",
      "training loss = 7.728248596191406\n",
      "validation loss = 7.613692409113834\n",
      "Batch 57 / 157\n",
      "training loss = 7.654918193817139\n",
      "validation loss = 7.612671751725046\n",
      "Batch 58 / 157\n",
      "training loss = 7.749746322631836\n",
      "validation loss = 7.609548794595819\n",
      "Batch 59 / 157\n",
      "training loss = 7.625789165496826\n",
      "validation loss = 7.606566027591103\n",
      "Batch 60 / 157\n",
      "training loss = 7.6615471839904785\n",
      "validation loss = 7.60534698084781\n",
      "Batch 61 / 157\n",
      "training loss = 7.494328498840332\n",
      "validation loss = 7.598958944019518\n",
      "Batch 62 / 157\n",
      "training loss = 7.94769287109375\n",
      "validation loss = 7.594361079366584\n",
      "Batch 63 / 157\n",
      "training loss = 7.680638790130615\n",
      "validation loss = 7.593670142324347\n",
      "Batch 64 / 157\n",
      "training loss = 7.667957305908203\n",
      "validation loss = 7.592926226164165\n",
      "Batch 65 / 157\n",
      "training loss = 7.790018081665039\n",
      "validation loss = 7.58907968119571\n",
      "Batch 66 / 157\n",
      "training loss = 7.6640143394470215\n",
      "validation loss = 7.587982052250912\n",
      "Batch 67 / 157\n",
      "training loss = 7.762814521789551\n",
      "validation loss = 7.583542447341116\n",
      "Batch 68 / 157\n",
      "training loss = 7.6654253005981445\n",
      "validation loss = 7.579536688955207\n",
      "Batch 69 / 157\n",
      "training loss = 7.43027400970459\n",
      "validation loss = 7.575229318518388\n",
      "Batch 70 / 157\n",
      "training loss = 7.5830607414245605\n",
      "validation loss = 7.570158356114438\n",
      "Batch 71 / 157\n",
      "training loss = 7.755411624908447\n",
      "validation loss = 7.568175642113936\n",
      "Batch 72 / 157\n",
      "training loss = 7.56812858581543\n",
      "validation loss = 7.568947741859837\n",
      "Batch 73 / 157\n",
      "training loss = 7.601527690887451\n",
      "validation loss = 7.565773562381142\n",
      "Batch 74 / 157\n",
      "training loss = 7.5294904708862305\n",
      "validation loss = 7.563976764678955\n",
      "Batch 75 / 157\n",
      "training loss = 7.417684078216553\n",
      "validation loss = 7.560062533930728\n",
      "Batch 76 / 157\n",
      "training loss = 7.576118469238281\n",
      "validation loss = 7.55649104871248\n",
      "Batch 77 / 157\n",
      "training loss = 7.432689189910889\n",
      "validation loss = 7.555742489664178\n",
      "Batch 78 / 157\n",
      "training loss = 7.409115791320801\n",
      "validation loss = 7.557177292673211\n",
      "Batch 79 / 157\n",
      "training loss = 7.785650253295898\n",
      "validation loss = 7.545346460844341\n",
      "Batch 80 / 157\n",
      "training loss = 7.64362907409668\n",
      "validation loss = 7.546059859426398\n",
      "Batch 81 / 157\n",
      "training loss = 7.564702033996582\n",
      "validation loss = 7.545514433007491\n",
      "Batch 82 / 157\n",
      "training loss = 7.816593647003174\n",
      "validation loss = 7.543117247129741\n",
      "Batch 83 / 157\n",
      "training loss = 7.734664440155029\n",
      "validation loss = 7.534759446194298\n",
      "Batch 84 / 157\n",
      "training loss = 7.610440254211426\n",
      "validation loss = 7.5341236716822575\n",
      "Batch 85 / 157\n",
      "training loss = 7.501098155975342\n",
      "validation loss = 7.531319768805253\n",
      "Batch 86 / 157\n",
      "training loss = 7.266409873962402\n",
      "validation loss = 7.530118088973196\n",
      "Batch 87 / 157\n",
      "training loss = 7.367963790893555\n",
      "validation loss = 7.527546807339317\n",
      "Batch 88 / 157\n",
      "training loss = 7.425387382507324\n",
      "validation loss = 7.526314308768825\n",
      "Batch 89 / 157\n",
      "training loss = 7.3550920486450195\n",
      "validation loss = 7.521802400287829\n",
      "Batch 90 / 157\n",
      "training loss = 7.412355899810791\n",
      "validation loss = 7.512825965881348\n",
      "Batch 91 / 157\n",
      "training loss = 7.548809051513672\n",
      "validation loss = 7.516807656539114\n",
      "Batch 92 / 157\n",
      "training loss = 7.403862476348877\n",
      "validation loss = 7.514885626341167\n",
      "Batch 93 / 157\n",
      "training loss = 7.405393123626709\n",
      "validation loss = 7.508758218664872\n",
      "Batch 94 / 157\n",
      "training loss = 7.470439910888672\n",
      "validation loss = 7.50105094909668\n",
      "Batch 95 / 157\n",
      "training loss = 7.557842254638672\n",
      "validation loss = 7.499957134849147\n",
      "Batch 96 / 157\n",
      "training loss = 7.701119422912598\n",
      "validation loss = 7.491293656198602\n",
      "Batch 97 / 157\n",
      "training loss = 7.95561408996582\n",
      "validation loss = 7.490799176065545\n",
      "Batch 98 / 157\n",
      "training loss = 7.462400436401367\n",
      "validation loss = 7.489837470807527\n",
      "Batch 99 / 157\n",
      "training loss = 7.137660503387451\n",
      "validation loss = 7.484860219453511\n",
      "Batch 100 / 157\n",
      "training loss = 7.591907978057861\n",
      "validation loss = 7.477946532400031\n",
      "Batch 101 / 157\n",
      "training loss = 7.409659385681152\n",
      "validation loss = 7.471933841705322\n",
      "Batch 102 / 157\n",
      "training loss = 7.341800212860107\n",
      "validation loss = 7.471914868605764\n",
      "Batch 103 / 157\n",
      "training loss = 7.581156253814697\n",
      "validation loss = 7.4705431335850765\n",
      "Batch 104 / 157\n",
      "training loss = 7.646749496459961\n",
      "validation loss = 7.467468763652601\n",
      "Batch 105 / 157\n",
      "training loss = 7.40903377532959\n",
      "validation loss = 7.4646606947246354\n",
      "Batch 106 / 157\n",
      "training loss = 7.328895092010498\n",
      "validation loss = 7.456065228110866\n",
      "Batch 107 / 157\n",
      "training loss = 7.620838165283203\n",
      "validation loss = 7.4541402866965845\n",
      "Batch 108 / 157\n",
      "training loss = 7.392006874084473\n",
      "validation loss = 7.453139003954436\n",
      "Batch 109 / 157\n",
      "training loss = 7.4087677001953125\n",
      "validation loss = 7.448895981437282\n",
      "Batch 110 / 157\n",
      "training loss = 7.61431884765625\n",
      "validation loss = 7.4472428121064835\n",
      "Batch 111 / 157\n",
      "training loss = 7.465749740600586\n",
      "validation loss = 7.446056315773411\n",
      "Batch 112 / 157\n",
      "training loss = 7.4833984375\n",
      "validation loss = 7.442245157141435\n",
      "Batch 113 / 157\n",
      "training loss = 7.354715824127197\n",
      "validation loss = 7.44052633486296\n",
      "Batch 114 / 157\n",
      "training loss = 7.524430274963379\n",
      "validation loss = 7.43443489074707\n",
      "Batch 115 / 157\n",
      "training loss = 7.616116523742676\n",
      "validation loss = 7.437706470489502\n",
      "Batch 116 / 157\n",
      "training loss = 7.422488689422607\n",
      "validation loss = 7.429455732044421\n",
      "Batch 117 / 157\n",
      "training loss = 7.459405422210693\n",
      "validation loss = 7.426533724132337\n",
      "Batch 118 / 157\n",
      "training loss = 7.28287935256958\n",
      "validation loss = 7.4189991198088\n",
      "Batch 119 / 157\n",
      "training loss = 7.196332931518555\n",
      "validation loss = 7.418509734304328\n",
      "Batch 120 / 157\n",
      "training loss = 7.460474014282227\n",
      "validation loss = 7.421323224117882\n",
      "Batch 121 / 157\n",
      "training loss = 7.220618724822998\n",
      "validation loss = 7.413218247263055\n",
      "Batch 122 / 157\n",
      "training loss = 7.4477009773254395\n",
      "validation loss = 7.415354979665656\n",
      "Batch 123 / 157\n",
      "training loss = 7.582009315490723\n",
      "validation loss = 7.409775733947754\n",
      "Batch 124 / 157\n",
      "training loss = 7.315029621124268\n",
      "validation loss = 7.406381029831736\n",
      "Batch 125 / 157\n",
      "training loss = 7.266749382019043\n",
      "validation loss = 7.401851879922967\n",
      "Batch 126 / 157\n",
      "training loss = 7.448266506195068\n",
      "validation loss = 7.394998249254729\n",
      "Batch 127 / 157\n",
      "training loss = 7.331844806671143\n",
      "validation loss = 7.3932590986552995\n",
      "Batch 128 / 157\n",
      "training loss = 7.47197961807251\n",
      "validation loss = 7.393894321040103\n",
      "Batch 129 / 157\n",
      "training loss = 7.330915451049805\n",
      "validation loss = 7.391510662279631\n",
      "Batch 130 / 157\n",
      "training loss = 7.56370210647583\n",
      "validation loss = 7.387658395265278\n",
      "Batch 131 / 157\n",
      "training loss = 7.564310073852539\n",
      "validation loss = 7.390053347537392\n",
      "Batch 132 / 157\n",
      "training loss = 7.625573635101318\n",
      "validation loss = 7.38830576444927\n",
      "Batch 133 / 157\n",
      "training loss = 7.317135810852051\n",
      "validation loss = 7.390690878817909\n",
      "Batch 134 / 157\n",
      "training loss = 7.505201816558838\n",
      "validation loss = 7.381464330773604\n",
      "Batch 135 / 157\n",
      "training loss = 7.337029933929443\n",
      "validation loss = 7.376514886554919\n",
      "Batch 136 / 157\n",
      "training loss = 7.432279586791992\n",
      "validation loss = 7.375249787380821\n",
      "Batch 137 / 157\n",
      "training loss = 7.824428081512451\n",
      "validation loss = 7.3785383575841\n",
      "Batch 138 / 157\n",
      "training loss = 7.519459247589111\n",
      "validation loss = 7.377415631946764\n",
      "Batch 139 / 157\n",
      "training loss = 7.518008708953857\n",
      "validation loss = 7.376779631564491\n",
      "Batch 140 / 157\n",
      "training loss = 7.294050216674805\n",
      "validation loss = 7.370948013506438\n",
      "Batch 141 / 157\n",
      "training loss = 7.356541633605957\n",
      "validation loss = 7.3706037872716\n",
      "Batch 142 / 157\n",
      "training loss = 7.423494338989258\n",
      "validation loss = 7.3654074919851205\n",
      "Batch 143 / 157\n",
      "training loss = 7.521023750305176\n",
      "validation loss = 7.363464681725753\n",
      "Batch 144 / 157\n",
      "training loss = 7.242392063140869\n",
      "validation loss = 7.357959345767372\n",
      "Batch 145 / 157\n",
      "training loss = 7.122869968414307\n",
      "validation loss = 7.351876108269942\n",
      "Batch 146 / 157\n",
      "training loss = 7.483264923095703\n",
      "validation loss = 7.350578157525313\n",
      "Batch 147 / 157\n",
      "training loss = 7.424448013305664\n",
      "validation loss = 7.348574111336156\n",
      "Batch 148 / 157\n",
      "training loss = 7.525084018707275\n",
      "validation loss = 7.350315119090833\n",
      "Batch 149 / 157\n",
      "training loss = 7.257084369659424\n",
      "validation loss = 7.347997815985429\n",
      "Batch 150 / 157\n",
      "training loss = 7.21377420425415\n",
      "validation loss = 7.350946752648604\n",
      "Batch 151 / 157\n",
      "training loss = 7.3218255043029785\n",
      "validation loss = 7.344303683230751\n",
      "Batch 152 / 157\n",
      "training loss = 7.483262538909912\n",
      "validation loss = 7.342526184885125\n",
      "Batch 153 / 157\n",
      "training loss = 7.288946628570557\n",
      "validation loss = 7.339458515769557\n",
      "Batch 154 / 157\n",
      "training loss = 7.278122425079346\n",
      "validation loss = 7.336406356409976\n",
      "Batch 155 / 157\n",
      "training loss = 7.528603553771973\n",
      "validation loss = 7.335235194156044\n",
      "Batch 156 / 157\n",
      "training loss = 7.143167972564697\n",
      "validation loss = 7.334395785080759\n",
      "Batch 157 / 157\n",
      "training loss = 7.101071357727051\n",
      "validation loss = 7.334512710571289\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.376087188720703\n",
      "validation loss = 10.32183441362883\n",
      "Batch 2 / 157\n",
      "training loss = 10.32498836517334\n",
      "validation loss = 10.205355543839303\n",
      "Batch 3 / 157\n",
      "training loss = 10.186748504638672\n",
      "validation loss = 10.086957228811164\n",
      "Batch 4 / 157\n",
      "training loss = 10.10671329498291\n",
      "validation loss = 9.968714764243678\n",
      "Batch 5 / 157\n",
      "training loss = 9.96534538269043\n",
      "validation loss = 9.850223340486226\n",
      "Batch 6 / 157\n",
      "training loss = 9.860732078552246\n",
      "validation loss = 9.731945188421951\n",
      "Batch 7 / 157\n",
      "training loss = 9.740708351135254\n",
      "validation loss = 9.615681397287469\n",
      "Batch 8 / 157\n",
      "training loss = 9.69625473022461\n",
      "validation loss = 9.500728004857114\n",
      "Batch 9 / 157\n",
      "training loss = 9.475900650024414\n",
      "validation loss = 9.386777877807617\n",
      "Batch 10 / 157\n",
      "training loss = 9.377470970153809\n",
      "validation loss = 9.277011971724662\n",
      "Batch 11 / 157\n",
      "training loss = 9.304566383361816\n",
      "validation loss = 9.172791681791606\n",
      "Batch 12 / 157\n",
      "training loss = 9.155299186706543\n",
      "validation loss = 9.070677205135947\n",
      "Batch 13 / 157\n",
      "training loss = 9.12257194519043\n",
      "validation loss = 8.970077163294741\n",
      "Batch 14 / 157\n",
      "training loss = 8.968513488769531\n",
      "validation loss = 8.875012849506579\n",
      "Batch 15 / 157\n",
      "training loss = 8.909276962280273\n",
      "validation loss = 8.786074186626234\n",
      "Batch 16 / 157\n",
      "training loss = 8.77728271484375\n",
      "validation loss = 8.696450534619784\n",
      "Batch 17 / 157\n",
      "training loss = 8.64035415649414\n",
      "validation loss = 8.613848886991802\n",
      "Batch 18 / 157\n",
      "training loss = 8.491329193115234\n",
      "validation loss = 8.534018064800062\n",
      "Batch 19 / 157\n",
      "training loss = 8.524747848510742\n",
      "validation loss = 8.458851663689865\n",
      "Batch 20 / 157\n",
      "training loss = 8.4553861618042\n",
      "validation loss = 8.386539910968981\n",
      "Batch 21 / 157\n",
      "training loss = 8.3501615524292\n",
      "validation loss = 8.316240712216025\n",
      "Batch 22 / 157\n",
      "training loss = 8.372647285461426\n",
      "validation loss = 8.254508269460578\n",
      "Batch 23 / 157\n",
      "training loss = 8.415767669677734\n",
      "validation loss = 8.196452692935342\n",
      "Batch 24 / 157\n",
      "training loss = 8.12283706665039\n",
      "validation loss = 8.139568429244193\n",
      "Batch 25 / 157\n",
      "training loss = 8.209993362426758\n",
      "validation loss = 8.084393902828818\n",
      "Batch 26 / 157\n",
      "training loss = 8.107892990112305\n",
      "validation loss = 8.031433983852988\n",
      "Batch 27 / 157\n",
      "training loss = 8.034503936767578\n",
      "validation loss = 7.988293873636346\n",
      "Batch 28 / 157\n",
      "training loss = 7.945931911468506\n",
      "validation loss = 7.941927458110609\n",
      "Batch 29 / 157\n",
      "training loss = 7.9051690101623535\n",
      "validation loss = 7.896847072400544\n",
      "Batch 30 / 157\n",
      "training loss = 7.907670974731445\n",
      "validation loss = 7.859661754808928\n",
      "Batch 31 / 157\n",
      "training loss = 8.031678199768066\n",
      "validation loss = 7.824093567697625\n",
      "Batch 32 / 157\n",
      "training loss = 7.876857280731201\n",
      "validation loss = 7.7942304611206055\n",
      "Batch 33 / 157\n",
      "training loss = 7.613086700439453\n",
      "validation loss = 7.764257154966655\n",
      "Batch 34 / 157\n",
      "training loss = 7.6063408851623535\n",
      "validation loss = 7.7365797695360685\n",
      "Batch 35 / 157\n",
      "training loss = 7.672845363616943\n",
      "validation loss = 7.712848813910234\n",
      "Batch 36 / 157\n",
      "training loss = 7.69055700302124\n",
      "validation loss = 7.693814905066239\n",
      "Batch 37 / 157\n",
      "training loss = 7.469153881072998\n",
      "validation loss = 7.6711247343766065\n",
      "Batch 38 / 157\n",
      "training loss = 7.5936994552612305\n",
      "validation loss = 7.658292117871736\n",
      "Batch 39 / 157\n",
      "training loss = 7.579225540161133\n",
      "validation loss = 7.645104684327778\n",
      "Batch 40 / 157\n",
      "training loss = 8.007344245910645\n",
      "validation loss = 7.635739125703511\n",
      "Batch 41 / 157\n",
      "training loss = 7.625351428985596\n",
      "validation loss = 7.621629589482358\n",
      "Batch 42 / 157\n",
      "training loss = 7.458285331726074\n",
      "validation loss = 7.615323443161814\n",
      "Batch 43 / 157\n",
      "training loss = 7.584034442901611\n",
      "validation loss = 7.6185329085902165\n",
      "Batch 44 / 157\n",
      "training loss = 7.326138973236084\n",
      "validation loss = 7.61164645144814\n",
      "Batch 45 / 157\n",
      "training loss = 7.563249588012695\n",
      "validation loss = 7.60347823092812\n",
      "Batch 46 / 157\n",
      "training loss = 7.513780117034912\n",
      "validation loss = 7.599587816941111\n",
      "Batch 47 / 157\n",
      "training loss = 7.596147060394287\n",
      "validation loss = 7.59969415162739\n",
      "Batch 48 / 157\n",
      "training loss = 7.722095489501953\n",
      "validation loss = 7.599746001394172\n",
      "Batch 49 / 157\n",
      "training loss = 7.630617141723633\n",
      "validation loss = 7.593801849766781\n",
      "Batch 50 / 157\n",
      "training loss = 7.523577690124512\n",
      "validation loss = 7.590799908888967\n",
      "Batch 51 / 157\n",
      "training loss = 7.677237033843994\n",
      "validation loss = 7.580559630143015\n",
      "Batch 52 / 157\n",
      "training loss = 7.593550205230713\n",
      "validation loss = 7.584023726613898\n",
      "Batch 53 / 157\n",
      "training loss = 7.6833696365356445\n",
      "validation loss = 7.577682746084113\n",
      "Batch 54 / 157\n",
      "training loss = 7.710165023803711\n",
      "validation loss = 7.5789799439279655\n",
      "Batch 55 / 157\n",
      "training loss = 7.61366605758667\n",
      "validation loss = 7.582048315750925\n",
      "Batch 56 / 157\n",
      "training loss = 7.371490955352783\n",
      "validation loss = 7.572794964438991\n",
      "Batch 57 / 157\n",
      "training loss = 8.060718536376953\n",
      "validation loss = 7.565309173182437\n",
      "Batch 58 / 157\n",
      "training loss = 7.382537364959717\n",
      "validation loss = 7.558824162734182\n",
      "Batch 59 / 157\n",
      "training loss = 7.720184326171875\n",
      "validation loss = 7.566178346935072\n",
      "Batch 60 / 157\n",
      "training loss = 7.520674228668213\n",
      "validation loss = 7.558797936690481\n",
      "Batch 61 / 157\n",
      "training loss = 7.430382251739502\n",
      "validation loss = 7.548623737535979\n",
      "Batch 62 / 157\n",
      "training loss = 7.422152996063232\n",
      "validation loss = 7.546010745199103\n",
      "Batch 63 / 157\n",
      "training loss = 7.547049522399902\n",
      "validation loss = 7.545572155400326\n",
      "Batch 64 / 157\n",
      "training loss = 7.438622951507568\n",
      "validation loss = 7.538011952450401\n",
      "Batch 65 / 157\n",
      "training loss = 7.78001070022583\n",
      "validation loss = 7.5319311744288395\n",
      "Batch 66 / 157\n",
      "training loss = 7.454837799072266\n",
      "validation loss = 7.523656493739078\n",
      "Batch 67 / 157\n",
      "training loss = 7.295839309692383\n",
      "validation loss = 7.520594872926411\n",
      "Batch 68 / 157\n",
      "training loss = 7.555551052093506\n",
      "validation loss = 7.521674833799663\n",
      "Batch 69 / 157\n",
      "training loss = 7.6399688720703125\n",
      "validation loss = 7.520595575633802\n",
      "Batch 70 / 157\n",
      "training loss = 7.423408508300781\n",
      "validation loss = 7.513940008063066\n",
      "Batch 71 / 157\n",
      "training loss = 7.364161491394043\n",
      "validation loss = 7.507073829048558\n",
      "Batch 72 / 157\n",
      "training loss = 7.148956298828125\n",
      "validation loss = 7.498990109092311\n",
      "Batch 73 / 157\n",
      "training loss = 7.439763069152832\n",
      "validation loss = 7.498961247895894\n",
      "Batch 74 / 157\n",
      "training loss = 7.56997537612915\n",
      "validation loss = 7.50025294956408\n",
      "Batch 75 / 157\n",
      "training loss = 7.456648349761963\n",
      "validation loss = 7.491520806362755\n",
      "Batch 76 / 157\n",
      "training loss = 7.436443328857422\n",
      "validation loss = 7.482787508713572\n",
      "Batch 77 / 157\n",
      "training loss = 7.3417229652404785\n",
      "validation loss = 7.479010029842979\n",
      "Batch 78 / 157\n",
      "training loss = 7.604439735412598\n",
      "validation loss = 7.475528014333625\n",
      "Batch 79 / 157\n",
      "training loss = 7.376571178436279\n",
      "validation loss = 7.473298424168637\n",
      "Batch 80 / 157\n",
      "training loss = 7.373630046844482\n",
      "validation loss = 7.472416752263119\n",
      "Batch 81 / 157\n",
      "training loss = 7.441383361816406\n",
      "validation loss = 7.462729278363679\n",
      "Batch 82 / 157\n",
      "training loss = 7.401480674743652\n",
      "validation loss = 7.45756553348742\n",
      "Batch 83 / 157\n",
      "training loss = 7.457859992980957\n",
      "validation loss = 7.45611205853914\n",
      "Batch 84 / 157\n",
      "training loss = 7.516932010650635\n",
      "validation loss = 7.447811979996531\n",
      "Batch 85 / 157\n",
      "training loss = 7.472301959991455\n",
      "validation loss = 7.451176241824501\n",
      "Batch 86 / 157\n",
      "training loss = 7.429007530212402\n",
      "validation loss = 7.447883856923957\n",
      "Batch 87 / 157\n",
      "training loss = 7.480190277099609\n",
      "validation loss = 7.439869529322574\n",
      "Batch 88 / 157\n",
      "training loss = 7.440059661865234\n",
      "validation loss = 7.436779072410182\n",
      "Batch 89 / 157\n",
      "training loss = 7.4935712814331055\n",
      "validation loss = 7.434155514365749\n",
      "Batch 90 / 157\n",
      "training loss = 7.337493419647217\n",
      "validation loss = 7.427010912644236\n",
      "Batch 91 / 157\n",
      "training loss = 7.442537784576416\n",
      "validation loss = 7.424589081814415\n",
      "Batch 92 / 157\n",
      "training loss = 7.347375392913818\n",
      "validation loss = 7.420774133581864\n",
      "Batch 93 / 157\n",
      "training loss = 7.478877067565918\n",
      "validation loss = 7.421639969474391\n",
      "Batch 94 / 157\n",
      "training loss = 7.645193576812744\n",
      "validation loss = 7.416545365986071\n",
      "Batch 95 / 157\n",
      "training loss = 7.293847560882568\n",
      "validation loss = 7.426916498886912\n",
      "Batch 96 / 157\n",
      "training loss = 7.432602882385254\n",
      "validation loss = 7.406616135647423\n",
      "Batch 97 / 157\n",
      "training loss = 7.555531978607178\n",
      "validation loss = 7.41009185188695\n",
      "Batch 98 / 157\n",
      "training loss = 7.432723522186279\n",
      "validation loss = 7.413837282281173\n",
      "Batch 99 / 157\n",
      "training loss = 7.464656829833984\n",
      "validation loss = 7.403923787568745\n",
      "Batch 100 / 157\n",
      "training loss = 7.332507133483887\n",
      "validation loss = 7.395384236385948\n",
      "Batch 101 / 157\n",
      "training loss = 7.267879962921143\n",
      "validation loss = 7.401338752947356\n",
      "Batch 102 / 157\n",
      "training loss = 7.442343711853027\n",
      "validation loss = 7.396298810055382\n",
      "Batch 103 / 157\n",
      "training loss = 7.419236183166504\n",
      "validation loss = 7.389558867404335\n",
      "Batch 104 / 157\n",
      "training loss = 7.279178142547607\n",
      "validation loss = 7.389561427266974\n",
      "Batch 105 / 157\n",
      "training loss = 7.681430339813232\n",
      "validation loss = 7.391978314048366\n",
      "Batch 106 / 157\n",
      "training loss = 7.505394458770752\n",
      "validation loss = 7.3870312289187785\n",
      "Batch 107 / 157\n",
      "training loss = 7.437722682952881\n",
      "validation loss = 7.376297850357859\n",
      "Batch 108 / 157\n",
      "training loss = 7.556173801422119\n",
      "validation loss = 7.377539559414513\n",
      "Batch 109 / 157\n",
      "training loss = 7.350249767303467\n",
      "validation loss = 7.3777741632963485\n",
      "Batch 110 / 157\n",
      "training loss = 7.460718154907227\n",
      "validation loss = 7.376143204538446\n",
      "Batch 111 / 157\n",
      "training loss = 7.400822162628174\n",
      "validation loss = 7.372949324156108\n",
      "Batch 112 / 157\n",
      "training loss = 7.3831024169921875\n",
      "validation loss = 7.36902369950947\n",
      "Batch 113 / 157\n",
      "training loss = 7.335779666900635\n",
      "validation loss = 7.367207075420179\n",
      "Batch 114 / 157\n",
      "training loss = 7.57224178314209\n",
      "validation loss = 7.374021178797672\n",
      "Batch 115 / 157\n",
      "training loss = 7.354507923126221\n",
      "validation loss = 7.360290025409899\n",
      "Batch 116 / 157\n",
      "training loss = 7.205226421356201\n",
      "validation loss = 7.357451765160811\n",
      "Batch 117 / 157\n",
      "training loss = 7.386031150817871\n",
      "validation loss = 7.352082503469367\n",
      "Batch 118 / 157\n",
      "training loss = 7.452051639556885\n",
      "validation loss = 7.351914857563219\n",
      "Batch 119 / 157\n",
      "training loss = 7.34788703918457\n",
      "validation loss = 7.35850002891139\n",
      "Batch 120 / 157\n",
      "training loss = 7.23110294342041\n",
      "validation loss = 7.34623364398354\n",
      "Batch 121 / 157\n",
      "training loss = 7.485735893249512\n",
      "validation loss = 7.342410238165605\n",
      "Batch 122 / 157\n",
      "training loss = 7.169991970062256\n",
      "validation loss = 7.343545813309519\n",
      "Batch 123 / 157\n",
      "training loss = 7.240692615509033\n",
      "validation loss = 7.343371466586464\n",
      "Batch 124 / 157\n",
      "training loss = 7.3855204582214355\n",
      "validation loss = 7.3455915451049805\n",
      "Batch 125 / 157\n",
      "training loss = 7.1561174392700195\n",
      "validation loss = 7.3363416320399235\n",
      "Batch 126 / 157\n",
      "training loss = 7.240075588226318\n",
      "validation loss = 7.333710017957185\n",
      "Batch 127 / 157\n",
      "training loss = 7.206641674041748\n",
      "validation loss = 7.328170324626722\n",
      "Batch 128 / 157\n",
      "training loss = 7.051158428192139\n",
      "validation loss = 7.332993607772024\n",
      "Batch 129 / 157\n",
      "training loss = 7.212673187255859\n",
      "validation loss = 7.327559697000604\n",
      "Batch 130 / 157\n",
      "training loss = 7.474562644958496\n",
      "validation loss = 7.324685448094418\n",
      "Batch 131 / 157\n",
      "training loss = 7.083176612854004\n",
      "validation loss = 7.324126343978079\n",
      "Batch 132 / 157\n",
      "training loss = 7.245135307312012\n",
      "validation loss = 7.313787309746993\n",
      "Batch 133 / 157\n",
      "training loss = 7.450727939605713\n",
      "validation loss = 7.310400912636204\n",
      "Batch 134 / 157\n",
      "training loss = 7.452436923980713\n",
      "validation loss = 7.312520102450722\n",
      "Batch 135 / 157\n",
      "training loss = 7.338846206665039\n",
      "validation loss = 7.3105488074453255\n",
      "Batch 136 / 157\n",
      "training loss = 7.132100582122803\n",
      "validation loss = 7.313763091438695\n",
      "Batch 137 / 157\n",
      "training loss = 7.157715320587158\n",
      "validation loss = 7.308112897370991\n",
      "Batch 138 / 157\n",
      "training loss = 7.820517539978027\n",
      "validation loss = 7.303754831615247\n",
      "Batch 139 / 157\n",
      "training loss = 7.1033782958984375\n",
      "validation loss = 7.302440266860159\n",
      "Batch 140 / 157\n",
      "training loss = 7.239737033843994\n",
      "validation loss = 7.297473355343468\n",
      "Batch 141 / 157\n",
      "training loss = 7.325130939483643\n",
      "validation loss = 7.288695912612112\n",
      "Batch 142 / 157\n",
      "training loss = 7.261281490325928\n",
      "validation loss = 7.295866012573242\n",
      "Batch 143 / 157\n",
      "training loss = 7.209534645080566\n",
      "validation loss = 7.2918367636831185\n",
      "Batch 144 / 157\n",
      "training loss = 7.330522537231445\n",
      "validation loss = 7.288939375626414\n",
      "Batch 145 / 157\n",
      "training loss = 7.309912204742432\n",
      "validation loss = 7.283420889001143\n",
      "Batch 146 / 157\n",
      "training loss = 7.081113815307617\n",
      "validation loss = 7.273817840375398\n",
      "Batch 147 / 157\n",
      "training loss = 7.147993564605713\n",
      "validation loss = 7.279988615136397\n",
      "Batch 148 / 157\n",
      "training loss = 7.03870153427124\n",
      "validation loss = 7.274222399059095\n",
      "Batch 149 / 157\n",
      "training loss = 7.272015571594238\n",
      "validation loss = 7.27511087216829\n",
      "Batch 150 / 157\n",
      "training loss = 7.186925411224365\n",
      "validation loss = 7.268329745844791\n",
      "Batch 151 / 157\n",
      "training loss = 7.343554496765137\n",
      "validation loss = 7.265213414242393\n",
      "Batch 152 / 157\n",
      "training loss = 7.030423641204834\n",
      "validation loss = 7.263649313073409\n",
      "Batch 153 / 157\n",
      "training loss = 7.101454257965088\n",
      "validation loss = 7.254787520358437\n",
      "Batch 154 / 157\n",
      "training loss = 7.346038818359375\n",
      "validation loss = 7.25853769402755\n",
      "Batch 155 / 157\n",
      "training loss = 7.314998626708984\n",
      "validation loss = 7.2548421056647046\n",
      "Batch 156 / 157\n",
      "training loss = 7.099432945251465\n",
      "validation loss = 7.252624009784899\n",
      "Batch 157 / 157\n",
      "training loss = 7.26719856262207\n",
      "validation loss = 7.252162607092607\n",
      "Average training loss: 7.72\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.377527236938477\n",
      "validation loss = 10.33385934327778\n",
      "Batch 2 / 157\n",
      "training loss = 10.328337669372559\n",
      "validation loss = 10.27057823381926\n",
      "Batch 3 / 157\n",
      "training loss = 10.252399444580078\n",
      "validation loss = 10.184149189999228\n",
      "Batch 4 / 157\n",
      "training loss = 10.207708358764648\n",
      "validation loss = 10.08091966729415\n",
      "Batch 5 / 157\n",
      "training loss = 10.060262680053711\n",
      "validation loss = 9.972756084642912\n",
      "Batch 6 / 157\n",
      "training loss = 10.01500415802002\n",
      "validation loss = 9.865771845767373\n",
      "Batch 7 / 157\n",
      "training loss = 9.849397659301758\n",
      "validation loss = 9.756858926070365\n",
      "Batch 8 / 157\n",
      "training loss = 9.720763206481934\n",
      "validation loss = 9.646698901527806\n",
      "Batch 9 / 157\n",
      "training loss = 9.585870742797852\n",
      "validation loss = 9.535286200673957\n",
      "Batch 10 / 157\n",
      "training loss = 9.58986759185791\n",
      "validation loss = 9.424663995441637\n",
      "Batch 11 / 157\n",
      "training loss = 9.402915000915527\n",
      "validation loss = 9.31298521945351\n",
      "Batch 12 / 157\n",
      "training loss = 9.197172164916992\n",
      "validation loss = 9.201448490745143\n",
      "Batch 13 / 157\n",
      "training loss = 9.214202880859375\n",
      "validation loss = 9.092787341067666\n",
      "Batch 14 / 157\n",
      "training loss = 9.0289945602417\n",
      "validation loss = 8.987212532445005\n",
      "Batch 15 / 157\n",
      "training loss = 8.922900199890137\n",
      "validation loss = 8.885116577148438\n",
      "Batch 16 / 157\n",
      "training loss = 9.037330627441406\n",
      "validation loss = 8.784699189035516\n",
      "Batch 17 / 157\n",
      "training loss = 8.812137603759766\n",
      "validation loss = 8.690891767802992\n",
      "Batch 18 / 157\n",
      "training loss = 8.897552490234375\n",
      "validation loss = 8.597037164788498\n",
      "Batch 19 / 157\n",
      "training loss = 8.655214309692383\n",
      "validation loss = 8.511344006187038\n",
      "Batch 20 / 157\n",
      "training loss = 8.45230484008789\n",
      "validation loss = 8.430303172061318\n",
      "Batch 21 / 157\n",
      "training loss = 8.368329048156738\n",
      "validation loss = 8.352298937345806\n",
      "Batch 22 / 157\n",
      "training loss = 8.305282592773438\n",
      "validation loss = 8.282645074944748\n",
      "Batch 23 / 157\n",
      "training loss = 8.475767135620117\n",
      "validation loss = 8.215092558609811\n",
      "Batch 24 / 157\n",
      "training loss = 8.349318504333496\n",
      "validation loss = 8.154820216329474\n",
      "Batch 25 / 157\n",
      "training loss = 8.365703582763672\n",
      "validation loss = 8.102235994840923\n",
      "Batch 26 / 157\n",
      "training loss = 8.328672409057617\n",
      "validation loss = 8.053125657533345\n",
      "Batch 27 / 157\n",
      "training loss = 8.10405445098877\n",
      "validation loss = 8.01064523897673\n",
      "Batch 28 / 157\n",
      "training loss = 8.228574752807617\n",
      "validation loss = 7.972932790455065\n",
      "Batch 29 / 157\n",
      "training loss = 8.068296432495117\n",
      "validation loss = 7.936193014446058\n",
      "Batch 30 / 157\n",
      "training loss = 7.682068347930908\n",
      "validation loss = 7.905497977608128\n",
      "Batch 31 / 157\n",
      "training loss = 7.678639888763428\n",
      "validation loss = 7.879639349485698\n",
      "Batch 32 / 157\n",
      "training loss = 7.868688106536865\n",
      "validation loss = 7.860107421875\n",
      "Batch 33 / 157\n",
      "training loss = 7.804594039916992\n",
      "validation loss = 7.840949936916954\n",
      "Batch 34 / 157\n",
      "training loss = 8.003546714782715\n",
      "validation loss = 7.826255999113384\n",
      "Batch 35 / 157\n",
      "training loss = 7.8187665939331055\n",
      "validation loss = 7.814361747942473\n",
      "Batch 36 / 157\n",
      "training loss = 7.730134963989258\n",
      "validation loss = 7.807056627775493\n",
      "Batch 37 / 157\n",
      "training loss = 7.2878570556640625\n",
      "validation loss = 7.7998278015538265\n",
      "Batch 38 / 157\n",
      "training loss = 7.599752902984619\n",
      "validation loss = 7.793908721522281\n",
      "Batch 39 / 157\n",
      "training loss = 7.893147945404053\n",
      "validation loss = 7.790556154753032\n",
      "Batch 40 / 157\n",
      "training loss = 7.9931721687316895\n",
      "validation loss = 7.781438200097335\n",
      "Batch 41 / 157\n",
      "training loss = 7.785433292388916\n",
      "validation loss = 7.7794005996302555\n",
      "Batch 42 / 157\n",
      "training loss = 7.73311185836792\n",
      "validation loss = 7.7736266286749585\n",
      "Batch 43 / 157\n",
      "training loss = 7.845063209533691\n",
      "validation loss = 7.76643675252011\n",
      "Batch 44 / 157\n",
      "training loss = 8.073899269104004\n",
      "validation loss = 7.753084559189646\n",
      "Batch 45 / 157\n",
      "training loss = 7.594086647033691\n",
      "validation loss = 7.740670931966681\n",
      "Batch 46 / 157\n",
      "training loss = 7.667862415313721\n",
      "validation loss = 7.7292641087582235\n",
      "Batch 47 / 157\n",
      "training loss = 7.535829067230225\n",
      "validation loss = 7.724288287915681\n",
      "Batch 48 / 157\n",
      "training loss = 7.947736740112305\n",
      "validation loss = 7.728840125234504\n",
      "Batch 49 / 157\n",
      "training loss = 7.933638572692871\n",
      "validation loss = 7.732160769010845\n",
      "Batch 50 / 157\n",
      "training loss = 8.084174156188965\n",
      "validation loss = 7.73971868816175\n",
      "Batch 51 / 157\n",
      "training loss = 7.750540256500244\n",
      "validation loss = 7.727362256301077\n",
      "Batch 52 / 157\n",
      "training loss = 7.440261363983154\n",
      "validation loss = 7.7101318710728695\n",
      "Batch 53 / 157\n",
      "training loss = 7.729120254516602\n",
      "validation loss = 7.7049972634566455\n",
      "Batch 54 / 157\n",
      "training loss = 7.978030681610107\n",
      "validation loss = 7.706870681361148\n",
      "Batch 55 / 157\n",
      "training loss = 7.408260345458984\n",
      "validation loss = 7.708087042758339\n",
      "Batch 56 / 157\n",
      "training loss = 7.803389549255371\n",
      "validation loss = 7.7019474882828565\n",
      "Batch 57 / 157\n",
      "training loss = 8.11752986907959\n",
      "validation loss = 7.698409180892141\n",
      "Batch 58 / 157\n",
      "training loss = 7.995285511016846\n",
      "validation loss = 7.694113580804122\n",
      "Batch 59 / 157\n",
      "training loss = 7.8420867919921875\n",
      "validation loss = 7.6776966546711165\n",
      "Batch 60 / 157\n",
      "training loss = 8.057613372802734\n",
      "validation loss = 7.678631933111894\n",
      "Batch 61 / 157\n",
      "training loss = 7.700396537780762\n",
      "validation loss = 7.677807431471975\n",
      "Batch 62 / 157\n",
      "training loss = 7.695308685302734\n",
      "validation loss = 7.681060916499088\n",
      "Batch 63 / 157\n",
      "training loss = 7.630137920379639\n",
      "validation loss = 7.683101729342812\n",
      "Batch 64 / 157\n",
      "training loss = 7.703853607177734\n",
      "validation loss = 7.670874043514854\n",
      "Batch 65 / 157\n",
      "training loss = 7.805797576904297\n",
      "validation loss = 7.670690561595716\n",
      "Batch 66 / 157\n",
      "training loss = 7.751582622528076\n",
      "validation loss = 7.65821193393908\n",
      "Batch 67 / 157\n",
      "training loss = 7.61494779586792\n",
      "validation loss = 7.655582804428904\n",
      "Batch 68 / 157\n",
      "training loss = 7.645863056182861\n",
      "validation loss = 7.654499831952546\n",
      "Batch 69 / 157\n",
      "training loss = 7.695629119873047\n",
      "validation loss = 7.649190902709961\n",
      "Batch 70 / 157\n",
      "training loss = 7.743566513061523\n",
      "validation loss = 7.6532567425778035\n",
      "Batch 71 / 157\n",
      "training loss = 7.568422317504883\n",
      "validation loss = 7.646522923519737\n",
      "Batch 72 / 157\n",
      "training loss = 7.524824619293213\n",
      "validation loss = 7.638908285843699\n",
      "Batch 73 / 157\n",
      "training loss = 7.56660795211792\n",
      "validation loss = 7.631115110296952\n",
      "Batch 74 / 157\n",
      "training loss = 7.788762092590332\n",
      "validation loss = 7.627774113102963\n",
      "Batch 75 / 157\n",
      "training loss = 7.572178363800049\n",
      "validation loss = 7.628544907820852\n",
      "Batch 76 / 157\n",
      "training loss = 7.691220283508301\n",
      "validation loss = 7.625559028826262\n",
      "Batch 77 / 157\n",
      "training loss = 7.515987873077393\n",
      "validation loss = 7.62184843264128\n",
      "Batch 78 / 157\n",
      "training loss = 7.616814613342285\n",
      "validation loss = 7.617720227492483\n",
      "Batch 79 / 157\n",
      "training loss = 7.230107307434082\n",
      "validation loss = 7.606845278488962\n",
      "Batch 80 / 157\n",
      "training loss = 7.519328594207764\n",
      "validation loss = 7.615433015321431\n",
      "Batch 81 / 157\n",
      "training loss = 7.506488800048828\n",
      "validation loss = 7.613127582951596\n",
      "Batch 82 / 157\n",
      "training loss = 7.644092082977295\n",
      "validation loss = 7.618511074467709\n",
      "Batch 83 / 157\n",
      "training loss = 7.391347408294678\n",
      "validation loss = 7.610593544809442\n",
      "Batch 84 / 157\n",
      "training loss = 7.919798374176025\n",
      "validation loss = 7.592172246230276\n",
      "Batch 85 / 157\n",
      "training loss = 7.913448333740234\n",
      "validation loss = 7.589054960953562\n",
      "Batch 86 / 157\n",
      "training loss = 7.196488380432129\n",
      "validation loss = 7.592446553079705\n",
      "Batch 87 / 157\n",
      "training loss = 7.6348395347595215\n",
      "validation loss = 7.5948772179452995\n",
      "Batch 88 / 157\n",
      "training loss = 7.802313327789307\n",
      "validation loss = 7.589926167538292\n",
      "Batch 89 / 157\n",
      "training loss = 7.447629928588867\n",
      "validation loss = 7.583718776702881\n",
      "Batch 90 / 157\n",
      "training loss = 7.008610725402832\n",
      "validation loss = 7.573185343491404\n",
      "Batch 91 / 157\n",
      "training loss = 7.381453037261963\n",
      "validation loss = 7.579583193126478\n",
      "Batch 92 / 157\n",
      "training loss = 7.419962406158447\n",
      "validation loss = 7.582847143474378\n",
      "Batch 93 / 157\n",
      "training loss = 7.817251205444336\n",
      "validation loss = 7.5793053978367855\n",
      "Batch 94 / 157\n",
      "training loss = 7.674683094024658\n",
      "validation loss = 7.567665978481895\n",
      "Batch 95 / 157\n",
      "training loss = 7.375796318054199\n",
      "validation loss = 7.559078718486585\n",
      "Batch 96 / 157\n",
      "training loss = 7.725119113922119\n",
      "validation loss = 7.5617646418119735\n",
      "Batch 97 / 157\n",
      "training loss = 7.579962730407715\n",
      "validation loss = 7.557822026704487\n",
      "Batch 98 / 157\n",
      "training loss = 7.963841915130615\n",
      "validation loss = 7.565362077010305\n",
      "Batch 99 / 157\n",
      "training loss = 7.818340301513672\n",
      "validation loss = 7.577919407894737\n",
      "Batch 100 / 157\n",
      "training loss = 7.580002784729004\n",
      "validation loss = 7.557237047898142\n",
      "Batch 101 / 157\n",
      "training loss = 7.922938346862793\n",
      "validation loss = 7.551260220377069\n",
      "Batch 102 / 157\n",
      "training loss = 7.567164421081543\n",
      "validation loss = 7.543147463547556\n",
      "Batch 103 / 157\n",
      "training loss = 7.7160258293151855\n",
      "validation loss = 7.538687103673031\n",
      "Batch 104 / 157\n",
      "training loss = 7.3866119384765625\n",
      "validation loss = 7.541413934607255\n",
      "Batch 105 / 157\n",
      "training loss = 7.462180137634277\n",
      "validation loss = 7.537561868366442\n",
      "Batch 106 / 157\n",
      "training loss = 7.848996162414551\n",
      "validation loss = 7.534857749938965\n",
      "Batch 107 / 157\n",
      "training loss = 7.63726806640625\n",
      "validation loss = 7.5244695512872\n",
      "Batch 108 / 157\n",
      "training loss = 7.539343357086182\n",
      "validation loss = 7.528053384078176\n",
      "Batch 109 / 157\n",
      "training loss = 7.289025783538818\n",
      "validation loss = 7.529113794627943\n",
      "Batch 110 / 157\n",
      "training loss = 7.417418003082275\n",
      "validation loss = 7.523297761615954\n",
      "Batch 111 / 157\n",
      "training loss = 7.765419960021973\n",
      "validation loss = 7.515587856895046\n",
      "Batch 112 / 157\n",
      "training loss = 7.50851583480835\n",
      "validation loss = 7.507960896742971\n",
      "Batch 113 / 157\n",
      "training loss = 7.468627452850342\n",
      "validation loss = 7.510980580982409\n",
      "Batch 114 / 157\n",
      "training loss = 7.371135711669922\n",
      "validation loss = 7.506498663048995\n",
      "Batch 115 / 157\n",
      "training loss = 7.644035816192627\n",
      "validation loss = 7.4936652183532715\n",
      "Batch 116 / 157\n",
      "training loss = 7.825918674468994\n",
      "validation loss = 7.498327757182874\n",
      "Batch 117 / 157\n",
      "training loss = 7.625495433807373\n",
      "validation loss = 7.501057700106972\n",
      "Batch 118 / 157\n",
      "training loss = 7.43766450881958\n",
      "validation loss = 7.490305248059724\n",
      "Batch 119 / 157\n",
      "training loss = 7.4586663246154785\n",
      "validation loss = 7.482353888059917\n",
      "Batch 120 / 157\n",
      "training loss = 7.376520156860352\n",
      "validation loss = 7.4897514142488175\n",
      "Batch 121 / 157\n",
      "training loss = 7.2425360679626465\n",
      "validation loss = 7.491337500120464\n",
      "Batch 122 / 157\n",
      "training loss = 7.460198879241943\n",
      "validation loss = 7.481177756660863\n",
      "Batch 123 / 157\n",
      "training loss = 7.4182353019714355\n",
      "validation loss = 7.467353770607396\n",
      "Batch 124 / 157\n",
      "training loss = 7.6715216636657715\n",
      "validation loss = 7.461375462381463\n",
      "Batch 125 / 157\n",
      "training loss = 7.630237102508545\n",
      "validation loss = 7.4746302052548055\n",
      "Batch 126 / 157\n",
      "training loss = 7.1241021156311035\n",
      "validation loss = 7.483469787396882\n",
      "Batch 127 / 157\n",
      "training loss = 7.4639692306518555\n",
      "validation loss = 7.46009437661422\n",
      "Batch 128 / 157\n",
      "training loss = 7.4476494789123535\n",
      "validation loss = 7.447534485867149\n",
      "Batch 129 / 157\n",
      "training loss = 7.455883502960205\n",
      "validation loss = 7.446728455392938\n",
      "Batch 130 / 157\n",
      "training loss = 7.151808261871338\n",
      "validation loss = 7.452222171582673\n",
      "Batch 131 / 157\n",
      "training loss = 8.13479232788086\n",
      "validation loss = 7.444812875044973\n",
      "Batch 132 / 157\n",
      "training loss = 7.103997707366943\n",
      "validation loss = 7.42950251227931\n",
      "Batch 133 / 157\n",
      "training loss = 7.908786773681641\n",
      "validation loss = 7.424159627211721\n",
      "Batch 134 / 157\n",
      "training loss = 7.327521800994873\n",
      "validation loss = 7.4270807316428735\n",
      "Batch 135 / 157\n",
      "training loss = 7.421188831329346\n",
      "validation loss = 7.434209572641473\n",
      "Batch 136 / 157\n",
      "training loss = 7.185091972351074\n",
      "validation loss = 7.427323667626632\n",
      "Batch 137 / 157\n",
      "training loss = 7.191220760345459\n",
      "validation loss = 7.411093686756335\n",
      "Batch 138 / 157\n",
      "training loss = 7.13899564743042\n",
      "validation loss = 7.407787072031121\n",
      "Batch 139 / 157\n",
      "training loss = 7.373046875\n",
      "validation loss = 7.415226258729634\n",
      "Batch 140 / 157\n",
      "training loss = 7.032344341278076\n",
      "validation loss = 7.41412898113853\n",
      "Batch 141 / 157\n",
      "training loss = 7.229507923126221\n",
      "validation loss = 7.413418769836426\n",
      "Batch 142 / 157\n",
      "training loss = 7.613584995269775\n",
      "validation loss = 7.406845343740363\n",
      "Batch 143 / 157\n",
      "training loss = 6.999312877655029\n",
      "validation loss = 7.409402219872725\n",
      "Batch 144 / 157\n",
      "training loss = 7.040884971618652\n",
      "validation loss = 7.39443480341058\n",
      "Batch 145 / 157\n",
      "training loss = 7.808393478393555\n",
      "validation loss = 7.3929825582002335\n",
      "Batch 146 / 157\n",
      "training loss = 7.360136032104492\n",
      "validation loss = 7.391437555614271\n",
      "Batch 147 / 157\n",
      "training loss = 7.3482770919799805\n",
      "validation loss = 7.387723470989027\n",
      "Batch 148 / 157\n",
      "training loss = 6.984457492828369\n",
      "validation loss = 7.386234710091038\n",
      "Batch 149 / 157\n",
      "training loss = 7.1326704025268555\n",
      "validation loss = 7.376272025861238\n",
      "Batch 150 / 157\n",
      "training loss = 7.487034320831299\n",
      "validation loss = 7.387380775652434\n",
      "Batch 151 / 157\n",
      "training loss = 7.39000129699707\n",
      "validation loss = 7.372478585494192\n",
      "Batch 152 / 157\n",
      "training loss = 7.236764430999756\n",
      "validation loss = 7.377432873374538\n",
      "Batch 153 / 157\n",
      "training loss = 7.1705546379089355\n",
      "validation loss = 7.370578715675755\n",
      "Batch 154 / 157\n",
      "training loss = 7.271688938140869\n",
      "validation loss = 7.372114206615247\n",
      "Batch 155 / 157\n",
      "training loss = 7.046759605407715\n",
      "validation loss = 7.3633698664213485\n",
      "Batch 156 / 157\n",
      "training loss = 7.103044033050537\n",
      "validation loss = 7.366476510700426\n",
      "Batch 157 / 157\n",
      "training loss = 7.657336235046387\n",
      "validation loss = 7.360336253517552\n",
      "Average training loss: 7.86\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.377741813659668\n",
      "validation loss = 10.328237985309801\n",
      "Batch 2 / 157\n",
      "training loss = 10.33129596710205\n",
      "validation loss = 10.238971860785233\n",
      "Batch 3 / 157\n",
      "training loss = 10.254837036132812\n",
      "validation loss = 10.129645949915835\n",
      "Batch 4 / 157\n",
      "training loss = 10.131421089172363\n",
      "validation loss = 10.013705153214303\n",
      "Batch 5 / 157\n",
      "training loss = 10.014554023742676\n",
      "validation loss = 9.89935598875347\n",
      "Batch 6 / 157\n",
      "training loss = 9.881272315979004\n",
      "validation loss = 9.782706662228232\n",
      "Batch 7 / 157\n",
      "training loss = 9.770977020263672\n",
      "validation loss = 9.665044382998818\n",
      "Batch 8 / 157\n",
      "training loss = 9.676889419555664\n",
      "validation loss = 9.54591535267077\n",
      "Batch 9 / 157\n",
      "training loss = 9.507600784301758\n",
      "validation loss = 9.429433220311216\n",
      "Batch 10 / 157\n",
      "training loss = 9.36484432220459\n",
      "validation loss = 9.313385310925936\n",
      "Batch 11 / 157\n",
      "training loss = 9.272254943847656\n",
      "validation loss = 9.200813142876877\n",
      "Batch 12 / 157\n",
      "training loss = 9.239925384521484\n",
      "validation loss = 9.092208159597297\n",
      "Batch 13 / 157\n",
      "training loss = 9.123506546020508\n",
      "validation loss = 8.986547470092773\n",
      "Batch 14 / 157\n",
      "training loss = 8.9673490524292\n",
      "validation loss = 8.88409097571122\n",
      "Batch 15 / 157\n",
      "training loss = 8.830236434936523\n",
      "validation loss = 8.787306484423185\n",
      "Batch 16 / 157\n",
      "training loss = 8.761557579040527\n",
      "validation loss = 8.691536401447497\n",
      "Batch 17 / 157\n",
      "training loss = 8.605232238769531\n",
      "validation loss = 8.603131495024028\n",
      "Batch 18 / 157\n",
      "training loss = 8.582883834838867\n",
      "validation loss = 8.519461180034437\n",
      "Batch 19 / 157\n",
      "training loss = 8.679872512817383\n",
      "validation loss = 8.437619510449862\n",
      "Batch 20 / 157\n",
      "training loss = 8.344432830810547\n",
      "validation loss = 8.361829054983039\n",
      "Batch 21 / 157\n",
      "training loss = 8.376522064208984\n",
      "validation loss = 8.291060046145791\n",
      "Batch 22 / 157\n",
      "training loss = 8.186132431030273\n",
      "validation loss = 8.225481585452432\n",
      "Batch 23 / 157\n",
      "training loss = 8.177709579467773\n",
      "validation loss = 8.16565207431191\n",
      "Batch 24 / 157\n",
      "training loss = 8.129955291748047\n",
      "validation loss = 8.108718897167005\n",
      "Batch 25 / 157\n",
      "training loss = 8.091678619384766\n",
      "validation loss = 8.055954280652498\n",
      "Batch 26 / 157\n",
      "training loss = 8.16357135772705\n",
      "validation loss = 8.007867838207044\n",
      "Batch 27 / 157\n",
      "training loss = 8.037113189697266\n",
      "validation loss = 7.962746569984837\n",
      "Batch 28 / 157\n",
      "training loss = 7.9253973960876465\n",
      "validation loss = 7.921123128188284\n",
      "Batch 29 / 157\n",
      "training loss = 8.00307846069336\n",
      "validation loss = 7.8875211163571\n",
      "Batch 30 / 157\n",
      "training loss = 7.942858695983887\n",
      "validation loss = 7.853513291007594\n",
      "Batch 31 / 157\n",
      "training loss = 7.887121677398682\n",
      "validation loss = 7.823200075249923\n",
      "Batch 32 / 157\n",
      "training loss = 8.288609504699707\n",
      "validation loss = 7.798768143904836\n",
      "Batch 33 / 157\n",
      "training loss = 7.807796478271484\n",
      "validation loss = 7.7777971970407584\n",
      "Batch 34 / 157\n",
      "training loss = 7.737429141998291\n",
      "validation loss = 7.756304590325606\n",
      "Batch 35 / 157\n",
      "training loss = 7.968990325927734\n",
      "validation loss = 7.735226154327393\n",
      "Batch 36 / 157\n",
      "training loss = 7.720867156982422\n",
      "validation loss = 7.720110918346204\n",
      "Batch 37 / 157\n",
      "training loss = 7.617237567901611\n",
      "validation loss = 7.701792842463443\n",
      "Batch 38 / 157\n",
      "training loss = 7.803236961364746\n",
      "validation loss = 7.697446371379652\n",
      "Batch 39 / 157\n",
      "training loss = 7.868624687194824\n",
      "validation loss = 7.68316462165431\n",
      "Batch 40 / 157\n",
      "training loss = 7.5642876625061035\n",
      "validation loss = 7.676178731416401\n",
      "Batch 41 / 157\n",
      "training loss = 7.676687240600586\n",
      "validation loss = 7.6673503925925806\n",
      "Batch 42 / 157\n",
      "training loss = 7.591783046722412\n",
      "validation loss = 7.6622595285114485\n",
      "Batch 43 / 157\n",
      "training loss = 7.856865882873535\n",
      "validation loss = 7.657909493697317\n",
      "Batch 44 / 157\n",
      "training loss = 7.8276238441467285\n",
      "validation loss = 7.649414589530544\n",
      "Batch 45 / 157\n",
      "training loss = 7.691084384918213\n",
      "validation loss = 7.646094548074823\n",
      "Batch 46 / 157\n",
      "training loss = 7.636000156402588\n",
      "validation loss = 7.643739951284308\n",
      "Batch 47 / 157\n",
      "training loss = 7.548871994018555\n",
      "validation loss = 7.643981155596282\n",
      "Batch 48 / 157\n",
      "training loss = 7.605232238769531\n",
      "validation loss = 7.638159174668162\n",
      "Batch 49 / 157\n",
      "training loss = 7.731220245361328\n",
      "validation loss = 7.6385447602523\n",
      "Batch 50 / 157\n",
      "training loss = 7.75779914855957\n",
      "validation loss = 7.643680447026303\n",
      "Batch 51 / 157\n",
      "training loss = 7.686965465545654\n",
      "validation loss = 7.640995954212389\n",
      "Batch 52 / 157\n",
      "training loss = 7.649535655975342\n",
      "validation loss = 7.633859709689491\n",
      "Batch 53 / 157\n",
      "training loss = 7.454799175262451\n",
      "validation loss = 7.63181051455046\n",
      "Batch 54 / 157\n",
      "training loss = 7.523701190948486\n",
      "validation loss = 7.625518020830657\n",
      "Batch 55 / 157\n",
      "training loss = 7.517077445983887\n",
      "validation loss = 7.631867709912751\n",
      "Batch 56 / 157\n",
      "training loss = 7.619603157043457\n",
      "validation loss = 7.628378165395636\n",
      "Batch 57 / 157\n",
      "training loss = 7.611294746398926\n",
      "validation loss = 7.626345132526598\n",
      "Batch 58 / 157\n",
      "training loss = 7.7025251388549805\n",
      "validation loss = 7.623413863934968\n",
      "Batch 59 / 157\n",
      "training loss = 7.642396926879883\n",
      "validation loss = 7.614671531476472\n",
      "Batch 60 / 157\n",
      "training loss = 7.685595512390137\n",
      "validation loss = 7.611088075135884\n",
      "Batch 61 / 157\n",
      "training loss = 7.625140190124512\n",
      "validation loss = 7.613133706544575\n",
      "Batch 62 / 157\n",
      "training loss = 7.570526599884033\n",
      "validation loss = 7.6093041771336605\n",
      "Batch 63 / 157\n",
      "training loss = 7.529462814331055\n",
      "validation loss = 7.607182151392887\n",
      "Batch 64 / 157\n",
      "training loss = 7.633356094360352\n",
      "validation loss = 7.603098492873342\n",
      "Batch 65 / 157\n",
      "training loss = 7.390695095062256\n",
      "validation loss = 7.602768647043328\n",
      "Batch 66 / 157\n",
      "training loss = 7.680694580078125\n",
      "validation loss = 7.6015119552612305\n",
      "Batch 67 / 157\n",
      "training loss = 7.630119323730469\n",
      "validation loss = 7.602026136297929\n",
      "Batch 68 / 157\n",
      "training loss = 7.591597080230713\n",
      "validation loss = 7.599994910390754\n",
      "Batch 69 / 157\n",
      "training loss = 7.660637378692627\n",
      "validation loss = 7.596342714209306\n",
      "Batch 70 / 157\n",
      "training loss = 7.693210124969482\n",
      "validation loss = 7.595590992977745\n",
      "Batch 71 / 157\n",
      "training loss = 7.386202335357666\n",
      "validation loss = 7.5956682907907584\n",
      "Batch 72 / 157\n",
      "training loss = 7.6466383934021\n",
      "validation loss = 7.592339440395958\n",
      "Batch 73 / 157\n",
      "training loss = 7.836857795715332\n",
      "validation loss = 7.593471100455837\n",
      "Batch 74 / 157\n",
      "training loss = 7.697989463806152\n",
      "validation loss = 7.5930252828096085\n",
      "Batch 75 / 157\n",
      "training loss = 7.586582660675049\n",
      "validation loss = 7.588647892600612\n",
      "Batch 76 / 157\n",
      "training loss = 7.664051532745361\n",
      "validation loss = 7.580386462964509\n",
      "Batch 77 / 157\n",
      "training loss = 7.521964073181152\n",
      "validation loss = 7.576374957436009\n",
      "Batch 78 / 157\n",
      "training loss = 7.586137294769287\n",
      "validation loss = 7.571220172078986\n",
      "Batch 79 / 157\n",
      "training loss = 7.647000789642334\n",
      "validation loss = 7.572068189319811\n",
      "Batch 80 / 157\n",
      "training loss = 7.524407863616943\n",
      "validation loss = 7.564852338088186\n",
      "Batch 81 / 157\n",
      "training loss = 7.796666622161865\n",
      "validation loss = 7.565729116138659\n",
      "Batch 82 / 157\n",
      "training loss = 7.435243606567383\n",
      "validation loss = 7.567455040781121\n",
      "Batch 83 / 157\n",
      "training loss = 7.719892501831055\n",
      "validation loss = 7.56455639788979\n",
      "Batch 84 / 157\n",
      "training loss = 7.494265556335449\n",
      "validation loss = 7.561047478726036\n",
      "Batch 85 / 157\n",
      "training loss = 7.545732021331787\n",
      "validation loss = 7.551351421757748\n",
      "Batch 86 / 157\n",
      "training loss = 7.572782039642334\n",
      "validation loss = 7.555293133384303\n",
      "Batch 87 / 157\n",
      "training loss = 7.576970100402832\n",
      "validation loss = 7.5496290859423185\n",
      "Batch 88 / 157\n",
      "training loss = 7.619478225708008\n",
      "validation loss = 7.544643075842607\n",
      "Batch 89 / 157\n",
      "training loss = 7.3819966316223145\n",
      "validation loss = 7.544734327416671\n",
      "Batch 90 / 157\n",
      "training loss = 7.5306830406188965\n",
      "validation loss = 7.542200289274517\n",
      "Batch 91 / 157\n",
      "training loss = 7.441019535064697\n",
      "validation loss = 7.533401991191663\n",
      "Batch 92 / 157\n",
      "training loss = 7.376705646514893\n",
      "validation loss = 7.526780153575697\n",
      "Batch 93 / 157\n",
      "training loss = 7.594695568084717\n",
      "validation loss = 7.520069247797916\n",
      "Batch 94 / 157\n",
      "training loss = 7.465418815612793\n",
      "validation loss = 7.517803041558516\n",
      "Batch 95 / 157\n",
      "training loss = 7.65513277053833\n",
      "validation loss = 7.509779578761051\n",
      "Batch 96 / 157\n",
      "training loss = 7.249863624572754\n",
      "validation loss = 7.512024703778718\n",
      "Batch 97 / 157\n",
      "training loss = 7.424442291259766\n",
      "validation loss = 7.5046336274398\n",
      "Batch 98 / 157\n",
      "training loss = 7.6527204513549805\n",
      "validation loss = 7.496933911976061\n",
      "Batch 99 / 157\n",
      "training loss = 7.493154525756836\n",
      "validation loss = 7.493815045607717\n",
      "Batch 100 / 157\n",
      "training loss = 7.5896830558776855\n",
      "validation loss = 7.4910219092118115\n",
      "Batch 101 / 157\n",
      "training loss = 7.533618927001953\n",
      "validation loss = 7.49154025629947\n",
      "Batch 102 / 157\n",
      "training loss = 7.4112229347229\n",
      "validation loss = 7.482306555697792\n",
      "Batch 103 / 157\n",
      "training loss = 7.293736934661865\n",
      "validation loss = 7.481284216830605\n",
      "Batch 104 / 157\n",
      "training loss = 7.637099266052246\n",
      "validation loss = 7.4791613127055925\n",
      "Batch 105 / 157\n",
      "training loss = 7.575117111206055\n",
      "validation loss = 7.478364040977077\n",
      "Batch 106 / 157\n",
      "training loss = 7.516984939575195\n",
      "validation loss = 7.466398188942357\n",
      "Batch 107 / 157\n",
      "training loss = 7.492402076721191\n",
      "validation loss = 7.4603164572464795\n",
      "Batch 108 / 157\n",
      "training loss = 7.761631965637207\n",
      "validation loss = 7.469603839673494\n",
      "Batch 109 / 157\n",
      "training loss = 7.387824058532715\n",
      "validation loss = 7.452745964652614\n",
      "Batch 110 / 157\n",
      "training loss = 7.280367374420166\n",
      "validation loss = 7.442527570222554\n",
      "Batch 111 / 157\n",
      "training loss = 7.716847896575928\n",
      "validation loss = 7.44846396697195\n",
      "Batch 112 / 157\n",
      "training loss = 7.620265483856201\n",
      "validation loss = 7.4444448320489185\n",
      "Batch 113 / 157\n",
      "training loss = 7.514487266540527\n",
      "validation loss = 7.442755648964329\n",
      "Batch 114 / 157\n",
      "training loss = 7.39296293258667\n",
      "validation loss = 7.436740724663985\n",
      "Batch 115 / 157\n",
      "training loss = 7.399428844451904\n",
      "validation loss = 7.4329080079731185\n",
      "Batch 116 / 157\n",
      "training loss = 7.497077941894531\n",
      "validation loss = 7.436499746222245\n",
      "Batch 117 / 157\n",
      "training loss = 7.614727020263672\n",
      "validation loss = 7.431017323544151\n",
      "Batch 118 / 157\n",
      "training loss = 7.729514122009277\n",
      "validation loss = 7.428649425506592\n",
      "Batch 119 / 157\n",
      "training loss = 7.5050530433654785\n",
      "validation loss = 7.427428371027896\n",
      "Batch 120 / 157\n",
      "training loss = 7.435579776763916\n",
      "validation loss = 7.42579261880172\n",
      "Batch 121 / 157\n",
      "training loss = 7.502320766448975\n",
      "validation loss = 7.416238759693346\n",
      "Batch 122 / 157\n",
      "training loss = 7.724302768707275\n",
      "validation loss = 7.41538519608347\n",
      "Batch 123 / 157\n",
      "training loss = 7.112842082977295\n",
      "validation loss = 7.41190410915174\n",
      "Batch 124 / 157\n",
      "training loss = 7.424108982086182\n",
      "validation loss = 7.411924562956157\n",
      "Batch 125 / 157\n",
      "training loss = 7.714864253997803\n",
      "validation loss = 7.404141802536814\n",
      "Batch 126 / 157\n",
      "training loss = 7.5390706062316895\n",
      "validation loss = 7.401337949853194\n",
      "Batch 127 / 157\n",
      "training loss = 7.43571138381958\n",
      "validation loss = 7.40368642305073\n",
      "Batch 128 / 157\n",
      "training loss = 7.6009063720703125\n",
      "validation loss = 7.40677745718705\n",
      "Batch 129 / 157\n",
      "training loss = 7.583864688873291\n",
      "validation loss = 7.397489095989027\n",
      "Batch 130 / 157\n",
      "training loss = 7.528045654296875\n",
      "validation loss = 7.396239632054379\n",
      "Batch 131 / 157\n",
      "training loss = 7.568865776062012\n",
      "validation loss = 7.391066450821726\n",
      "Batch 132 / 157\n",
      "training loss = 7.272805690765381\n",
      "validation loss = 7.388024781879626\n",
      "Batch 133 / 157\n",
      "training loss = 7.354151725769043\n",
      "validation loss = 7.387392596194618\n",
      "Batch 134 / 157\n",
      "training loss = 7.318612575531006\n",
      "validation loss = 7.384337023684853\n",
      "Batch 135 / 157\n",
      "training loss = 7.379759311676025\n",
      "validation loss = 7.377569750735634\n",
      "Batch 136 / 157\n",
      "training loss = 7.28679084777832\n",
      "validation loss = 7.3784513222543815\n",
      "Batch 137 / 157\n",
      "training loss = 7.454570293426514\n",
      "validation loss = 7.378317230626156\n",
      "Batch 138 / 157\n",
      "training loss = 7.0880889892578125\n",
      "validation loss = 7.375003940180728\n",
      "Batch 139 / 157\n",
      "training loss = 7.41397762298584\n",
      "validation loss = 7.370784433264482\n",
      "Batch 140 / 157\n",
      "training loss = 7.42968225479126\n",
      "validation loss = 7.369566691549201\n",
      "Batch 141 / 157\n",
      "training loss = 7.244776725769043\n",
      "validation loss = 7.366306731575413\n",
      "Batch 142 / 157\n",
      "training loss = 7.216318607330322\n",
      "validation loss = 7.365873562662225\n",
      "Batch 143 / 157\n",
      "training loss = 7.456899166107178\n",
      "validation loss = 7.364207242664538\n",
      "Batch 144 / 157\n",
      "training loss = 7.622399806976318\n",
      "validation loss = 7.359904439825761\n",
      "Batch 145 / 157\n",
      "training loss = 7.458900451660156\n",
      "validation loss = 7.36109091106214\n",
      "Batch 146 / 157\n",
      "training loss = 7.238742351531982\n",
      "validation loss = 7.357746826974969\n",
      "Batch 147 / 157\n",
      "training loss = 7.149203300476074\n",
      "validation loss = 7.3522258808738306\n",
      "Batch 148 / 157\n",
      "training loss = 7.412060260772705\n",
      "validation loss = 7.3542992943211605\n",
      "Batch 149 / 157\n",
      "training loss = 7.358837127685547\n",
      "validation loss = 7.344566596181769\n",
      "Batch 150 / 157\n",
      "training loss = 7.40934419631958\n",
      "validation loss = 7.352944700341475\n",
      "Batch 151 / 157\n",
      "training loss = 7.621166706085205\n",
      "validation loss = 7.347490511442485\n",
      "Batch 152 / 157\n",
      "training loss = 7.424888610839844\n",
      "validation loss = 7.347216254786441\n",
      "Batch 153 / 157\n",
      "training loss = 7.4479546546936035\n",
      "validation loss = 7.347410377703215\n",
      "Batch 154 / 157\n",
      "training loss = 7.7451300621032715\n",
      "validation loss = 7.34183743125514\n",
      "Batch 155 / 157\n",
      "training loss = 7.610381603240967\n",
      "validation loss = 7.334615632107384\n",
      "Batch 156 / 157\n",
      "training loss = 7.165970325469971\n",
      "validation loss = 7.338535785675049\n",
      "Batch 157 / 157\n",
      "training loss = 7.341999053955078\n",
      "validation loss = 7.336226111964176\n",
      "Average training loss: 7.82\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.375601768493652\n",
      "validation loss = 10.344651724162855\n",
      "Batch 2 / 157\n",
      "training loss = 10.343335151672363\n",
      "validation loss = 10.238731685437655\n",
      "Batch 3 / 157\n",
      "training loss = 10.233623504638672\n",
      "validation loss = 10.11966720380281\n",
      "Batch 4 / 157\n",
      "training loss = 10.156210899353027\n",
      "validation loss = 10.00066200055574\n",
      "Batch 5 / 157\n",
      "training loss = 10.00300121307373\n",
      "validation loss = 9.881262126721834\n",
      "Batch 6 / 157\n",
      "training loss = 9.89234733581543\n",
      "validation loss = 9.764432656137567\n",
      "Batch 7 / 157\n",
      "training loss = 9.7698335647583\n",
      "validation loss = 9.649228397168612\n",
      "Batch 8 / 157\n",
      "training loss = 9.614202499389648\n",
      "validation loss = 9.531698277122096\n",
      "Batch 9 / 157\n",
      "training loss = 9.515390396118164\n",
      "validation loss = 9.417920514156943\n",
      "Batch 10 / 157\n",
      "training loss = 9.412866592407227\n",
      "validation loss = 9.305823978624845\n",
      "Batch 11 / 157\n",
      "training loss = 9.275311470031738\n",
      "validation loss = 9.197224366037469\n",
      "Batch 12 / 157\n",
      "training loss = 9.144166946411133\n",
      "validation loss = 9.091511676186009\n",
      "Batch 13 / 157\n",
      "training loss = 9.086681365966797\n",
      "validation loss = 8.986604138424521\n",
      "Batch 14 / 157\n",
      "training loss = 8.99141788482666\n",
      "validation loss = 8.89091612163343\n",
      "Batch 15 / 157\n",
      "training loss = 8.928983688354492\n",
      "validation loss = 8.79439986379523\n",
      "Batch 16 / 157\n",
      "training loss = 8.768510818481445\n",
      "validation loss = 8.70590837378251\n",
      "Batch 17 / 157\n",
      "training loss = 8.727872848510742\n",
      "validation loss = 8.616867617556924\n",
      "Batch 18 / 157\n",
      "training loss = 8.59634017944336\n",
      "validation loss = 8.533038139343262\n",
      "Batch 19 / 157\n",
      "training loss = 8.512334823608398\n",
      "validation loss = 8.454777617203561\n",
      "Batch 20 / 157\n",
      "training loss = 8.48902416229248\n",
      "validation loss = 8.382775256508275\n",
      "Batch 21 / 157\n",
      "training loss = 8.476040840148926\n",
      "validation loss = 8.31176411478143\n",
      "Batch 22 / 157\n",
      "training loss = 8.408137321472168\n",
      "validation loss = 8.251084227310983\n",
      "Batch 23 / 157\n",
      "training loss = 8.344320297241211\n",
      "validation loss = 8.186835088227925\n",
      "Batch 24 / 157\n",
      "training loss = 8.21254825592041\n",
      "validation loss = 8.126103702344393\n",
      "Batch 25 / 157\n",
      "training loss = 8.24023151397705\n",
      "validation loss = 8.076472784343519\n",
      "Batch 26 / 157\n",
      "training loss = 8.05467414855957\n",
      "validation loss = 8.028912393670334\n",
      "Batch 27 / 157\n",
      "training loss = 7.97199821472168\n",
      "validation loss = 7.9830320759823445\n",
      "Batch 28 / 157\n",
      "training loss = 7.922527313232422\n",
      "validation loss = 7.936634239397551\n",
      "Batch 29 / 157\n",
      "training loss = 7.898420333862305\n",
      "validation loss = 7.895006982903731\n",
      "Batch 30 / 157\n",
      "training loss = 7.82996129989624\n",
      "validation loss = 7.8558511232074935\n",
      "Batch 31 / 157\n",
      "training loss = 7.698988914489746\n",
      "validation loss = 7.819440289547569\n",
      "Batch 32 / 157\n",
      "training loss = 7.74812126159668\n",
      "validation loss = 7.7893923709267066\n",
      "Batch 33 / 157\n",
      "training loss = 7.84536600112915\n",
      "validation loss = 7.760794363523784\n",
      "Batch 34 / 157\n",
      "training loss = 7.591702461242676\n",
      "validation loss = 7.74016433013113\n",
      "Batch 35 / 157\n",
      "training loss = 7.837878227233887\n",
      "validation loss = 7.716361497577868\n",
      "Batch 36 / 157\n",
      "training loss = 7.571707248687744\n",
      "validation loss = 7.693315556174831\n",
      "Batch 37 / 157\n",
      "training loss = 7.716674327850342\n",
      "validation loss = 7.67540010653044\n",
      "Batch 38 / 157\n",
      "training loss = 7.876834869384766\n",
      "validation loss = 7.663492779982717\n",
      "Batch 39 / 157\n",
      "training loss = 7.745040416717529\n",
      "validation loss = 7.645908907840126\n",
      "Batch 40 / 157\n",
      "training loss = 7.9393839836120605\n",
      "validation loss = 7.638881382189299\n",
      "Batch 41 / 157\n",
      "training loss = 7.637444496154785\n",
      "validation loss = 7.621886680000706\n",
      "Batch 42 / 157\n",
      "training loss = 7.586550235748291\n",
      "validation loss = 7.619349103224905\n",
      "Batch 43 / 157\n",
      "training loss = 7.647385597229004\n",
      "validation loss = 7.606507602490876\n",
      "Batch 44 / 157\n",
      "training loss = 7.445718765258789\n",
      "validation loss = 7.604969175238359\n",
      "Batch 45 / 157\n",
      "training loss = 7.6192193031311035\n",
      "validation loss = 7.599508862746389\n",
      "Batch 46 / 157\n",
      "training loss = 7.56181526184082\n",
      "validation loss = 7.600183838292172\n",
      "Batch 47 / 157\n",
      "training loss = 7.550073146820068\n",
      "validation loss = 7.593232129749499\n",
      "Batch 48 / 157\n",
      "training loss = 7.466519355773926\n",
      "validation loss = 7.590928127891139\n",
      "Batch 49 / 157\n",
      "training loss = 7.575801849365234\n",
      "validation loss = 7.5905475867422005\n",
      "Batch 50 / 157\n",
      "training loss = 7.54586124420166\n",
      "validation loss = 7.594646002116956\n",
      "Batch 51 / 157\n",
      "training loss = 7.430276393890381\n",
      "validation loss = 7.593378794820685\n",
      "Batch 52 / 157\n",
      "training loss = 7.902166843414307\n",
      "validation loss = 7.590580588892887\n",
      "Batch 53 / 157\n",
      "training loss = 7.582293510437012\n",
      "validation loss = 7.582981335489373\n",
      "Batch 54 / 157\n",
      "training loss = 7.723194122314453\n",
      "validation loss = 7.587265968322754\n",
      "Batch 55 / 157\n",
      "training loss = 7.572742462158203\n",
      "validation loss = 7.587640636845639\n",
      "Batch 56 / 157\n",
      "training loss = 7.439609050750732\n",
      "validation loss = 7.582211293672261\n",
      "Batch 57 / 157\n",
      "training loss = 7.531950950622559\n",
      "validation loss = 7.584252784126683\n",
      "Batch 58 / 157\n",
      "training loss = 7.59696626663208\n",
      "validation loss = 7.578442422967208\n",
      "Batch 59 / 157\n",
      "training loss = 7.517714023590088\n",
      "validation loss = 7.580472619909989\n",
      "Batch 60 / 157\n",
      "training loss = 7.789262771606445\n",
      "validation loss = 7.575770955336721\n",
      "Batch 61 / 157\n",
      "training loss = 7.2171711921691895\n",
      "validation loss = 7.58514057962518\n",
      "Batch 62 / 157\n",
      "training loss = 7.485193252563477\n",
      "validation loss = 7.575611792112651\n",
      "Batch 63 / 157\n",
      "training loss = 7.411334037780762\n",
      "validation loss = 7.577870544634368\n",
      "Batch 64 / 157\n",
      "training loss = 7.573432922363281\n",
      "validation loss = 7.577592674054597\n",
      "Batch 65 / 157\n",
      "training loss = 7.425624370574951\n",
      "validation loss = 7.571971868213854\n",
      "Batch 66 / 157\n",
      "training loss = 7.5444135665893555\n",
      "validation loss = 7.567071764092696\n",
      "Batch 67 / 157\n",
      "training loss = 7.297301769256592\n",
      "validation loss = 7.563358683335154\n",
      "Batch 68 / 157\n",
      "training loss = 7.462493896484375\n",
      "validation loss = 7.557590233652215\n",
      "Batch 69 / 157\n",
      "training loss = 7.42092227935791\n",
      "validation loss = 7.5538046234532406\n",
      "Batch 70 / 157\n",
      "training loss = 7.474222183227539\n",
      "validation loss = 7.547610307994642\n",
      "Batch 71 / 157\n",
      "training loss = 7.721944332122803\n",
      "validation loss = 7.543477585441188\n",
      "Batch 72 / 157\n",
      "training loss = 7.494364261627197\n",
      "validation loss = 7.54256178203382\n",
      "Batch 73 / 157\n",
      "training loss = 7.271450996398926\n",
      "validation loss = 7.544333759107087\n",
      "Batch 74 / 157\n",
      "training loss = 7.407684803009033\n",
      "validation loss = 7.532258134139211\n",
      "Batch 75 / 157\n",
      "training loss = 7.502847194671631\n",
      "validation loss = 7.5229464832105135\n",
      "Batch 76 / 157\n",
      "training loss = 8.11646842956543\n",
      "validation loss = 7.524416973716335\n",
      "Batch 77 / 157\n",
      "training loss = 7.590007781982422\n",
      "validation loss = 7.527249687596371\n",
      "Batch 78 / 157\n",
      "training loss = 7.304502487182617\n",
      "validation loss = 7.516919110950671\n",
      "Batch 79 / 157\n",
      "training loss = 7.410811424255371\n",
      "validation loss = 7.508690256821482\n",
      "Batch 80 / 157\n",
      "training loss = 7.305917739868164\n",
      "validation loss = 7.508507879156816\n",
      "Batch 81 / 157\n",
      "training loss = 7.2610650062561035\n",
      "validation loss = 7.502967006281803\n",
      "Batch 82 / 157\n",
      "training loss = 7.8299360275268555\n",
      "validation loss = 7.503289774844521\n",
      "Batch 83 / 157\n",
      "training loss = 7.393251895904541\n",
      "validation loss = 7.491133464010138\n",
      "Batch 84 / 157\n",
      "training loss = 7.519896030426025\n",
      "validation loss = 7.489174290707237\n",
      "Batch 85 / 157\n",
      "training loss = 7.452361583709717\n",
      "validation loss = 7.486329329641242\n",
      "Batch 86 / 157\n",
      "training loss = 7.18265962600708\n",
      "validation loss = 7.486479332572536\n",
      "Batch 87 / 157\n",
      "training loss = 7.6366190910339355\n",
      "validation loss = 7.483327464053505\n",
      "Batch 88 / 157\n",
      "training loss = 7.559657573699951\n",
      "validation loss = 7.478778537951018\n",
      "Batch 89 / 157\n",
      "training loss = 7.515155792236328\n",
      "validation loss = 7.471628364763762\n",
      "Batch 90 / 157\n",
      "training loss = 7.505949020385742\n",
      "validation loss = 7.467850333765933\n",
      "Batch 91 / 157\n",
      "training loss = 7.581228733062744\n",
      "validation loss = 7.464105857046027\n",
      "Batch 92 / 157\n",
      "training loss = 7.3094964027404785\n",
      "validation loss = 7.4528772203545826\n",
      "Batch 93 / 157\n",
      "training loss = 7.36640739440918\n",
      "validation loss = 7.446924862108733\n",
      "Batch 94 / 157\n",
      "training loss = 7.416675090789795\n",
      "validation loss = 7.446584124314158\n",
      "Batch 95 / 157\n",
      "training loss = 7.564255714416504\n",
      "validation loss = 7.450131416320801\n",
      "Batch 96 / 157\n",
      "training loss = 7.377763748168945\n",
      "validation loss = 7.4377643685591845\n",
      "Batch 97 / 157\n",
      "training loss = 7.417876243591309\n",
      "validation loss = 7.439168202249627\n",
      "Batch 98 / 157\n",
      "training loss = 7.391575813293457\n",
      "validation loss = 7.427647891797517\n",
      "Batch 99 / 157\n",
      "training loss = 7.434950351715088\n",
      "validation loss = 7.420250942832546\n",
      "Batch 100 / 157\n",
      "training loss = 7.483928203582764\n",
      "validation loss = 7.415280818939209\n",
      "Batch 101 / 157\n",
      "training loss = 7.319283962249756\n",
      "validation loss = 7.409393335643568\n",
      "Batch 102 / 157\n",
      "training loss = 7.278537273406982\n",
      "validation loss = 7.412058880454616\n",
      "Batch 103 / 157\n",
      "training loss = 7.473712921142578\n",
      "validation loss = 7.4124673793190405\n",
      "Batch 104 / 157\n",
      "training loss = 7.460204601287842\n",
      "validation loss = 7.404473430231998\n",
      "Batch 105 / 157\n",
      "training loss = 7.251527786254883\n",
      "validation loss = 7.398132951636064\n",
      "Batch 106 / 157\n",
      "training loss = 7.542093276977539\n",
      "validation loss = 7.3926189322220655\n",
      "Batch 107 / 157\n",
      "training loss = 7.43200159072876\n",
      "validation loss = 7.386929712797466\n",
      "Batch 108 / 157\n",
      "training loss = 7.378841400146484\n",
      "validation loss = 7.392251240579705\n",
      "Batch 109 / 157\n",
      "training loss = 7.3719305992126465\n",
      "validation loss = 7.374193266818398\n",
      "Batch 110 / 157\n",
      "training loss = 7.3628950119018555\n",
      "validation loss = 7.374632233067563\n",
      "Batch 111 / 157\n",
      "training loss = 7.446132183074951\n",
      "validation loss = 7.376636730997186\n",
      "Batch 112 / 157\n",
      "training loss = 7.339579105377197\n",
      "validation loss = 7.369202488347104\n",
      "Batch 113 / 157\n",
      "training loss = 7.272799968719482\n",
      "validation loss = 7.3659687293203255\n",
      "Batch 114 / 157\n",
      "training loss = 7.25782585144043\n",
      "validation loss = 7.362193709925601\n",
      "Batch 115 / 157\n",
      "training loss = 7.502284526824951\n",
      "validation loss = 7.3622567277205615\n",
      "Batch 116 / 157\n",
      "training loss = 7.11444616317749\n",
      "validation loss = 7.36014431401303\n",
      "Batch 117 / 157\n",
      "training loss = 7.374292850494385\n",
      "validation loss = 7.361876362248471\n",
      "Batch 118 / 157\n",
      "training loss = 7.353814125061035\n",
      "validation loss = 7.350536798176012\n",
      "Batch 119 / 157\n",
      "training loss = 7.395875453948975\n",
      "validation loss = 7.350241209331312\n",
      "Batch 120 / 157\n",
      "training loss = 7.425243377685547\n",
      "validation loss = 7.351928635647423\n",
      "Batch 121 / 157\n",
      "training loss = 7.357141494750977\n",
      "validation loss = 7.3462627310501905\n",
      "Batch 122 / 157\n",
      "training loss = 7.161535263061523\n",
      "validation loss = 7.342288970947266\n",
      "Batch 123 / 157\n",
      "training loss = 7.321792125701904\n",
      "validation loss = 7.33514068001195\n",
      "Batch 124 / 157\n",
      "training loss = 7.290146827697754\n",
      "validation loss = 7.326644495913857\n",
      "Batch 125 / 157\n",
      "training loss = 6.939835071563721\n",
      "validation loss = 7.332826413606343\n",
      "Batch 126 / 157\n",
      "training loss = 7.407475471496582\n",
      "validation loss = 7.333206954755281\n",
      "Batch 127 / 157\n",
      "training loss = 7.205446720123291\n",
      "validation loss = 7.324326841454757\n",
      "Batch 128 / 157\n",
      "training loss = 7.155011177062988\n",
      "validation loss = 7.321096144224468\n",
      "Batch 129 / 157\n",
      "training loss = 7.4832844734191895\n",
      "validation loss = 7.318727192125823\n",
      "Batch 130 / 157\n",
      "training loss = 7.535618305206299\n",
      "validation loss = 7.31004197973954\n",
      "Batch 131 / 157\n",
      "training loss = 7.5166239738464355\n",
      "validation loss = 7.310284263209293\n",
      "Batch 132 / 157\n",
      "training loss = 7.400066375732422\n",
      "validation loss = 7.310397976323178\n",
      "Batch 133 / 157\n",
      "training loss = 7.55220365524292\n",
      "validation loss = 7.302797116731343\n",
      "Batch 134 / 157\n",
      "training loss = 7.292703628540039\n",
      "validation loss = 7.302030362580952\n",
      "Batch 135 / 157\n",
      "training loss = 7.285606861114502\n",
      "validation loss = 7.295470062055085\n",
      "Batch 136 / 157\n",
      "training loss = 7.205746650695801\n",
      "validation loss = 7.291822609148528\n",
      "Batch 137 / 157\n",
      "training loss = 7.207724094390869\n",
      "validation loss = 7.2894054964969035\n",
      "Batch 138 / 157\n",
      "training loss = 6.971604824066162\n",
      "validation loss = 7.283039494564659\n",
      "Batch 139 / 157\n",
      "training loss = 7.466608047485352\n",
      "validation loss = 7.27853694714998\n",
      "Batch 140 / 157\n",
      "training loss = 7.287247180938721\n",
      "validation loss = 7.280276674973337\n",
      "Batch 141 / 157\n",
      "training loss = 7.011605262756348\n",
      "validation loss = 7.272327322708933\n",
      "Batch 142 / 157\n",
      "training loss = 7.379810810089111\n",
      "validation loss = 7.273590012600548\n",
      "Batch 143 / 157\n",
      "training loss = 7.214054107666016\n",
      "validation loss = 7.266798195086028\n",
      "Batch 144 / 157\n",
      "training loss = 7.197320461273193\n",
      "validation loss = 7.269825458526611\n",
      "Batch 145 / 157\n",
      "training loss = 7.267022609710693\n",
      "validation loss = 7.2678624705264445\n",
      "Batch 146 / 157\n",
      "training loss = 7.150453567504883\n",
      "validation loss = 7.260936134739926\n",
      "Batch 147 / 157\n",
      "training loss = 7.300728797912598\n",
      "validation loss = 7.252341772380628\n",
      "Batch 148 / 157\n",
      "training loss = 7.19028377532959\n",
      "validation loss = 7.24733977568777\n",
      "Batch 149 / 157\n",
      "training loss = 7.297055244445801\n",
      "validation loss = 7.255816660429302\n",
      "Batch 150 / 157\n",
      "training loss = 7.131420135498047\n",
      "validation loss = 7.24765682220459\n",
      "Batch 151 / 157\n",
      "training loss = 7.369740009307861\n",
      "validation loss = 7.240851352089329\n",
      "Batch 152 / 157\n",
      "training loss = 7.226729393005371\n",
      "validation loss = 7.244329201547723\n",
      "Batch 153 / 157\n",
      "training loss = 7.461512088775635\n",
      "validation loss = 7.239938610478451\n",
      "Batch 154 / 157\n",
      "training loss = 7.202536106109619\n",
      "validation loss = 7.23409336491635\n",
      "Batch 155 / 157\n",
      "training loss = 7.301914215087891\n",
      "validation loss = 7.234216564580014\n",
      "Batch 156 / 157\n",
      "training loss = 7.552354335784912\n",
      "validation loss = 7.235700381429572\n",
      "Batch 157 / 157\n",
      "training loss = 7.4282121658325195\n",
      "validation loss = 7.2393973250138135\n",
      "Average training loss: 7.73\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.381714820861816\n",
      "validation loss = 10.332317402488307\n",
      "Batch 2 / 157\n",
      "training loss = 10.327352523803711\n",
      "validation loss = 10.270041164598966\n",
      "Batch 3 / 157\n",
      "training loss = 10.266953468322754\n",
      "validation loss = 10.182736446982936\n",
      "Batch 4 / 157\n",
      "training loss = 10.191041946411133\n",
      "validation loss = 10.082265552721525\n",
      "Batch 5 / 157\n",
      "training loss = 10.132519721984863\n",
      "validation loss = 9.97750136726781\n",
      "Batch 6 / 157\n",
      "training loss = 9.9208402633667\n",
      "validation loss = 9.868565258226896\n",
      "Batch 7 / 157\n",
      "training loss = 9.906218528747559\n",
      "validation loss = 9.757588687695955\n",
      "Batch 8 / 157\n",
      "training loss = 9.764108657836914\n",
      "validation loss = 9.645466302570544\n",
      "Batch 9 / 157\n",
      "training loss = 9.69736099243164\n",
      "validation loss = 9.531176818044562\n",
      "Batch 10 / 157\n",
      "training loss = 9.500723838806152\n",
      "validation loss = 9.42202743731047\n",
      "Batch 11 / 157\n",
      "training loss = 9.417854309082031\n",
      "validation loss = 9.309456875449733\n",
      "Batch 12 / 157\n",
      "training loss = 9.24947738647461\n",
      "validation loss = 9.200934309708444\n",
      "Batch 13 / 157\n",
      "training loss = 9.165574073791504\n",
      "validation loss = 9.092667780424419\n",
      "Batch 14 / 157\n",
      "training loss = 8.923670768737793\n",
      "validation loss = 8.988037109375\n",
      "Batch 15 / 157\n",
      "training loss = 9.05400562286377\n",
      "validation loss = 8.886927303514982\n",
      "Batch 16 / 157\n",
      "training loss = 8.903730392456055\n",
      "validation loss = 8.785483761837607\n",
      "Batch 17 / 157\n",
      "training loss = 8.711763381958008\n",
      "validation loss = 8.693610542698911\n",
      "Batch 18 / 157\n",
      "training loss = 8.804784774780273\n",
      "validation loss = 8.599759754381681\n",
      "Batch 19 / 157\n",
      "training loss = 8.537603378295898\n",
      "validation loss = 8.51247411025198\n",
      "Batch 20 / 157\n",
      "training loss = 8.398619651794434\n",
      "validation loss = 8.433783029255114\n",
      "Batch 21 / 157\n",
      "training loss = 8.672043800354004\n",
      "validation loss = 8.354273545114618\n",
      "Batch 22 / 157\n",
      "training loss = 8.29823112487793\n",
      "validation loss = 8.289245906629061\n",
      "Batch 23 / 157\n",
      "training loss = 8.291776657104492\n",
      "validation loss = 8.218843711049933\n",
      "Batch 24 / 157\n",
      "training loss = 8.153702735900879\n",
      "validation loss = 8.16179441150866\n",
      "Batch 25 / 157\n",
      "training loss = 8.141357421875\n",
      "validation loss = 8.10660141392758\n",
      "Batch 26 / 157\n",
      "training loss = 8.161299705505371\n",
      "validation loss = 8.053055989114862\n",
      "Batch 27 / 157\n",
      "training loss = 7.671697616577148\n",
      "validation loss = 8.013939807289525\n",
      "Batch 28 / 157\n",
      "training loss = 8.061810493469238\n",
      "validation loss = 7.974600390384071\n",
      "Batch 29 / 157\n",
      "training loss = 8.04032039642334\n",
      "validation loss = 7.941357938866866\n",
      "Batch 30 / 157\n",
      "training loss = 7.824339866638184\n",
      "validation loss = 7.902958117033306\n",
      "Batch 31 / 157\n",
      "training loss = 8.092510223388672\n",
      "validation loss = 7.884249436227899\n",
      "Batch 32 / 157\n",
      "training loss = 8.00991439819336\n",
      "validation loss = 7.861297983872263\n",
      "Batch 33 / 157\n",
      "training loss = 7.794153213500977\n",
      "validation loss = 7.839125382272821\n",
      "Batch 34 / 157\n",
      "training loss = 7.805620193481445\n",
      "validation loss = 7.824503045333059\n",
      "Batch 35 / 157\n",
      "training loss = 7.839699745178223\n",
      "validation loss = 7.815112289629485\n",
      "Batch 36 / 157\n",
      "training loss = 7.731906890869141\n",
      "validation loss = 7.804407872651753\n",
      "Batch 37 / 157\n",
      "training loss = 8.23011589050293\n",
      "validation loss = 7.797821797822651\n",
      "Batch 38 / 157\n",
      "training loss = 8.012014389038086\n",
      "validation loss = 7.792103064687629\n",
      "Batch 39 / 157\n",
      "training loss = 8.117987632751465\n",
      "validation loss = 7.783597469329834\n",
      "Batch 40 / 157\n",
      "training loss = 7.412230491638184\n",
      "validation loss = 7.778989791870117\n",
      "Batch 41 / 157\n",
      "training loss = 7.805051326751709\n",
      "validation loss = 7.766349666997006\n",
      "Batch 42 / 157\n",
      "training loss = 8.098851203918457\n",
      "validation loss = 7.764306846417878\n",
      "Batch 43 / 157\n",
      "training loss = 7.943327903747559\n",
      "validation loss = 7.747339675300999\n",
      "Batch 44 / 157\n",
      "training loss = 8.031103134155273\n",
      "validation loss = 7.74565295169228\n",
      "Batch 45 / 157\n",
      "training loss = 7.821290493011475\n",
      "validation loss = 7.7325081072355575\n",
      "Batch 46 / 157\n",
      "training loss = 7.679661273956299\n",
      "validation loss = 7.737085091440301\n",
      "Batch 47 / 157\n",
      "training loss = 7.47611141204834\n",
      "validation loss = 7.727369333568372\n",
      "Batch 48 / 157\n",
      "training loss = 7.429966449737549\n",
      "validation loss = 7.71685424603914\n",
      "Batch 49 / 157\n",
      "training loss = 8.032561302185059\n",
      "validation loss = 7.71431857661197\n",
      "Batch 50 / 157\n",
      "training loss = 7.630937099456787\n",
      "validation loss = 7.710010879918149\n",
      "Batch 51 / 157\n",
      "training loss = 7.809508323669434\n",
      "validation loss = 7.706966149179559\n",
      "Batch 52 / 157\n",
      "training loss = 8.12076187133789\n",
      "validation loss = 7.70619713632684\n",
      "Batch 53 / 157\n",
      "training loss = 7.67234992980957\n",
      "validation loss = 7.700268770519056\n",
      "Batch 54 / 157\n",
      "training loss = 7.631219387054443\n",
      "validation loss = 7.698107192390843\n",
      "Batch 55 / 157\n",
      "training loss = 7.583496570587158\n",
      "validation loss = 7.69013934386404\n",
      "Batch 56 / 157\n",
      "training loss = 7.745100498199463\n",
      "validation loss = 7.689299909692061\n",
      "Batch 57 / 157\n",
      "training loss = 7.828184127807617\n",
      "validation loss = 7.6809327978836865\n",
      "Batch 58 / 157\n",
      "training loss = 7.6519694328308105\n",
      "validation loss = 7.6785053202980444\n",
      "Batch 59 / 157\n",
      "training loss = 7.829483509063721\n",
      "validation loss = 7.673867802870901\n",
      "Batch 60 / 157\n",
      "training loss = 7.622376918792725\n",
      "validation loss = 7.673216393119411\n",
      "Batch 61 / 157\n",
      "training loss = 7.3462748527526855\n",
      "validation loss = 7.667247270282946\n",
      "Batch 62 / 157\n",
      "training loss = 7.972584247589111\n",
      "validation loss = 7.668892960799368\n",
      "Batch 63 / 157\n",
      "training loss = 7.622340679168701\n",
      "validation loss = 7.663381275377776\n",
      "Batch 64 / 157\n",
      "training loss = 7.723348617553711\n",
      "validation loss = 7.656085089633339\n",
      "Batch 65 / 157\n",
      "training loss = 7.859823703765869\n",
      "validation loss = 7.652592458223042\n",
      "Batch 66 / 157\n",
      "training loss = 7.65555477142334\n",
      "validation loss = 7.654275768681576\n",
      "Batch 67 / 157\n",
      "training loss = 7.757321357727051\n",
      "validation loss = 7.647252835725483\n",
      "Batch 68 / 157\n",
      "training loss = 7.810572624206543\n",
      "validation loss = 7.649542181115401\n",
      "Batch 69 / 157\n",
      "training loss = 7.57395076751709\n",
      "validation loss = 7.642589443608334\n",
      "Batch 70 / 157\n",
      "training loss = 7.283090591430664\n",
      "validation loss = 7.632722854614258\n",
      "Batch 71 / 157\n",
      "training loss = 7.638235092163086\n",
      "validation loss = 7.63853100726479\n",
      "Batch 72 / 157\n",
      "training loss = 7.48685359954834\n",
      "validation loss = 7.634124228828831\n",
      "Batch 73 / 157\n",
      "training loss = 7.710486888885498\n",
      "validation loss = 7.630760895578485\n",
      "Batch 74 / 157\n",
      "training loss = 7.463156700134277\n",
      "validation loss = 7.627159495102732\n",
      "Batch 75 / 157\n",
      "training loss = 7.573206901550293\n",
      "validation loss = 7.61836586500469\n",
      "Batch 76 / 157\n",
      "training loss = 7.462262153625488\n",
      "validation loss = 7.618305206298828\n",
      "Batch 77 / 157\n",
      "training loss = 7.554131031036377\n",
      "validation loss = 7.613736453809236\n",
      "Batch 78 / 157\n",
      "training loss = 7.606231689453125\n",
      "validation loss = 7.614817544033653\n",
      "Batch 79 / 157\n",
      "training loss = 7.623308181762695\n",
      "validation loss = 7.611361629084537\n",
      "Batch 80 / 157\n",
      "training loss = 7.573903560638428\n",
      "validation loss = 7.604374985945852\n",
      "Batch 81 / 157\n",
      "training loss = 7.5308661460876465\n",
      "validation loss = 7.59227022371794\n",
      "Batch 82 / 157\n",
      "training loss = 7.870509147644043\n",
      "validation loss = 7.603732134166517\n",
      "Batch 83 / 157\n",
      "training loss = 7.697603225708008\n",
      "validation loss = 7.615654343052914\n",
      "Batch 84 / 157\n",
      "training loss = 7.448378086090088\n",
      "validation loss = 7.590137130335758\n",
      "Batch 85 / 157\n",
      "training loss = 7.822115898132324\n",
      "validation loss = 7.579637125918739\n",
      "Batch 86 / 157\n",
      "training loss = 7.7741827964782715\n",
      "validation loss = 7.581140166834781\n",
      "Batch 87 / 157\n",
      "training loss = 7.311992645263672\n",
      "validation loss = 7.58351469039917\n",
      "Batch 88 / 157\n",
      "training loss = 7.363918304443359\n",
      "validation loss = 7.581181250120464\n",
      "Batch 89 / 157\n",
      "training loss = 7.5225348472595215\n",
      "validation loss = 7.578782533344469\n",
      "Batch 90 / 157\n",
      "training loss = 7.542527675628662\n",
      "validation loss = 7.5588811824196265\n",
      "Batch 91 / 157\n",
      "training loss = 7.378771781921387\n",
      "validation loss = 7.5555008838051245\n",
      "Batch 92 / 157\n",
      "training loss = 7.322948932647705\n",
      "validation loss = 7.552495856034128\n",
      "Batch 93 / 157\n",
      "training loss = 7.291429042816162\n",
      "validation loss = 7.550580576846474\n",
      "Batch 94 / 157\n",
      "training loss = 7.389467239379883\n",
      "validation loss = 7.54623922548796\n",
      "Batch 95 / 157\n",
      "training loss = 7.066819667816162\n",
      "validation loss = 7.541825520364862\n",
      "Batch 96 / 157\n",
      "training loss = 7.689029693603516\n",
      "validation loss = 7.545095443725586\n",
      "Batch 97 / 157\n",
      "training loss = 7.418050765991211\n",
      "validation loss = 7.540331037420976\n",
      "Batch 98 / 157\n",
      "training loss = 7.6880879402160645\n",
      "validation loss = 7.527839384580913\n",
      "Batch 99 / 157\n",
      "training loss = 7.506896018981934\n",
      "validation loss = 7.524115888695968\n",
      "Batch 100 / 157\n",
      "training loss = 7.398427486419678\n",
      "validation loss = 7.515329662122224\n",
      "Batch 101 / 157\n",
      "training loss = 7.908004283905029\n",
      "validation loss = 7.508248228775828\n",
      "Batch 102 / 157\n",
      "training loss = 7.576112747192383\n",
      "validation loss = 7.503094999413741\n",
      "Batch 103 / 157\n",
      "training loss = 7.483894348144531\n",
      "validation loss = 7.504519086135061\n",
      "Batch 104 / 157\n",
      "training loss = 7.652812957763672\n",
      "validation loss = 7.505658450879548\n",
      "Batch 105 / 157\n",
      "training loss = 7.466116905212402\n",
      "validation loss = 7.493730695624101\n",
      "Batch 106 / 157\n",
      "training loss = 7.713042259216309\n",
      "validation loss = 7.485565210643568\n",
      "Batch 107 / 157\n",
      "training loss = 7.207918167114258\n",
      "validation loss = 7.484778153268914\n",
      "Batch 108 / 157\n",
      "training loss = 7.382569789886475\n",
      "validation loss = 7.478557109832764\n",
      "Batch 109 / 157\n",
      "training loss = 7.422091484069824\n",
      "validation loss = 7.469493916160182\n",
      "Batch 110 / 157\n",
      "training loss = 7.414724349975586\n",
      "validation loss = 7.4673284480446265\n",
      "Batch 111 / 157\n",
      "training loss = 7.416843891143799\n",
      "validation loss = 7.463631077816612\n",
      "Batch 112 / 157\n",
      "training loss = 7.552199363708496\n",
      "validation loss = 7.459533114182322\n",
      "Batch 113 / 157\n",
      "training loss = 7.268523693084717\n",
      "validation loss = 7.450575753262169\n",
      "Batch 114 / 157\n",
      "training loss = 7.219765663146973\n",
      "validation loss = 7.4558696495859245\n",
      "Batch 115 / 157\n",
      "training loss = 7.376842975616455\n",
      "validation loss = 7.448880998711837\n",
      "Batch 116 / 157\n",
      "training loss = 7.5829949378967285\n",
      "validation loss = 7.432246082707455\n",
      "Batch 117 / 157\n",
      "training loss = 7.555359840393066\n",
      "validation loss = 7.434964581539757\n",
      "Batch 118 / 157\n",
      "training loss = 7.093373775482178\n",
      "validation loss = 7.423220057236521\n",
      "Batch 119 / 157\n",
      "training loss = 7.4525556564331055\n",
      "validation loss = 7.408519343325966\n",
      "Batch 120 / 157\n",
      "training loss = 7.365460395812988\n",
      "validation loss = 7.40614158228824\n",
      "Batch 121 / 157\n",
      "training loss = 7.2135701179504395\n",
      "validation loss = 7.400228023529053\n",
      "Batch 122 / 157\n",
      "training loss = 7.631787300109863\n",
      "validation loss = 7.3985796978599145\n",
      "Batch 123 / 157\n",
      "training loss = 7.734396934509277\n",
      "validation loss = 7.394393795414975\n",
      "Batch 124 / 157\n",
      "training loss = 7.343369007110596\n",
      "validation loss = 7.38640032316509\n",
      "Batch 125 / 157\n",
      "training loss = 7.3722734451293945\n",
      "validation loss = 7.378912549269827\n",
      "Batch 126 / 157\n",
      "training loss = 7.393954753875732\n",
      "validation loss = 7.370732332530775\n",
      "Batch 127 / 157\n",
      "training loss = 7.509812831878662\n",
      "validation loss = 7.3699517250061035\n",
      "Batch 128 / 157\n",
      "training loss = 7.603382587432861\n",
      "validation loss = 7.368209889060573\n",
      "Batch 129 / 157\n",
      "training loss = 7.2480926513671875\n",
      "validation loss = 7.35740307757729\n",
      "Batch 130 / 157\n",
      "training loss = 6.953311443328857\n",
      "validation loss = 7.355958361374705\n",
      "Batch 131 / 157\n",
      "training loss = 7.54479455947876\n",
      "validation loss = 7.351965301915219\n",
      "Batch 132 / 157\n",
      "training loss = 7.163809299468994\n",
      "validation loss = 7.344163217042622\n",
      "Batch 133 / 157\n",
      "training loss = 7.197619438171387\n",
      "validation loss = 7.33806007786801\n",
      "Batch 134 / 157\n",
      "training loss = 7.14677619934082\n",
      "validation loss = 7.330186818775378\n",
      "Batch 135 / 157\n",
      "training loss = 7.2969136238098145\n",
      "validation loss = 7.3342799638447005\n",
      "Batch 136 / 157\n",
      "training loss = 7.480233192443848\n",
      "validation loss = 7.331744194030762\n",
      "Batch 137 / 157\n",
      "training loss = 6.9572672843933105\n",
      "validation loss = 7.320267451436896\n",
      "Batch 138 / 157\n",
      "training loss = 6.742434501647949\n",
      "validation loss = 7.327860004023502\n",
      "Batch 139 / 157\n",
      "training loss = 7.576888561248779\n",
      "validation loss = 7.3146037804452995\n",
      "Batch 140 / 157\n",
      "training loss = 7.701655387878418\n",
      "validation loss = 7.321498318722374\n",
      "Batch 141 / 157\n",
      "training loss = 7.324667930603027\n",
      "validation loss = 7.319530110610159\n",
      "Batch 142 / 157\n",
      "training loss = 7.400084495544434\n",
      "validation loss = 7.323573589324951\n",
      "Batch 143 / 157\n",
      "training loss = 7.656912326812744\n",
      "validation loss = 7.3114989431280835\n",
      "Batch 144 / 157\n",
      "training loss = 7.127732276916504\n",
      "validation loss = 7.307142182400352\n",
      "Batch 145 / 157\n",
      "training loss = 7.075094699859619\n",
      "validation loss = 7.299893354114733\n",
      "Batch 146 / 157\n",
      "training loss = 7.27462100982666\n",
      "validation loss = 7.298218300468044\n",
      "Batch 147 / 157\n",
      "training loss = 7.100769996643066\n",
      "validation loss = 7.292291967492354\n",
      "Batch 148 / 157\n",
      "training loss = 7.023448944091797\n",
      "validation loss = 7.295827840503893\n",
      "Batch 149 / 157\n",
      "training loss = 7.424454212188721\n",
      "validation loss = 7.292679611005281\n",
      "Batch 150 / 157\n",
      "training loss = 7.524824142456055\n",
      "validation loss = 7.28387619319715\n",
      "Batch 151 / 157\n",
      "training loss = 7.284046173095703\n",
      "validation loss = 7.276845932006836\n",
      "Batch 152 / 157\n",
      "training loss = 7.542664527893066\n",
      "validation loss = 7.282155287893195\n",
      "Batch 153 / 157\n",
      "training loss = 7.440009593963623\n",
      "validation loss = 7.279920954453318\n",
      "Batch 154 / 157\n",
      "training loss = 6.8926215171813965\n",
      "validation loss = 7.276661094866301\n",
      "Batch 155 / 157\n",
      "training loss = 7.197307109832764\n",
      "validation loss = 7.289759133991442\n",
      "Batch 156 / 157\n",
      "training loss = 7.671301364898682\n",
      "validation loss = 7.273786544799805\n",
      "Batch 157 / 157\n",
      "training loss = 7.797299861907959\n",
      "validation loss = 7.267137025531969\n",
      "Average training loss: 7.82\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.370965957641602\n",
      "validation loss = 10.329009407445005\n",
      "Batch 2 / 157\n",
      "training loss = 10.326850891113281\n",
      "validation loss = 10.23950421182733\n",
      "Batch 3 / 157\n",
      "training loss = 10.245623588562012\n",
      "validation loss = 10.127420676381965\n",
      "Batch 4 / 157\n",
      "training loss = 10.136699676513672\n",
      "validation loss = 10.010891010886745\n",
      "Batch 5 / 157\n",
      "training loss = 10.01423454284668\n",
      "validation loss = 9.892944084970575\n",
      "Batch 6 / 157\n",
      "training loss = 9.901591300964355\n",
      "validation loss = 9.774723554912367\n",
      "Batch 7 / 157\n",
      "training loss = 9.795204162597656\n",
      "validation loss = 9.655450218602232\n",
      "Batch 8 / 157\n",
      "training loss = 9.664457321166992\n",
      "validation loss = 9.535218289023952\n",
      "Batch 9 / 157\n",
      "training loss = 9.510211944580078\n",
      "validation loss = 9.41740517867239\n",
      "Batch 10 / 157\n",
      "training loss = 9.447729110717773\n",
      "validation loss = 9.302040501644736\n",
      "Batch 11 / 157\n",
      "training loss = 9.312119483947754\n",
      "validation loss = 9.190585337187114\n",
      "Batch 12 / 157\n",
      "training loss = 9.216429710388184\n",
      "validation loss = 9.080161847566304\n",
      "Batch 13 / 157\n",
      "training loss = 9.049555778503418\n",
      "validation loss = 8.973841817755448\n",
      "Batch 14 / 157\n",
      "training loss = 8.886578559875488\n",
      "validation loss = 8.874418911180998\n",
      "Batch 15 / 157\n",
      "training loss = 8.949564933776855\n",
      "validation loss = 8.775643599660773\n",
      "Batch 16 / 157\n",
      "training loss = 8.752846717834473\n",
      "validation loss = 8.683694036383377\n",
      "Batch 17 / 157\n",
      "training loss = 8.740164756774902\n",
      "validation loss = 8.594526893214175\n",
      "Batch 18 / 157\n",
      "training loss = 8.529232025146484\n",
      "validation loss = 8.510373918633713\n",
      "Batch 19 / 157\n",
      "training loss = 8.52389907836914\n",
      "validation loss = 8.433256099098607\n",
      "Batch 20 / 157\n",
      "training loss = 8.487783432006836\n",
      "validation loss = 8.356700847023411\n",
      "Batch 21 / 157\n",
      "training loss = 8.321823120117188\n",
      "validation loss = 8.292009905764932\n",
      "Batch 22 / 157\n",
      "training loss = 8.25743579864502\n",
      "validation loss = 8.223261130483527\n",
      "Batch 23 / 157\n",
      "training loss = 8.306644439697266\n",
      "validation loss = 8.16173357712595\n",
      "Batch 24 / 157\n",
      "training loss = 8.229476928710938\n",
      "validation loss = 8.112025762859144\n",
      "Batch 25 / 157\n",
      "training loss = 7.960765838623047\n",
      "validation loss = 8.054321464739347\n",
      "Batch 26 / 157\n",
      "training loss = 8.192416191101074\n",
      "validation loss = 8.005725258275083\n",
      "Batch 27 / 157\n",
      "training loss = 8.062758445739746\n",
      "validation loss = 7.961270984850432\n",
      "Batch 28 / 157\n",
      "training loss = 8.127279281616211\n",
      "validation loss = 7.922325661307887\n",
      "Batch 29 / 157\n",
      "training loss = 7.949023723602295\n",
      "validation loss = 7.88059515702097\n",
      "Batch 30 / 157\n",
      "training loss = 7.977085590362549\n",
      "validation loss = 7.849989062861392\n",
      "Batch 31 / 157\n",
      "training loss = 7.735054016113281\n",
      "validation loss = 7.816697195956581\n",
      "Batch 32 / 157\n",
      "training loss = 7.733338832855225\n",
      "validation loss = 7.784740548384817\n",
      "Batch 33 / 157\n",
      "training loss = 7.65338134765625\n",
      "validation loss = 7.766998692562706\n",
      "Batch 34 / 157\n",
      "training loss = 7.64207649230957\n",
      "validation loss = 7.74543749658685\n",
      "Batch 35 / 157\n",
      "training loss = 7.981459617614746\n",
      "validation loss = 7.726323930840743\n",
      "Batch 36 / 157\n",
      "training loss = 7.964687347412109\n",
      "validation loss = 7.712145981035735\n",
      "Batch 37 / 157\n",
      "training loss = 7.920238494873047\n",
      "validation loss = 7.700046288339715\n",
      "Batch 38 / 157\n",
      "training loss = 7.772886276245117\n",
      "validation loss = 7.687931387048018\n",
      "Batch 39 / 157\n",
      "training loss = 7.697569847106934\n",
      "validation loss = 7.683128105966668\n",
      "Batch 40 / 157\n",
      "training loss = 7.784369468688965\n",
      "validation loss = 7.673305787538228\n",
      "Batch 41 / 157\n",
      "training loss = 7.768519878387451\n",
      "validation loss = 7.667948070325349\n",
      "Batch 42 / 157\n",
      "training loss = 7.699661731719971\n",
      "validation loss = 7.6581088367261385\n",
      "Batch 43 / 157\n",
      "training loss = 8.004623413085938\n",
      "validation loss = 7.653352310782985\n",
      "Batch 44 / 157\n",
      "training loss = 7.558407783508301\n",
      "validation loss = 7.65365269309596\n",
      "Batch 45 / 157\n",
      "training loss = 7.751400470733643\n",
      "validation loss = 7.642427318974545\n",
      "Batch 46 / 157\n",
      "training loss = 7.575345039367676\n",
      "validation loss = 7.6353943222447445\n",
      "Batch 47 / 157\n",
      "training loss = 7.496998310089111\n",
      "validation loss = 7.634805854998137\n",
      "Batch 48 / 157\n",
      "training loss = 7.664845943450928\n",
      "validation loss = 7.639331365886488\n",
      "Batch 49 / 157\n",
      "training loss = 7.522355556488037\n",
      "validation loss = 7.633109519356175\n",
      "Batch 50 / 157\n",
      "training loss = 7.446048259735107\n",
      "validation loss = 7.62674627806011\n",
      "Batch 51 / 157\n",
      "training loss = 7.5341033935546875\n",
      "validation loss = 7.6290029224596525\n",
      "Batch 52 / 157\n",
      "training loss = 7.477451324462891\n",
      "validation loss = 7.626317902615196\n",
      "Batch 53 / 157\n",
      "training loss = 7.57424259185791\n",
      "validation loss = 7.627658517737138\n",
      "Batch 54 / 157\n",
      "training loss = 7.696713924407959\n",
      "validation loss = 7.62325936869571\n",
      "Batch 55 / 157\n",
      "training loss = 7.422600746154785\n",
      "validation loss = 7.626854720868562\n",
      "Batch 56 / 157\n",
      "training loss = 7.556148052215576\n",
      "validation loss = 7.623664705376876\n",
      "Batch 57 / 157\n",
      "training loss = 7.482138633728027\n",
      "validation loss = 7.6177521755820825\n",
      "Batch 58 / 157\n",
      "training loss = 7.513815402984619\n",
      "validation loss = 7.618484522167005\n",
      "Batch 59 / 157\n",
      "training loss = 7.45863151550293\n",
      "validation loss = 7.613308279137862\n",
      "Batch 60 / 157\n",
      "training loss = 7.5854973793029785\n",
      "validation loss = 7.608352184295654\n",
      "Batch 61 / 157\n",
      "training loss = 7.577861785888672\n",
      "validation loss = 7.610176186812551\n",
      "Batch 62 / 157\n",
      "training loss = 7.833645820617676\n",
      "validation loss = 7.610793515255577\n",
      "Batch 63 / 157\n",
      "training loss = 7.572390079498291\n",
      "validation loss = 7.605760348470588\n",
      "Batch 64 / 157\n",
      "training loss = 7.80033016204834\n",
      "validation loss = 7.6069863219010205\n",
      "Batch 65 / 157\n",
      "training loss = 7.790914058685303\n",
      "validation loss = 7.60640980068006\n",
      "Batch 66 / 157\n",
      "training loss = 7.468149185180664\n",
      "validation loss = 7.5989721448797924\n",
      "Batch 67 / 157\n",
      "training loss = 8.040515899658203\n",
      "validation loss = 7.599524472889147\n",
      "Batch 68 / 157\n",
      "training loss = 7.808374881744385\n",
      "validation loss = 7.59460825669138\n",
      "Batch 69 / 157\n",
      "training loss = 7.5200886726379395\n",
      "validation loss = 7.590601444244385\n",
      "Batch 70 / 157\n",
      "training loss = 7.615362644195557\n",
      "validation loss = 7.58931649358649\n",
      "Batch 71 / 157\n",
      "training loss = 7.562178134918213\n",
      "validation loss = 7.589046327691329\n",
      "Batch 72 / 157\n",
      "training loss = 7.562851428985596\n",
      "validation loss = 7.5800604820251465\n",
      "Batch 73 / 157\n",
      "training loss = 7.505135536193848\n",
      "validation loss = 7.581359336250706\n",
      "Batch 74 / 157\n",
      "training loss = 7.358798027038574\n",
      "validation loss = 7.578486668436151\n",
      "Batch 75 / 157\n",
      "training loss = 7.408764362335205\n",
      "validation loss = 7.574123307278282\n",
      "Batch 76 / 157\n",
      "training loss = 7.607741355895996\n",
      "validation loss = 7.572382425007067\n",
      "Batch 77 / 157\n",
      "training loss = 7.522282123565674\n",
      "validation loss = 7.565541041524787\n",
      "Batch 78 / 157\n",
      "training loss = 7.848462104797363\n",
      "validation loss = 7.564466627020585\n",
      "Batch 79 / 157\n",
      "training loss = 7.600866794586182\n",
      "validation loss = 7.558284157200863\n",
      "Batch 80 / 157\n",
      "training loss = 7.57492733001709\n",
      "validation loss = 7.557811435900237\n",
      "Batch 81 / 157\n",
      "training loss = 7.583324432373047\n",
      "validation loss = 7.556745554271497\n",
      "Batch 82 / 157\n",
      "training loss = 7.718113422393799\n",
      "validation loss = 7.553603498559249\n",
      "Batch 83 / 157\n",
      "training loss = 7.885089874267578\n",
      "validation loss = 7.554009312077572\n",
      "Batch 84 / 157\n",
      "training loss = 7.585381984710693\n",
      "validation loss = 7.5494711273594906\n",
      "Batch 85 / 157\n",
      "training loss = 7.741053581237793\n",
      "validation loss = 7.542627334594727\n",
      "Batch 86 / 157\n",
      "training loss = 7.72629451751709\n",
      "validation loss = 7.538422032406456\n",
      "Batch 87 / 157\n",
      "training loss = 7.5422682762146\n",
      "validation loss = 7.534043788909912\n",
      "Batch 88 / 157\n",
      "training loss = 7.373549938201904\n",
      "validation loss = 7.531724051425331\n",
      "Batch 89 / 157\n",
      "training loss = 7.67039155960083\n",
      "validation loss = 7.530195110722592\n",
      "Batch 90 / 157\n",
      "training loss = 7.420782566070557\n",
      "validation loss = 7.526078550439132\n",
      "Batch 91 / 157\n",
      "training loss = 7.7005839347839355\n",
      "validation loss = 7.517936530866121\n",
      "Batch 92 / 157\n",
      "training loss = 7.598077774047852\n",
      "validation loss = 7.525129719784386\n",
      "Batch 93 / 157\n",
      "training loss = 7.520822525024414\n",
      "validation loss = 7.513401608718069\n",
      "Batch 94 / 157\n",
      "training loss = 7.5092363357543945\n",
      "validation loss = 7.510173998380962\n",
      "Batch 95 / 157\n",
      "training loss = 7.587111473083496\n",
      "validation loss = 7.505295502512078\n",
      "Batch 96 / 157\n",
      "training loss = 7.498109817504883\n",
      "validation loss = 7.497573350605212\n",
      "Batch 97 / 157\n",
      "training loss = 7.354412078857422\n",
      "validation loss = 7.494241162350304\n",
      "Batch 98 / 157\n",
      "training loss = 7.750428676605225\n",
      "validation loss = 7.48797627499229\n",
      "Batch 99 / 157\n",
      "training loss = 7.504854679107666\n",
      "validation loss = 7.486634706196032\n",
      "Batch 100 / 157\n",
      "training loss = 7.862342834472656\n",
      "validation loss = 7.491070119958175\n",
      "Batch 101 / 157\n",
      "training loss = 7.2831878662109375\n",
      "validation loss = 7.4844724755538135\n",
      "Batch 102 / 157\n",
      "training loss = 7.5251994132995605\n",
      "validation loss = 7.476772182866147\n",
      "Batch 103 / 157\n",
      "training loss = 7.54025936126709\n",
      "validation loss = 7.467935737810637\n",
      "Batch 104 / 157\n",
      "training loss = 7.6250834465026855\n",
      "validation loss = 7.465379639675743\n",
      "Batch 105 / 157\n",
      "training loss = 7.2535400390625\n",
      "validation loss = 7.457911315717195\n",
      "Batch 106 / 157\n",
      "training loss = 7.477269172668457\n",
      "validation loss = 7.453272016424882\n",
      "Batch 107 / 157\n",
      "training loss = 7.460275173187256\n",
      "validation loss = 7.450150038066663\n",
      "Batch 108 / 157\n",
      "training loss = 7.351314067840576\n",
      "validation loss = 7.451239008652537\n",
      "Batch 109 / 157\n",
      "training loss = 7.21483039855957\n",
      "validation loss = 7.445243283321983\n",
      "Batch 110 / 157\n",
      "training loss = 7.473176956176758\n",
      "validation loss = 7.443036029213353\n",
      "Batch 111 / 157\n",
      "training loss = 7.430731773376465\n",
      "validation loss = 7.434826926181191\n",
      "Batch 112 / 157\n",
      "training loss = 7.490732192993164\n",
      "validation loss = 7.436109969490452\n",
      "Batch 113 / 157\n",
      "training loss = 7.620974540710449\n",
      "validation loss = 7.42876682783428\n",
      "Batch 114 / 157\n",
      "training loss = 7.536118030548096\n",
      "validation loss = 7.4230973846034\n",
      "Batch 115 / 157\n",
      "training loss = 7.278281211853027\n",
      "validation loss = 7.421482563018799\n",
      "Batch 116 / 157\n",
      "training loss = 7.560044765472412\n",
      "validation loss = 7.416716425042403\n",
      "Batch 117 / 157\n",
      "training loss = 7.101644515991211\n",
      "validation loss = 7.417881488800049\n",
      "Batch 118 / 157\n",
      "training loss = 7.480790138244629\n",
      "validation loss = 7.416192054748535\n",
      "Batch 119 / 157\n",
      "training loss = 7.2651686668396\n",
      "validation loss = 7.410301384172942\n",
      "Batch 120 / 157\n",
      "training loss = 7.221944808959961\n",
      "validation loss = 7.407279064780788\n",
      "Batch 121 / 157\n",
      "training loss = 7.252265453338623\n",
      "validation loss = 7.407773770784077\n",
      "Batch 122 / 157\n",
      "training loss = 7.346627235412598\n",
      "validation loss = 7.405115554207249\n",
      "Batch 123 / 157\n",
      "training loss = 7.379557132720947\n",
      "validation loss = 7.394489815360622\n",
      "Batch 124 / 157\n",
      "training loss = 7.605792999267578\n",
      "validation loss = 7.3904499003761694\n",
      "Batch 125 / 157\n",
      "training loss = 7.547486782073975\n",
      "validation loss = 7.396233157107704\n",
      "Batch 126 / 157\n",
      "training loss = 7.432412624359131\n",
      "validation loss = 7.3905742544876905\n",
      "Batch 127 / 157\n",
      "training loss = 7.656036853790283\n",
      "validation loss = 7.3862275324369735\n",
      "Batch 128 / 157\n",
      "training loss = 7.496741771697998\n",
      "validation loss = 7.378774241397255\n",
      "Batch 129 / 157\n",
      "training loss = 7.407654762268066\n",
      "validation loss = 7.3725561593708235\n",
      "Batch 130 / 157\n",
      "training loss = 7.58944034576416\n",
      "validation loss = 7.373837445911608\n",
      "Batch 131 / 157\n",
      "training loss = 7.692183494567871\n",
      "validation loss = 7.37413393823724\n",
      "Batch 132 / 157\n",
      "training loss = 7.614867210388184\n",
      "validation loss = 7.37316874453896\n",
      "Batch 133 / 157\n",
      "training loss = 7.357173442840576\n",
      "validation loss = 7.362179279327393\n",
      "Batch 134 / 157\n",
      "training loss = 7.4260334968566895\n",
      "validation loss = 7.369711524561832\n",
      "Batch 135 / 157\n",
      "training loss = 7.383084774017334\n",
      "validation loss = 7.363039167303788\n",
      "Batch 136 / 157\n",
      "training loss = 7.4772562980651855\n",
      "validation loss = 7.358056043323717\n",
      "Batch 137 / 157\n",
      "training loss = 7.286571025848389\n",
      "validation loss = 7.358937188198692\n",
      "Batch 138 / 157\n",
      "training loss = 7.209090232849121\n",
      "validation loss = 7.3581313835947135\n",
      "Batch 139 / 157\n",
      "training loss = 7.740720272064209\n",
      "validation loss = 7.3563587791041325\n",
      "Batch 140 / 157\n",
      "training loss = 7.485677719116211\n",
      "validation loss = 7.352550857945492\n",
      "Batch 141 / 157\n",
      "training loss = 7.4431376457214355\n",
      "validation loss = 7.350064177262156\n",
      "Batch 142 / 157\n",
      "training loss = 7.527683734893799\n",
      "validation loss = 7.356091624812076\n",
      "Batch 143 / 157\n",
      "training loss = 7.388707160949707\n",
      "validation loss = 7.347610297955964\n",
      "Batch 144 / 157\n",
      "training loss = 7.0798163414001465\n",
      "validation loss = 7.347617776770341\n",
      "Batch 145 / 157\n",
      "training loss = 7.570472240447998\n",
      "validation loss = 7.342967158869693\n",
      "Batch 146 / 157\n",
      "training loss = 7.215382099151611\n",
      "validation loss = 7.340014357315867\n",
      "Batch 147 / 157\n",
      "training loss = 7.444428443908691\n",
      "validation loss = 7.341813739977385\n",
      "Batch 148 / 157\n",
      "training loss = 7.2605133056640625\n",
      "validation loss = 7.3387547041240495\n",
      "Batch 149 / 157\n",
      "training loss = 7.386542320251465\n",
      "validation loss = 7.329854036632337\n",
      "Batch 150 / 157\n",
      "training loss = 7.594470500946045\n",
      "validation loss = 7.3331850955360816\n",
      "Batch 151 / 157\n",
      "training loss = 7.12706995010376\n",
      "validation loss = 7.3286535865382145\n",
      "Batch 152 / 157\n",
      "training loss = 7.325412273406982\n",
      "validation loss = 7.3211057060643245\n",
      "Batch 153 / 157\n",
      "training loss = 7.3817338943481445\n",
      "validation loss = 7.324811960521497\n",
      "Batch 154 / 157\n",
      "training loss = 7.398347854614258\n",
      "validation loss = 7.322141220695094\n",
      "Batch 155 / 157\n",
      "training loss = 7.538479328155518\n",
      "validation loss = 7.3256309659857495\n",
      "Batch 156 / 157\n",
      "training loss = 7.37797212600708\n",
      "validation loss = 7.324002567090486\n",
      "Batch 157 / 157\n",
      "training loss = 7.514681339263916\n",
      "validation loss = 7.328617221430728\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.376919746398926\n",
      "validation loss = 10.327754974365234\n",
      "Batch 2 / 157\n",
      "training loss = 10.336736679077148\n",
      "validation loss = 10.207873143647847\n",
      "Batch 3 / 157\n",
      "training loss = 10.211270332336426\n",
      "validation loss = 10.087336540222168\n",
      "Batch 4 / 157\n",
      "training loss = 10.084962844848633\n",
      "validation loss = 9.969778864007248\n",
      "Batch 5 / 157\n",
      "training loss = 9.967342376708984\n",
      "validation loss = 9.850089725695158\n",
      "Batch 6 / 157\n",
      "training loss = 9.854305267333984\n",
      "validation loss = 9.731269083525005\n",
      "Batch 7 / 157\n",
      "training loss = 9.725481033325195\n",
      "validation loss = 9.61225790726511\n",
      "Batch 8 / 157\n",
      "training loss = 9.604035377502441\n",
      "validation loss = 9.493608123377749\n",
      "Batch 9 / 157\n",
      "training loss = 9.469117164611816\n",
      "validation loss = 9.377734786585757\n",
      "Batch 10 / 157\n",
      "training loss = 9.32060718536377\n",
      "validation loss = 9.264904975891113\n",
      "Batch 11 / 157\n",
      "training loss = 9.227554321289062\n",
      "validation loss = 9.155607022737202\n",
      "Batch 12 / 157\n",
      "training loss = 9.093321800231934\n",
      "validation loss = 9.051177878128854\n",
      "Batch 13 / 157\n",
      "training loss = 9.027523040771484\n",
      "validation loss = 8.950580697310599\n",
      "Batch 14 / 157\n",
      "training loss = 8.885025978088379\n",
      "validation loss = 8.853368106641268\n",
      "Batch 15 / 157\n",
      "training loss = 8.779763221740723\n",
      "validation loss = 8.759904660676655\n",
      "Batch 16 / 157\n",
      "training loss = 8.680241584777832\n",
      "validation loss = 8.664812891106857\n",
      "Batch 17 / 157\n",
      "training loss = 8.57623291015625\n",
      "validation loss = 8.578914290980288\n",
      "Batch 18 / 157\n",
      "training loss = 8.577301025390625\n",
      "validation loss = 8.497844344691226\n",
      "Batch 19 / 157\n",
      "training loss = 8.459942817687988\n",
      "validation loss = 8.42187846334357\n",
      "Batch 20 / 157\n",
      "training loss = 8.579963684082031\n",
      "validation loss = 8.348817724930612\n",
      "Batch 21 / 157\n",
      "training loss = 8.372673988342285\n",
      "validation loss = 8.284644478245786\n",
      "Batch 22 / 157\n",
      "training loss = 8.389046669006348\n",
      "validation loss = 8.219192354302658\n",
      "Batch 23 / 157\n",
      "training loss = 8.116598129272461\n",
      "validation loss = 8.158897199128804\n",
      "Batch 24 / 157\n",
      "training loss = 8.023420333862305\n",
      "validation loss = 8.10101506584569\n",
      "Batch 25 / 157\n",
      "training loss = 8.203804016113281\n",
      "validation loss = 8.049386375828794\n",
      "Batch 26 / 157\n",
      "training loss = 7.9403157234191895\n",
      "validation loss = 8.005635763469495\n",
      "Batch 27 / 157\n",
      "training loss = 8.074498176574707\n",
      "validation loss = 7.954379734240081\n",
      "Batch 28 / 157\n",
      "training loss = 7.950248718261719\n",
      "validation loss = 7.910897531007466\n",
      "Batch 29 / 157\n",
      "training loss = 7.838637351989746\n",
      "validation loss = 7.876209786063747\n",
      "Batch 30 / 157\n",
      "training loss = 7.941036224365234\n",
      "validation loss = 7.8400123997738485\n",
      "Batch 31 / 157\n",
      "training loss = 7.825225830078125\n",
      "validation loss = 7.8080095993845084\n",
      "Batch 32 / 157\n",
      "training loss = 8.041731834411621\n",
      "validation loss = 7.779188984318783\n",
      "Batch 33 / 157\n",
      "training loss = 7.6934428215026855\n",
      "validation loss = 7.748089689957468\n",
      "Batch 34 / 157\n",
      "training loss = 7.650240898132324\n",
      "validation loss = 7.725194278516267\n",
      "Batch 35 / 157\n",
      "training loss = 8.070123672485352\n",
      "validation loss = 7.708543325725355\n",
      "Batch 36 / 157\n",
      "training loss = 7.8322930335998535\n",
      "validation loss = 7.681802272796631\n",
      "Batch 37 / 157\n",
      "training loss = 7.639710903167725\n",
      "validation loss = 7.667852376636706\n",
      "Batch 38 / 157\n",
      "training loss = 7.512778282165527\n",
      "validation loss = 7.655800668816817\n",
      "Batch 39 / 157\n",
      "training loss = 7.5634565353393555\n",
      "validation loss = 7.642833433653179\n",
      "Batch 40 / 157\n",
      "training loss = 8.058759689331055\n",
      "validation loss = 7.62691563054135\n",
      "Batch 41 / 157\n",
      "training loss = 7.512414455413818\n",
      "validation loss = 7.617627093666478\n",
      "Batch 42 / 157\n",
      "training loss = 7.574169158935547\n",
      "validation loss = 7.61421160948904\n",
      "Batch 43 / 157\n",
      "training loss = 7.3553786277771\n",
      "validation loss = 7.603620303304572\n",
      "Batch 44 / 157\n",
      "training loss = 7.502386569976807\n",
      "validation loss = 7.604879856109619\n",
      "Batch 45 / 157\n",
      "training loss = 7.664964199066162\n",
      "validation loss = 7.605201746288099\n",
      "Batch 46 / 157\n",
      "training loss = 7.654056549072266\n",
      "validation loss = 7.607653592762194\n",
      "Batch 47 / 157\n",
      "training loss = 7.575273036956787\n",
      "validation loss = 7.59837700191297\n",
      "Batch 48 / 157\n",
      "training loss = 7.442431449890137\n",
      "validation loss = 7.60198618236341\n",
      "Batch 49 / 157\n",
      "training loss = 7.712772846221924\n",
      "validation loss = 7.599472798799214\n",
      "Batch 50 / 157\n",
      "training loss = 7.8284196853637695\n",
      "validation loss = 7.598551900763261\n",
      "Batch 51 / 157\n",
      "training loss = 7.548727035522461\n",
      "validation loss = 7.593006661063747\n",
      "Batch 52 / 157\n",
      "training loss = 7.560957908630371\n",
      "validation loss = 7.5914615329943205\n",
      "Batch 53 / 157\n",
      "training loss = 7.584903717041016\n",
      "validation loss = 7.594120176214921\n",
      "Batch 54 / 157\n",
      "training loss = 7.581784725189209\n",
      "validation loss = 7.5907997834055045\n",
      "Batch 55 / 157\n",
      "training loss = 7.496829986572266\n",
      "validation loss = 7.586557112242046\n",
      "Batch 56 / 157\n",
      "training loss = 7.525288105010986\n",
      "validation loss = 7.588446090095921\n",
      "Batch 57 / 157\n",
      "training loss = 7.624861717224121\n",
      "validation loss = 7.584319189975136\n",
      "Batch 58 / 157\n",
      "training loss = 7.4866251945495605\n",
      "validation loss = 7.585059868661981\n",
      "Batch 59 / 157\n",
      "training loss = 7.374388694763184\n",
      "validation loss = 7.572305428354364\n",
      "Batch 60 / 157\n",
      "training loss = 7.5411834716796875\n",
      "validation loss = 7.573807992433247\n",
      "Batch 61 / 157\n",
      "training loss = 7.7770161628723145\n",
      "validation loss = 7.57401516563014\n",
      "Batch 62 / 157\n",
      "training loss = 7.7713847160339355\n",
      "validation loss = 7.575068674589458\n",
      "Batch 63 / 157\n",
      "training loss = 7.581003665924072\n",
      "validation loss = 7.576614982203433\n",
      "Batch 64 / 157\n",
      "training loss = 7.603786468505859\n",
      "validation loss = 7.576272763703999\n",
      "Batch 65 / 157\n",
      "training loss = 7.534027099609375\n",
      "validation loss = 7.5637745355304915\n",
      "Batch 66 / 157\n",
      "training loss = 7.64019250869751\n",
      "validation loss = 7.559815758153012\n",
      "Batch 67 / 157\n",
      "training loss = 7.5652947425842285\n",
      "validation loss = 7.566015092950118\n",
      "Batch 68 / 157\n",
      "training loss = 7.842336177825928\n",
      "validation loss = 7.561182247964959\n",
      "Batch 69 / 157\n",
      "training loss = 7.604695796966553\n",
      "validation loss = 7.565050074928685\n",
      "Batch 70 / 157\n",
      "training loss = 7.610367298126221\n",
      "validation loss = 7.5612413757725765\n",
      "Batch 71 / 157\n",
      "training loss = 7.450563907623291\n",
      "validation loss = 7.55187830172087\n",
      "Batch 72 / 157\n",
      "training loss = 7.769080638885498\n",
      "validation loss = 7.541769178290116\n",
      "Batch 73 / 157\n",
      "training loss = 7.4026265144348145\n",
      "validation loss = 7.544761256167763\n",
      "Batch 74 / 157\n",
      "training loss = 7.422944068908691\n",
      "validation loss = 7.539928285699141\n",
      "Batch 75 / 157\n",
      "training loss = 7.728043556213379\n",
      "validation loss = 7.542724534084923\n",
      "Batch 76 / 157\n",
      "training loss = 7.503731727600098\n",
      "validation loss = 7.526995709067897\n",
      "Batch 77 / 157\n",
      "training loss = 7.46049165725708\n",
      "validation loss = 7.520910764995374\n",
      "Batch 78 / 157\n",
      "training loss = 7.673664569854736\n",
      "validation loss = 7.518311977386475\n",
      "Batch 79 / 157\n",
      "training loss = 7.566171169281006\n",
      "validation loss = 7.51868566713835\n",
      "Batch 80 / 157\n",
      "training loss = 7.594195365905762\n",
      "validation loss = 7.5128169310720345\n",
      "Batch 81 / 157\n",
      "training loss = 7.518603324890137\n",
      "validation loss = 7.506456425315456\n",
      "Batch 82 / 157\n",
      "training loss = 7.59102201461792\n",
      "validation loss = 7.502697041160182\n",
      "Batch 83 / 157\n",
      "training loss = 7.350672245025635\n",
      "validation loss = 7.501680700402511\n",
      "Batch 84 / 157\n",
      "training loss = 7.409970760345459\n",
      "validation loss = 7.496793144627621\n",
      "Batch 85 / 157\n",
      "training loss = 7.379054546356201\n",
      "validation loss = 7.490278118535092\n",
      "Batch 86 / 157\n",
      "training loss = 7.423778533935547\n",
      "validation loss = 7.48643348091527\n",
      "Batch 87 / 157\n",
      "training loss = 7.812834739685059\n",
      "validation loss = 7.486300393154747\n",
      "Batch 88 / 157\n",
      "training loss = 7.60385799407959\n",
      "validation loss = 7.483787837781404\n",
      "Batch 89 / 157\n",
      "training loss = 7.5073957443237305\n",
      "validation loss = 7.477570207495439\n",
      "Batch 90 / 157\n",
      "training loss = 7.362612724304199\n",
      "validation loss = 7.474467152043393\n",
      "Batch 91 / 157\n",
      "training loss = 7.40731954574585\n",
      "validation loss = 7.472056840595446\n",
      "Batch 92 / 157\n",
      "training loss = 7.6119585037231445\n",
      "validation loss = 7.47577280747263\n",
      "Batch 93 / 157\n",
      "training loss = 7.28352165222168\n",
      "validation loss = 7.468581701579847\n",
      "Batch 94 / 157\n",
      "training loss = 7.532662391662598\n",
      "validation loss = 7.458883561586079\n",
      "Batch 95 / 157\n",
      "training loss = 7.594344139099121\n",
      "validation loss = 7.452081253654079\n",
      "Batch 96 / 157\n",
      "training loss = 7.481670379638672\n",
      "validation loss = 7.4464343472530965\n",
      "Batch 97 / 157\n",
      "training loss = 7.3789167404174805\n",
      "validation loss = 7.440411818654914\n",
      "Batch 98 / 157\n",
      "training loss = 7.35040807723999\n",
      "validation loss = 7.433751959549753\n",
      "Batch 99 / 157\n",
      "training loss = 7.3952317237854\n",
      "validation loss = 7.436796313837955\n",
      "Batch 100 / 157\n",
      "training loss = 7.469527244567871\n",
      "validation loss = 7.429999301308079\n",
      "Batch 101 / 157\n",
      "training loss = 7.357108116149902\n",
      "validation loss = 7.421358786131206\n",
      "Batch 102 / 157\n",
      "training loss = 7.485598564147949\n",
      "validation loss = 7.424072918139006\n",
      "Batch 103 / 157\n",
      "training loss = 7.3570780754089355\n",
      "validation loss = 7.414502645793714\n",
      "Batch 104 / 157\n",
      "training loss = 7.099846839904785\n",
      "validation loss = 7.419747954920719\n",
      "Batch 105 / 157\n",
      "training loss = 7.192561149597168\n",
      "validation loss = 7.407856012645521\n",
      "Batch 106 / 157\n",
      "training loss = 7.531374454498291\n",
      "validation loss = 7.404342726657265\n",
      "Batch 107 / 157\n",
      "training loss = 7.501367568969727\n",
      "validation loss = 7.407518110777202\n",
      "Batch 108 / 157\n",
      "training loss = 7.514207363128662\n",
      "validation loss = 7.411572456359863\n",
      "Batch 109 / 157\n",
      "training loss = 7.364002704620361\n",
      "validation loss = 7.4015373179787085\n",
      "Batch 110 / 157\n",
      "training loss = 7.407561302185059\n",
      "validation loss = 7.395233681327419\n",
      "Batch 111 / 157\n",
      "training loss = 7.171595573425293\n",
      "validation loss = 7.385198944493344\n",
      "Batch 112 / 157\n",
      "training loss = 7.359889030456543\n",
      "validation loss = 7.38462832099513\n",
      "Batch 113 / 157\n",
      "training loss = 7.216459274291992\n",
      "validation loss = 7.384929581692345\n",
      "Batch 114 / 157\n",
      "training loss = 7.43763542175293\n",
      "validation loss = 7.380719787196109\n",
      "Batch 115 / 157\n",
      "training loss = 7.319953918457031\n",
      "validation loss = 7.380036203484786\n",
      "Batch 116 / 157\n",
      "training loss = 7.426652908325195\n",
      "validation loss = 7.37001223313181\n",
      "Batch 117 / 157\n",
      "training loss = 7.083305358886719\n",
      "validation loss = 7.3688906619423316\n",
      "Batch 118 / 157\n",
      "training loss = 7.411015033721924\n",
      "validation loss = 7.364687417682848\n",
      "Batch 119 / 157\n",
      "training loss = 7.403957843780518\n",
      "validation loss = 7.356609846416273\n",
      "Batch 120 / 157\n",
      "training loss = 7.399173259735107\n",
      "validation loss = 7.35706249036287\n",
      "Batch 121 / 157\n",
      "training loss = 7.461487770080566\n",
      "validation loss = 7.353480138276753\n",
      "Batch 122 / 157\n",
      "training loss = 7.3023529052734375\n",
      "validation loss = 7.347102541672556\n",
      "Batch 123 / 157\n",
      "training loss = 7.3448805809021\n",
      "validation loss = 7.348671009666042\n",
      "Batch 124 / 157\n",
      "training loss = 7.4335036277771\n",
      "validation loss = 7.346147210974443\n",
      "Batch 125 / 157\n",
      "training loss = 7.133939743041992\n",
      "validation loss = 7.346110795673571\n",
      "Batch 126 / 157\n",
      "training loss = 7.240110874176025\n",
      "validation loss = 7.344421060461747\n",
      "Batch 127 / 157\n",
      "training loss = 7.305696487426758\n",
      "validation loss = 7.3353422817431\n",
      "Batch 128 / 157\n",
      "training loss = 7.328609943389893\n",
      "validation loss = 7.344796732852333\n",
      "Batch 129 / 157\n",
      "training loss = 7.433503150939941\n",
      "validation loss = 7.339781485105815\n",
      "Batch 130 / 157\n",
      "training loss = 7.043978214263916\n",
      "validation loss = 7.333292383896677\n",
      "Batch 131 / 157\n",
      "training loss = 7.4038472175598145\n",
      "validation loss = 7.326188212946842\n",
      "Batch 132 / 157\n",
      "training loss = 7.457289218902588\n",
      "validation loss = 7.327289179751747\n",
      "Batch 133 / 157\n",
      "training loss = 7.157683849334717\n",
      "validation loss = 7.328794780530427\n",
      "Batch 134 / 157\n",
      "training loss = 7.41312837600708\n",
      "validation loss = 7.322920146741365\n",
      "Batch 135 / 157\n",
      "training loss = 7.1656718254089355\n",
      "validation loss = 7.320828161741558\n",
      "Batch 136 / 157\n",
      "training loss = 7.627216339111328\n",
      "validation loss = 7.321556342275519\n",
      "Batch 137 / 157\n",
      "training loss = 7.323461055755615\n",
      "validation loss = 7.318887986634907\n",
      "Batch 138 / 157\n",
      "training loss = 7.3446221351623535\n",
      "validation loss = 7.311141265066047\n",
      "Batch 139 / 157\n",
      "training loss = 7.302878379821777\n",
      "validation loss = 7.310535882648669\n",
      "Batch 140 / 157\n",
      "training loss = 7.070180892944336\n",
      "validation loss = 7.312439968711452\n",
      "Batch 141 / 157\n",
      "training loss = 7.478664398193359\n",
      "validation loss = 7.3053060581809595\n",
      "Batch 142 / 157\n",
      "training loss = 7.383297920227051\n",
      "validation loss = 7.297066638344212\n",
      "Batch 143 / 157\n",
      "training loss = 7.195891857147217\n",
      "validation loss = 7.301798193078292\n",
      "Batch 144 / 157\n",
      "training loss = 6.959497928619385\n",
      "validation loss = 7.298001264270983\n",
      "Batch 145 / 157\n",
      "training loss = 7.202130317687988\n",
      "validation loss = 7.2938184236225325\n",
      "Batch 146 / 157\n",
      "training loss = 7.242605209350586\n",
      "validation loss = 7.277347765470806\n",
      "Batch 147 / 157\n",
      "training loss = 7.329628944396973\n",
      "validation loss = 7.280762873197856\n",
      "Batch 148 / 157\n",
      "training loss = 7.291563034057617\n",
      "validation loss = 7.280879171271073\n",
      "Batch 149 / 157\n",
      "training loss = 7.155224323272705\n",
      "validation loss = 7.274565746909694\n",
      "Batch 150 / 157\n",
      "training loss = 7.213009357452393\n",
      "validation loss = 7.273551288403962\n",
      "Batch 151 / 157\n",
      "training loss = 7.351691722869873\n",
      "validation loss = 7.2765403044851205\n",
      "Batch 152 / 157\n",
      "training loss = 7.340569496154785\n",
      "validation loss = 7.2681382329840405\n",
      "Batch 153 / 157\n",
      "training loss = 7.300513744354248\n",
      "validation loss = 7.266658983732524\n",
      "Batch 154 / 157\n",
      "training loss = 7.330570697784424\n",
      "validation loss = 7.2617575745833545\n",
      "Batch 155 / 157\n",
      "training loss = 7.4923415184021\n",
      "validation loss = 7.261930892342015\n",
      "Batch 156 / 157\n",
      "training loss = 7.387166500091553\n",
      "validation loss = 7.264281021921258\n",
      "Batch 157 / 157\n",
      "training loss = 7.4499382972717285\n",
      "validation loss = 7.261107821213572\n",
      "Average training loss: 7.74\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.371384620666504\n",
      "validation loss = 10.30172011726781\n",
      "Batch 2 / 157\n",
      "training loss = 10.29857063293457\n",
      "validation loss = 10.195525119179173\n",
      "Batch 3 / 157\n",
      "training loss = 10.206719398498535\n",
      "validation loss = 10.099813561690482\n",
      "Batch 4 / 157\n",
      "training loss = 10.107993125915527\n",
      "validation loss = 9.997416747243781\n",
      "Batch 5 / 157\n",
      "training loss = 9.965168952941895\n",
      "validation loss = 9.891034276861893\n",
      "Batch 6 / 157\n",
      "training loss = 9.914422035217285\n",
      "validation loss = 9.784607485720986\n",
      "Batch 7 / 157\n",
      "training loss = 9.841618537902832\n",
      "validation loss = 9.677293526498895\n",
      "Batch 8 / 157\n",
      "training loss = 9.559085845947266\n",
      "validation loss = 9.568791489852103\n",
      "Batch 9 / 157\n",
      "training loss = 9.565474510192871\n",
      "validation loss = 9.457959275496634\n",
      "Batch 10 / 157\n",
      "training loss = 9.404611587524414\n",
      "validation loss = 9.349629301773874\n",
      "Batch 11 / 157\n",
      "training loss = 9.355602264404297\n",
      "validation loss = 9.241744643763491\n",
      "Batch 12 / 157\n",
      "training loss = 9.330364227294922\n",
      "validation loss = 9.135322319833856\n",
      "Batch 13 / 157\n",
      "training loss = 9.196229934692383\n",
      "validation loss = 9.030548497250205\n",
      "Batch 14 / 157\n",
      "training loss = 8.978920936584473\n",
      "validation loss = 8.931676412883558\n",
      "Batch 15 / 157\n",
      "training loss = 9.076693534851074\n",
      "validation loss = 8.833750072278475\n",
      "Batch 16 / 157\n",
      "training loss = 8.902607917785645\n",
      "validation loss = 8.74034013246235\n",
      "Batch 17 / 157\n",
      "training loss = 8.766377449035645\n",
      "validation loss = 8.648251784475226\n",
      "Batch 18 / 157\n",
      "training loss = 8.925235748291016\n",
      "validation loss = 8.558129461188065\n",
      "Batch 19 / 157\n",
      "training loss = 8.538004875183105\n",
      "validation loss = 8.479540975470291\n",
      "Batch 20 / 157\n",
      "training loss = 8.53498363494873\n",
      "validation loss = 8.403918517263312\n",
      "Batch 21 / 157\n",
      "training loss = 8.17485523223877\n",
      "validation loss = 8.33064395502994\n",
      "Batch 22 / 157\n",
      "training loss = 8.371374130249023\n",
      "validation loss = 8.26893108769467\n",
      "Batch 23 / 157\n",
      "training loss = 8.3131103515625\n",
      "validation loss = 8.204901996411776\n",
      "Batch 24 / 157\n",
      "training loss = 8.369367599487305\n",
      "validation loss = 8.148244581724468\n",
      "Batch 25 / 157\n",
      "training loss = 8.085460662841797\n",
      "validation loss = 8.09438765676398\n",
      "Batch 26 / 157\n",
      "training loss = 8.284846305847168\n",
      "validation loss = 8.048752533762078\n",
      "Batch 27 / 157\n",
      "training loss = 7.964934349060059\n",
      "validation loss = 8.00827427914268\n",
      "Batch 28 / 157\n",
      "training loss = 8.195708274841309\n",
      "validation loss = 7.97074556350708\n",
      "Batch 29 / 157\n",
      "training loss = 7.871158123016357\n",
      "validation loss = 7.934883167869167\n",
      "Batch 30 / 157\n",
      "training loss = 7.904305934906006\n",
      "validation loss = 7.912299080898888\n",
      "Batch 31 / 157\n",
      "training loss = 7.659926414489746\n",
      "validation loss = 7.877565559587981\n",
      "Batch 32 / 157\n",
      "training loss = 7.793229103088379\n",
      "validation loss = 7.8588738441467285\n",
      "Batch 33 / 157\n",
      "training loss = 7.925942420959473\n",
      "validation loss = 7.842536926269531\n",
      "Batch 34 / 157\n",
      "training loss = 7.846389293670654\n",
      "validation loss = 7.828258012470446\n",
      "Batch 35 / 157\n",
      "training loss = 8.016432762145996\n",
      "validation loss = 7.816850335974443\n",
      "Batch 36 / 157\n",
      "training loss = 7.861180305480957\n",
      "validation loss = 7.80390571293078\n",
      "Batch 37 / 157\n",
      "training loss = 7.672399044036865\n",
      "validation loss = 7.7965523820174365\n",
      "Batch 38 / 157\n",
      "training loss = 7.654963970184326\n",
      "validation loss = 7.7823661754005835\n",
      "Batch 39 / 157\n",
      "training loss = 7.799849033355713\n",
      "validation loss = 7.77087936903301\n",
      "Batch 40 / 157\n",
      "training loss = 7.805106163024902\n",
      "validation loss = 7.773012562801964\n",
      "Batch 41 / 157\n",
      "training loss = 7.600465774536133\n",
      "validation loss = 7.759963512420654\n",
      "Batch 42 / 157\n",
      "training loss = 7.829367637634277\n",
      "validation loss = 7.753351462514777\n",
      "Batch 43 / 157\n",
      "training loss = 7.561956405639648\n",
      "validation loss = 7.7473329493874\n",
      "Batch 44 / 157\n",
      "training loss = 7.810909748077393\n",
      "validation loss = 7.74242303245946\n",
      "Batch 45 / 157\n",
      "training loss = 7.820744037628174\n",
      "validation loss = 7.736159901869924\n",
      "Batch 46 / 157\n",
      "training loss = 7.523804664611816\n",
      "validation loss = 7.737823385941355\n",
      "Batch 47 / 157\n",
      "training loss = 7.507258892059326\n",
      "validation loss = 7.7432890691255265\n",
      "Batch 48 / 157\n",
      "training loss = 7.95137882232666\n",
      "validation loss = 7.734733957993357\n",
      "Batch 49 / 157\n",
      "training loss = 7.608159065246582\n",
      "validation loss = 7.724101844586824\n",
      "Batch 50 / 157\n",
      "training loss = 7.505457878112793\n",
      "validation loss = 7.716113165805214\n",
      "Batch 51 / 157\n",
      "training loss = 7.645812034606934\n",
      "validation loss = 7.7188443886606315\n",
      "Batch 52 / 157\n",
      "training loss = 7.642874717712402\n",
      "validation loss = 7.706713651355944\n",
      "Batch 53 / 157\n",
      "training loss = 7.810633182525635\n",
      "validation loss = 7.704329741628547\n",
      "Batch 54 / 157\n",
      "training loss = 7.806802749633789\n",
      "validation loss = 7.705265195746171\n",
      "Batch 55 / 157\n",
      "training loss = 7.620270252227783\n",
      "validation loss = 7.684903747157047\n",
      "Batch 56 / 157\n",
      "training loss = 7.984571933746338\n",
      "validation loss = 7.679340563322368\n",
      "Batch 57 / 157\n",
      "training loss = 7.591281414031982\n",
      "validation loss = 7.670918238790412\n",
      "Batch 58 / 157\n",
      "training loss = 7.7029948234558105\n",
      "validation loss = 7.6618775066576505\n",
      "Batch 59 / 157\n",
      "training loss = 7.588173866271973\n",
      "validation loss = 7.661044974076121\n",
      "Batch 60 / 157\n",
      "training loss = 7.950550079345703\n",
      "validation loss = 7.660025019394724\n",
      "Batch 61 / 157\n",
      "training loss = 7.804901123046875\n",
      "validation loss = 7.652227150766473\n",
      "Batch 62 / 157\n",
      "training loss = 7.7545342445373535\n",
      "validation loss = 7.652751571253726\n",
      "Batch 63 / 157\n",
      "training loss = 7.452725410461426\n",
      "validation loss = 7.651048158344469\n",
      "Batch 64 / 157\n",
      "training loss = 7.69541597366333\n",
      "validation loss = 7.64739842163889\n",
      "Batch 65 / 157\n",
      "training loss = 7.217011451721191\n",
      "validation loss = 7.645828648617393\n",
      "Batch 66 / 157\n",
      "training loss = 7.868759632110596\n",
      "validation loss = 7.630397319793701\n",
      "Batch 67 / 157\n",
      "training loss = 7.878252029418945\n",
      "validation loss = 7.658240544168573\n",
      "Batch 68 / 157\n",
      "training loss = 7.445265293121338\n",
      "validation loss = 7.626058478104441\n",
      "Batch 69 / 157\n",
      "training loss = 7.653639316558838\n",
      "validation loss = 7.629767919841566\n",
      "Batch 70 / 157\n",
      "training loss = 7.426093101501465\n",
      "validation loss = 7.639760770295796\n",
      "Batch 71 / 157\n",
      "training loss = 7.730867385864258\n",
      "validation loss = 7.615425561603747\n",
      "Batch 72 / 157\n",
      "training loss = 7.666460037231445\n",
      "validation loss = 7.604447565580669\n",
      "Batch 73 / 157\n",
      "training loss = 7.966676712036133\n",
      "validation loss = 7.625312930659244\n",
      "Batch 74 / 157\n",
      "training loss = 7.787268161773682\n",
      "validation loss = 7.625446344676771\n",
      "Batch 75 / 157\n",
      "training loss = 7.8236284255981445\n",
      "validation loss = 7.601269696888171\n",
      "Batch 76 / 157\n",
      "training loss = 7.726281642913818\n",
      "validation loss = 7.601389834755345\n",
      "Batch 77 / 157\n",
      "training loss = 7.340294361114502\n",
      "validation loss = 7.5978413883008455\n",
      "Batch 78 / 157\n",
      "training loss = 7.224180698394775\n",
      "validation loss = 7.606683329532021\n",
      "Batch 79 / 157\n",
      "training loss = 7.7295145988464355\n",
      "validation loss = 7.5970865048860245\n",
      "Batch 80 / 157\n",
      "training loss = 7.326547145843506\n",
      "validation loss = 7.592431595450954\n",
      "Batch 81 / 157\n",
      "training loss = 7.3889594078063965\n",
      "validation loss = 7.580038346742329\n",
      "Batch 82 / 157\n",
      "training loss = 7.460362911224365\n",
      "validation loss = 7.569432158219187\n",
      "Batch 83 / 157\n",
      "training loss = 7.490847110748291\n",
      "validation loss = 7.565238902443333\n",
      "Batch 84 / 157\n",
      "training loss = 7.813765525817871\n",
      "validation loss = 7.5758027277494735\n",
      "Batch 85 / 157\n",
      "training loss = 7.8438334465026855\n",
      "validation loss = 7.577515727595279\n",
      "Batch 86 / 157\n",
      "training loss = 7.683459758758545\n",
      "validation loss = 7.559091392316316\n",
      "Batch 87 / 157\n",
      "training loss = 7.626794815063477\n",
      "validation loss = 7.549467739305999\n",
      "Batch 88 / 157\n",
      "training loss = 7.24303674697876\n",
      "validation loss = 7.548919150703831\n",
      "Batch 89 / 157\n",
      "training loss = 7.613410472869873\n",
      "validation loss = 7.551127810227244\n",
      "Batch 90 / 157\n",
      "training loss = 7.545979976654053\n",
      "validation loss = 7.550289580696507\n",
      "Batch 91 / 157\n",
      "training loss = 7.443228721618652\n",
      "validation loss = 7.5502582349275285\n",
      "Batch 92 / 157\n",
      "training loss = 7.590917110443115\n",
      "validation loss = 7.537355824520714\n",
      "Batch 93 / 157\n",
      "training loss = 7.819512844085693\n",
      "validation loss = 7.543994326340525\n",
      "Batch 94 / 157\n",
      "training loss = 7.527468681335449\n",
      "validation loss = 7.547739229704204\n",
      "Batch 95 / 157\n",
      "training loss = 7.112157344818115\n",
      "validation loss = 7.529461559496428\n",
      "Batch 96 / 157\n",
      "training loss = 7.33484411239624\n",
      "validation loss = 7.516364900689376\n",
      "Batch 97 / 157\n",
      "training loss = 7.436033248901367\n",
      "validation loss = 7.5176087931582805\n",
      "Batch 98 / 157\n",
      "training loss = 7.414262771606445\n",
      "validation loss = 7.524413786436382\n",
      "Batch 99 / 157\n",
      "training loss = 7.597309112548828\n",
      "validation loss = 7.5224205318250155\n",
      "Batch 100 / 157\n",
      "training loss = 7.485655784606934\n",
      "validation loss = 7.5113772342079566\n",
      "Batch 101 / 157\n",
      "training loss = 7.4539361000061035\n",
      "validation loss = 7.501248987097489\n",
      "Batch 102 / 157\n",
      "training loss = 7.557356834411621\n",
      "validation loss = 7.495361052061382\n",
      "Batch 103 / 157\n",
      "training loss = 7.922079563140869\n",
      "validation loss = 7.511068645276521\n",
      "Batch 104 / 157\n",
      "training loss = 7.142809867858887\n",
      "validation loss = 7.4981559703224585\n",
      "Batch 105 / 157\n",
      "training loss = 7.337491512298584\n",
      "validation loss = 7.489905181683992\n",
      "Batch 106 / 157\n",
      "training loss = 7.255304336547852\n",
      "validation loss = 7.484643333836606\n",
      "Batch 107 / 157\n",
      "training loss = 7.433541774749756\n",
      "validation loss = 7.472595490907368\n",
      "Batch 108 / 157\n",
      "training loss = 7.486448764801025\n",
      "validation loss = 7.46576462293926\n",
      "Batch 109 / 157\n",
      "training loss = 7.411468505859375\n",
      "validation loss = 7.46865009006701\n",
      "Batch 110 / 157\n",
      "training loss = 7.326052665710449\n",
      "validation loss = 7.47133009057296\n",
      "Batch 111 / 157\n",
      "training loss = 7.5297393798828125\n",
      "validation loss = 7.469947287910863\n",
      "Batch 112 / 157\n",
      "training loss = 7.433554649353027\n",
      "validation loss = 7.459577485134727\n",
      "Batch 113 / 157\n",
      "training loss = 7.290884971618652\n",
      "validation loss = 7.450933707387824\n",
      "Batch 114 / 157\n",
      "training loss = 7.416118144989014\n",
      "validation loss = 7.446053580233925\n",
      "Batch 115 / 157\n",
      "training loss = 7.4354705810546875\n",
      "validation loss = 7.438101040689569\n",
      "Batch 116 / 157\n",
      "training loss = 7.476266860961914\n",
      "validation loss = 7.434744433352821\n",
      "Batch 117 / 157\n",
      "training loss = 7.316909313201904\n",
      "validation loss = 7.431288794467323\n",
      "Batch 118 / 157\n",
      "training loss = 7.732757091522217\n",
      "validation loss = 7.423033764487819\n",
      "Batch 119 / 157\n",
      "training loss = 7.106149673461914\n",
      "validation loss = 7.424843963823821\n",
      "Batch 120 / 157\n",
      "training loss = 7.755573272705078\n",
      "validation loss = 7.417825247112074\n",
      "Batch 121 / 157\n",
      "training loss = 7.313107013702393\n",
      "validation loss = 7.408768754256399\n",
      "Batch 122 / 157\n",
      "training loss = 7.337316036224365\n",
      "validation loss = 7.401808989675422\n",
      "Batch 123 / 157\n",
      "training loss = 7.387704849243164\n",
      "validation loss = 7.400682800694516\n",
      "Batch 124 / 157\n",
      "training loss = 7.306822776794434\n",
      "validation loss = 7.403402955908525\n",
      "Batch 125 / 157\n",
      "training loss = 7.488704204559326\n",
      "validation loss = 7.391472916854055\n",
      "Batch 126 / 157\n",
      "training loss = 7.22138786315918\n",
      "validation loss = 7.384976663087544\n",
      "Batch 127 / 157\n",
      "training loss = 7.4354777336120605\n",
      "validation loss = 7.390825321799831\n",
      "Batch 128 / 157\n",
      "training loss = 7.715269565582275\n",
      "validation loss = 7.3916503253735995\n",
      "Batch 129 / 157\n",
      "training loss = 7.261453151702881\n",
      "validation loss = 7.3835195240221525\n",
      "Batch 130 / 157\n",
      "training loss = 7.368081569671631\n",
      "validation loss = 7.377174226861251\n",
      "Batch 131 / 157\n",
      "training loss = 7.390992641448975\n",
      "validation loss = 7.3635505877043075\n",
      "Batch 132 / 157\n",
      "training loss = 7.387175559997559\n",
      "validation loss = 7.361393276013826\n",
      "Batch 133 / 157\n",
      "training loss = 7.705896377563477\n",
      "validation loss = 7.360516096416273\n",
      "Batch 134 / 157\n",
      "training loss = 7.1435627937316895\n",
      "validation loss = 7.360062247828433\n",
      "Batch 135 / 157\n",
      "training loss = 7.4936957359313965\n",
      "validation loss = 7.3602499208952255\n",
      "Batch 136 / 157\n",
      "training loss = 7.283592700958252\n",
      "validation loss = 7.353039089002107\n",
      "Batch 137 / 157\n",
      "training loss = 7.139728546142578\n",
      "validation loss = 7.352751857356021\n",
      "Batch 138 / 157\n",
      "training loss = 7.190287113189697\n",
      "validation loss = 7.352872170900044\n",
      "Batch 139 / 157\n",
      "training loss = 7.370415210723877\n",
      "validation loss = 7.342334922991301\n",
      "Batch 140 / 157\n",
      "training loss = 7.521269798278809\n",
      "validation loss = 7.338991591804906\n",
      "Batch 141 / 157\n",
      "training loss = 7.49896764755249\n",
      "validation loss = 7.333353820600007\n",
      "Batch 142 / 157\n",
      "training loss = 7.116786479949951\n",
      "validation loss = 7.328917202196624\n",
      "Batch 143 / 157\n",
      "training loss = 7.743288993835449\n",
      "validation loss = 7.330061084345767\n",
      "Batch 144 / 157\n",
      "training loss = 7.458632946014404\n",
      "validation loss = 7.332566989095588\n",
      "Batch 145 / 157\n",
      "training loss = 7.0917582511901855\n",
      "validation loss = 7.331987807625218\n",
      "Batch 146 / 157\n",
      "training loss = 7.1620306968688965\n",
      "validation loss = 7.326265636243318\n",
      "Batch 147 / 157\n",
      "training loss = 7.43379020690918\n",
      "validation loss = 7.321409501527485\n",
      "Batch 148 / 157\n",
      "training loss = 7.366473197937012\n",
      "validation loss = 7.311787931542647\n",
      "Batch 149 / 157\n",
      "training loss = 7.467174053192139\n",
      "validation loss = 7.310723354941921\n",
      "Batch 150 / 157\n",
      "training loss = 7.153637409210205\n",
      "validation loss = 7.310889896593596\n",
      "Batch 151 / 157\n",
      "training loss = 7.202804088592529\n",
      "validation loss = 7.311017939918919\n",
      "Batch 152 / 157\n",
      "training loss = 7.045927047729492\n",
      "validation loss = 7.306435484635203\n",
      "Batch 153 / 157\n",
      "training loss = 7.600961208343506\n",
      "validation loss = 7.305374772925126\n",
      "Batch 154 / 157\n",
      "training loss = 7.267988204956055\n",
      "validation loss = 7.306147751055266\n",
      "Batch 155 / 157\n",
      "training loss = 7.419192314147949\n",
      "validation loss = 7.306398667787251\n",
      "Batch 156 / 157\n",
      "training loss = 7.686071872711182\n",
      "validation loss = 7.312093759837904\n",
      "Batch 157 / 157\n",
      "training loss = 6.897534370422363\n",
      "validation loss = 7.3001714505647355\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.384073257446289\n",
      "validation loss = 10.312068939208984\n",
      "Batch 2 / 157\n",
      "training loss = 10.314749717712402\n",
      "validation loss = 10.196129347148695\n",
      "Batch 3 / 157\n",
      "training loss = 10.195162773132324\n",
      "validation loss = 10.081621069657174\n",
      "Batch 4 / 157\n",
      "training loss = 10.093050956726074\n",
      "validation loss = 9.967746383265444\n",
      "Batch 5 / 157\n",
      "training loss = 9.977616310119629\n",
      "validation loss = 9.851262945877878\n",
      "Batch 6 / 157\n",
      "training loss = 9.81893253326416\n",
      "validation loss = 9.733224316647178\n",
      "Batch 7 / 157\n",
      "training loss = 9.744186401367188\n",
      "validation loss = 9.616183280944824\n",
      "Batch 8 / 157\n",
      "training loss = 9.630949974060059\n",
      "validation loss = 9.49956296619616\n",
      "Batch 9 / 157\n",
      "training loss = 9.547492027282715\n",
      "validation loss = 9.38487078014173\n",
      "Batch 10 / 157\n",
      "training loss = 9.354297637939453\n",
      "validation loss = 9.272377064353542\n",
      "Batch 11 / 157\n",
      "training loss = 9.26647663116455\n",
      "validation loss = 9.162856302763286\n",
      "Batch 12 / 157\n",
      "training loss = 9.09006404876709\n",
      "validation loss = 9.054883655748869\n",
      "Batch 13 / 157\n",
      "training loss = 9.172639846801758\n",
      "validation loss = 8.952266341761538\n",
      "Batch 14 / 157\n",
      "training loss = 8.9226713180542\n",
      "validation loss = 8.854844043129368\n",
      "Batch 15 / 157\n",
      "training loss = 8.885910034179688\n",
      "validation loss = 8.75969409942627\n",
      "Batch 16 / 157\n",
      "training loss = 8.88180923461914\n",
      "validation loss = 8.670256514298288\n",
      "Batch 17 / 157\n",
      "training loss = 8.559268951416016\n",
      "validation loss = 8.585474164862381\n",
      "Batch 18 / 157\n",
      "training loss = 8.585671424865723\n",
      "validation loss = 8.506326725608425\n",
      "Batch 19 / 157\n",
      "training loss = 8.556814193725586\n",
      "validation loss = 8.431748540777908\n",
      "Batch 20 / 157\n",
      "training loss = 8.36514663696289\n",
      "validation loss = 8.359144110428659\n",
      "Batch 21 / 157\n",
      "training loss = 8.381858825683594\n",
      "validation loss = 8.292890849866366\n",
      "Batch 22 / 157\n",
      "training loss = 8.122546195983887\n",
      "validation loss = 8.228582131235223\n",
      "Batch 23 / 157\n",
      "training loss = 8.169089317321777\n",
      "validation loss = 8.172377511074668\n",
      "Batch 24 / 157\n",
      "training loss = 8.299362182617188\n",
      "validation loss = 8.11783594834177\n",
      "Batch 25 / 157\n",
      "training loss = 8.190303802490234\n",
      "validation loss = 8.06586524059898\n",
      "Batch 26 / 157\n",
      "training loss = 8.048977851867676\n",
      "validation loss = 8.022959382910477\n",
      "Batch 27 / 157\n",
      "training loss = 8.10409164428711\n",
      "validation loss = 7.979093099895277\n",
      "Batch 28 / 157\n",
      "training loss = 7.705880165100098\n",
      "validation loss = 7.9399472286826684\n",
      "Batch 29 / 157\n",
      "training loss = 8.254274368286133\n",
      "validation loss = 7.904478474667198\n",
      "Batch 30 / 157\n",
      "training loss = 8.105484962463379\n",
      "validation loss = 7.870111791711104\n",
      "Batch 31 / 157\n",
      "training loss = 7.945746421813965\n",
      "validation loss = 7.842072336297286\n",
      "Batch 32 / 157\n",
      "training loss = 7.86038064956665\n",
      "validation loss = 7.813564400923879\n",
      "Batch 33 / 157\n",
      "training loss = 7.8336029052734375\n",
      "validation loss = 7.789261315998278\n",
      "Batch 34 / 157\n",
      "training loss = 7.912222385406494\n",
      "validation loss = 7.769418616043894\n",
      "Batch 35 / 157\n",
      "training loss = 8.025629043579102\n",
      "validation loss = 7.740896024202046\n",
      "Batch 36 / 157\n",
      "training loss = 7.69140100479126\n",
      "validation loss = 7.725063976488616\n",
      "Batch 37 / 157\n",
      "training loss = 7.788008213043213\n",
      "validation loss = 7.713000950060393\n",
      "Batch 38 / 157\n",
      "training loss = 7.49051570892334\n",
      "validation loss = 7.700032660835667\n",
      "Batch 39 / 157\n",
      "training loss = 7.585115909576416\n",
      "validation loss = 7.682364965740003\n",
      "Batch 40 / 157\n",
      "training loss = 7.811031818389893\n",
      "validation loss = 7.680952724657561\n",
      "Batch 41 / 157\n",
      "training loss = 7.681548118591309\n",
      "validation loss = 7.66571381217555\n",
      "Batch 42 / 157\n",
      "training loss = 7.598325252532959\n",
      "validation loss = 7.658863318593879\n",
      "Batch 43 / 157\n",
      "training loss = 7.76684045791626\n",
      "validation loss = 7.655548597636976\n",
      "Batch 44 / 157\n",
      "training loss = 7.652618885040283\n",
      "validation loss = 7.6498770462839225\n",
      "Batch 45 / 157\n",
      "training loss = 7.333434104919434\n",
      "validation loss = 7.647748294629548\n",
      "Batch 46 / 157\n",
      "training loss = 7.666229248046875\n",
      "validation loss = 7.637395432120876\n",
      "Batch 47 / 157\n",
      "training loss = 7.5933756828308105\n",
      "validation loss = 7.636658944581685\n",
      "Batch 48 / 157\n",
      "training loss = 7.739306926727295\n",
      "validation loss = 7.630537058177747\n",
      "Batch 49 / 157\n",
      "training loss = 7.725002288818359\n",
      "validation loss = 7.631845649920012\n",
      "Batch 50 / 157\n",
      "training loss = 7.783784866333008\n",
      "validation loss = 7.632627010345459\n",
      "Batch 51 / 157\n",
      "training loss = 7.725077152252197\n",
      "validation loss = 7.635447125685842\n",
      "Batch 52 / 157\n",
      "training loss = 7.751509666442871\n",
      "validation loss = 7.6298086015801685\n",
      "Batch 53 / 157\n",
      "training loss = 7.746828556060791\n",
      "validation loss = 7.621962396722091\n",
      "Batch 54 / 157\n",
      "training loss = 7.576573848724365\n",
      "validation loss = 7.6173426226565715\n",
      "Batch 55 / 157\n",
      "training loss = 7.77573299407959\n",
      "validation loss = 7.623162771526136\n",
      "Batch 56 / 157\n",
      "training loss = 7.559464931488037\n",
      "validation loss = 7.6185197830200195\n",
      "Batch 57 / 157\n",
      "training loss = 7.6214823722839355\n",
      "validation loss = 7.621721342990273\n",
      "Batch 58 / 157\n",
      "training loss = 7.612531661987305\n",
      "validation loss = 7.612645550778038\n",
      "Batch 59 / 157\n",
      "training loss = 7.771889686584473\n",
      "validation loss = 7.612445856395521\n",
      "Batch 60 / 157\n",
      "training loss = 7.660180568695068\n",
      "validation loss = 7.611883916352925\n",
      "Batch 61 / 157\n",
      "training loss = 7.811535358428955\n",
      "validation loss = 7.619343330985622\n",
      "Batch 62 / 157\n",
      "training loss = 7.727262020111084\n",
      "validation loss = 7.617232623853181\n",
      "Batch 63 / 157\n",
      "training loss = 7.407489776611328\n",
      "validation loss = 7.609024599978798\n",
      "Batch 64 / 157\n",
      "training loss = 7.632286071777344\n",
      "validation loss = 7.603143139889366\n",
      "Batch 65 / 157\n",
      "training loss = 7.626802921295166\n",
      "validation loss = 7.6010111507616545\n",
      "Batch 66 / 157\n",
      "training loss = 7.531656265258789\n",
      "validation loss = 7.602519562369899\n",
      "Batch 67 / 157\n",
      "training loss = 7.402400970458984\n",
      "validation loss = 7.605711886757298\n",
      "Batch 68 / 157\n",
      "training loss = 7.7590460777282715\n",
      "validation loss = 7.598381870671322\n",
      "Batch 69 / 157\n",
      "training loss = 7.799532413482666\n",
      "validation loss = 7.591634976236444\n",
      "Batch 70 / 157\n",
      "training loss = 7.725904941558838\n",
      "validation loss = 7.599173470547325\n",
      "Batch 71 / 157\n",
      "training loss = 7.5824384689331055\n",
      "validation loss = 7.591177739595112\n",
      "Batch 72 / 157\n",
      "training loss = 7.539180278778076\n",
      "validation loss = 7.585623339602821\n",
      "Batch 73 / 157\n",
      "training loss = 7.492468357086182\n",
      "validation loss = 7.57885722110146\n",
      "Batch 74 / 157\n",
      "training loss = 7.680511474609375\n",
      "validation loss = 7.584887178320634\n",
      "Batch 75 / 157\n",
      "training loss = 7.732901096343994\n",
      "validation loss = 7.580480023434288\n",
      "Batch 76 / 157\n",
      "training loss = 7.934219837188721\n",
      "validation loss = 7.579087081708406\n",
      "Batch 77 / 157\n",
      "training loss = 7.4859161376953125\n",
      "validation loss = 7.576147255144622\n",
      "Batch 78 / 157\n",
      "training loss = 7.847202777862549\n",
      "validation loss = 7.580576143766704\n",
      "Batch 79 / 157\n",
      "training loss = 7.641103744506836\n",
      "validation loss = 7.579752294640792\n",
      "Batch 80 / 157\n",
      "training loss = 7.63074254989624\n",
      "validation loss = 7.588691761619167\n",
      "Batch 81 / 157\n",
      "training loss = 7.511941432952881\n",
      "validation loss = 7.58115627891139\n",
      "Batch 82 / 157\n",
      "training loss = 7.571870803833008\n",
      "validation loss = 7.570892760628148\n",
      "Batch 83 / 157\n",
      "training loss = 7.544665336608887\n",
      "validation loss = 7.567844692029451\n",
      "Batch 84 / 157\n",
      "training loss = 7.709883689880371\n",
      "validation loss = 7.569099275689376\n",
      "Batch 85 / 157\n",
      "training loss = 7.710616588592529\n",
      "validation loss = 7.560688596022756\n",
      "Batch 86 / 157\n",
      "training loss = 7.370206832885742\n",
      "validation loss = 7.558533292067678\n",
      "Batch 87 / 157\n",
      "training loss = 7.454837799072266\n",
      "validation loss = 7.555291000165437\n",
      "Batch 88 / 157\n",
      "training loss = 7.491872787475586\n",
      "validation loss = 7.550616716083727\n",
      "Batch 89 / 157\n",
      "training loss = 7.493983745574951\n",
      "validation loss = 7.542899809385601\n",
      "Batch 90 / 157\n",
      "training loss = 7.664485931396484\n",
      "validation loss = 7.538991551650198\n",
      "Batch 91 / 157\n",
      "training loss = 7.701601982116699\n",
      "validation loss = 7.534628315975792\n",
      "Batch 92 / 157\n",
      "training loss = 7.574143886566162\n",
      "validation loss = 7.535459769399543\n",
      "Batch 93 / 157\n",
      "training loss = 7.512028694152832\n",
      "validation loss = 7.530657015348735\n",
      "Batch 94 / 157\n",
      "training loss = 7.6667561531066895\n",
      "validation loss = 7.536730716103001\n",
      "Batch 95 / 157\n",
      "training loss = 7.629922866821289\n",
      "validation loss = 7.532144521412096\n",
      "Batch 96 / 157\n",
      "training loss = 7.518539905548096\n",
      "validation loss = 7.5208493282920434\n",
      "Batch 97 / 157\n",
      "training loss = 7.56017541885376\n",
      "validation loss = 7.51975531327097\n",
      "Batch 98 / 157\n",
      "training loss = 7.503253936767578\n",
      "validation loss = 7.513461539619847\n",
      "Batch 99 / 157\n",
      "training loss = 7.493500709533691\n",
      "validation loss = 7.505790810835989\n",
      "Batch 100 / 157\n",
      "training loss = 7.71571683883667\n",
      "validation loss = 7.50424703798796\n",
      "Batch 101 / 157\n",
      "training loss = 7.880190849304199\n",
      "validation loss = 7.511399294200697\n",
      "Batch 102 / 157\n",
      "training loss = 7.5633063316345215\n",
      "validation loss = 7.510081517068963\n",
      "Batch 103 / 157\n",
      "training loss = 7.467832088470459\n",
      "validation loss = 7.491336722122996\n",
      "Batch 104 / 157\n",
      "training loss = 7.381992340087891\n",
      "validation loss = 7.487204752470317\n",
      "Batch 105 / 157\n",
      "training loss = 7.306295871734619\n",
      "validation loss = 7.49381715372989\n",
      "Batch 106 / 157\n",
      "training loss = 7.4680681228637695\n",
      "validation loss = 7.493333565561395\n",
      "Batch 107 / 157\n",
      "training loss = 7.778520584106445\n",
      "validation loss = 7.491937035008481\n",
      "Batch 108 / 157\n",
      "training loss = 7.8088836669921875\n",
      "validation loss = 7.486024906760768\n",
      "Batch 109 / 157\n",
      "training loss = 7.411612510681152\n",
      "validation loss = 7.47638305864836\n",
      "Batch 110 / 157\n",
      "training loss = 7.490090847015381\n",
      "validation loss = 7.477917972363923\n",
      "Batch 111 / 157\n",
      "training loss = 7.311595916748047\n",
      "validation loss = 7.472914971803364\n",
      "Batch 112 / 157\n",
      "training loss = 7.56894063949585\n",
      "validation loss = 7.470401111402009\n",
      "Batch 113 / 157\n",
      "training loss = 7.543813228607178\n",
      "validation loss = 7.467015567578767\n",
      "Batch 114 / 157\n",
      "training loss = 7.510390758514404\n",
      "validation loss = 7.460171473653693\n",
      "Batch 115 / 157\n",
      "training loss = 7.25414514541626\n",
      "validation loss = 7.459731804697137\n",
      "Batch 116 / 157\n",
      "training loss = 7.547479152679443\n",
      "validation loss = 7.453273898676822\n",
      "Batch 117 / 157\n",
      "training loss = 7.355607509613037\n",
      "validation loss = 7.455954024666234\n",
      "Batch 118 / 157\n",
      "training loss = 7.361973762512207\n",
      "validation loss = 7.455358379765561\n",
      "Batch 119 / 157\n",
      "training loss = 7.596252918243408\n",
      "validation loss = 7.453817844390869\n",
      "Batch 120 / 157\n",
      "training loss = 7.150216102600098\n",
      "validation loss = 7.451798037478798\n",
      "Batch 121 / 157\n",
      "training loss = 7.40976619720459\n",
      "validation loss = 7.4446919591803296\n",
      "Batch 122 / 157\n",
      "training loss = 7.548983097076416\n",
      "validation loss = 7.440229114733245\n",
      "Batch 123 / 157\n",
      "training loss = 7.821632385253906\n",
      "validation loss = 7.444406685076262\n",
      "Batch 124 / 157\n",
      "training loss = 7.329784870147705\n",
      "validation loss = 7.4419959720812345\n",
      "Batch 125 / 157\n",
      "training loss = 7.384712219238281\n",
      "validation loss = 7.437775762457597\n",
      "Batch 126 / 157\n",
      "training loss = 7.174830913543701\n",
      "validation loss = 7.437723360563579\n",
      "Batch 127 / 157\n",
      "training loss = 7.6736531257629395\n",
      "validation loss = 7.430886946226421\n",
      "Batch 128 / 157\n",
      "training loss = 7.245368480682373\n",
      "validation loss = 7.427285771620901\n",
      "Batch 129 / 157\n",
      "training loss = 7.608905792236328\n",
      "validation loss = 7.423164191998933\n",
      "Batch 130 / 157\n",
      "training loss = 7.528716564178467\n",
      "validation loss = 7.421649430927477\n",
      "Batch 131 / 157\n",
      "training loss = 7.384350299835205\n",
      "validation loss = 7.431685548079641\n",
      "Batch 132 / 157\n",
      "training loss = 7.599827289581299\n",
      "validation loss = 7.429286806206954\n",
      "Batch 133 / 157\n",
      "training loss = 7.4757561683654785\n",
      "validation loss = 7.421353917372854\n",
      "Batch 134 / 157\n",
      "training loss = 7.203888893127441\n",
      "validation loss = 7.414394052405107\n",
      "Batch 135 / 157\n",
      "training loss = 7.030015468597412\n",
      "validation loss = 7.422089978268272\n",
      "Batch 136 / 157\n",
      "training loss = 7.383704662322998\n",
      "validation loss = 7.418041505311665\n",
      "Batch 137 / 157\n",
      "training loss = 7.5669145584106445\n",
      "validation loss = 7.405327822032728\n",
      "Batch 138 / 157\n",
      "training loss = 7.5904741287231445\n",
      "validation loss = 7.406077811592503\n",
      "Batch 139 / 157\n",
      "training loss = 7.424862384796143\n",
      "validation loss = 7.418000723186292\n",
      "Batch 140 / 157\n",
      "training loss = 7.3941473960876465\n",
      "validation loss = 7.409159735629433\n",
      "Batch 141 / 157\n",
      "training loss = 7.478780269622803\n",
      "validation loss = 7.39634172539962\n",
      "Batch 142 / 157\n",
      "training loss = 7.575039386749268\n",
      "validation loss = 7.392241603449771\n",
      "Batch 143 / 157\n",
      "training loss = 7.538420677185059\n",
      "validation loss = 7.393459094198127\n",
      "Batch 144 / 157\n",
      "training loss = 7.449554920196533\n",
      "validation loss = 7.38322471317492\n",
      "Batch 145 / 157\n",
      "training loss = 7.370495796203613\n",
      "validation loss = 7.383153589148271\n",
      "Batch 146 / 157\n",
      "training loss = 7.444486618041992\n",
      "validation loss = 7.383862997356214\n",
      "Batch 147 / 157\n",
      "training loss = 7.589311599731445\n",
      "validation loss = 7.386161201878598\n",
      "Batch 148 / 157\n",
      "training loss = 7.462564945220947\n",
      "validation loss = 7.38533363844219\n",
      "Batch 149 / 157\n",
      "training loss = 7.268379211425781\n",
      "validation loss = 7.379573671441329\n",
      "Batch 150 / 157\n",
      "training loss = 7.216918468475342\n",
      "validation loss = 7.375241078828511\n",
      "Batch 151 / 157\n",
      "training loss = 7.370769500732422\n",
      "validation loss = 7.368300161863628\n",
      "Batch 152 / 157\n",
      "training loss = 7.429030895233154\n",
      "validation loss = 7.371487391622443\n",
      "Batch 153 / 157\n",
      "training loss = 7.409978866577148\n",
      "validation loss = 7.37056827545166\n",
      "Batch 154 / 157\n",
      "training loss = 7.343947887420654\n",
      "validation loss = 7.368610683240388\n",
      "Batch 155 / 157\n",
      "training loss = 7.2269134521484375\n",
      "validation loss = 7.371586222397654\n",
      "Batch 156 / 157\n",
      "training loss = 7.2575178146362305\n",
      "validation loss = 7.368350556022243\n",
      "Batch 157 / 157\n",
      "training loss = 7.493429183959961\n",
      "validation loss = 7.3599474304600765\n",
      "Average training loss: 7.83\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.377095222473145\n",
      "validation loss = 10.323474582872892\n",
      "Batch 2 / 157\n",
      "training loss = 10.31806755065918\n",
      "validation loss = 10.189217868604159\n",
      "Batch 3 / 157\n",
      "training loss = 10.192627906799316\n",
      "validation loss = 10.073974057247764\n",
      "Batch 4 / 157\n",
      "training loss = 10.0573091506958\n",
      "validation loss = 9.957713177329616\n",
      "Batch 5 / 157\n",
      "training loss = 9.937784194946289\n",
      "validation loss = 9.839883653741134\n",
      "Batch 6 / 157\n",
      "training loss = 9.853880882263184\n",
      "validation loss = 9.722107234754061\n",
      "Batch 7 / 157\n",
      "training loss = 9.726978302001953\n",
      "validation loss = 9.606636950844212\n",
      "Batch 8 / 157\n",
      "training loss = 9.550264358520508\n",
      "validation loss = 9.491362923070005\n",
      "Batch 9 / 157\n",
      "training loss = 9.57098388671875\n",
      "validation loss = 9.381428467599969\n",
      "Batch 10 / 157\n",
      "training loss = 9.314168930053711\n",
      "validation loss = 9.271042020697342\n",
      "Batch 11 / 157\n",
      "training loss = 9.285869598388672\n",
      "validation loss = 9.163861726459704\n",
      "Batch 12 / 157\n",
      "training loss = 9.268684387207031\n",
      "validation loss = 9.062543166311164\n",
      "Batch 13 / 157\n",
      "training loss = 9.059093475341797\n",
      "validation loss = 8.96377548418547\n",
      "Batch 14 / 157\n",
      "training loss = 8.924854278564453\n",
      "validation loss = 8.867646819666811\n",
      "Batch 15 / 157\n",
      "training loss = 8.812531471252441\n",
      "validation loss = 8.773669042085347\n",
      "Batch 16 / 157\n",
      "training loss = 8.765347480773926\n",
      "validation loss = 8.686310216000205\n",
      "Batch 17 / 157\n",
      "training loss = 8.658233642578125\n",
      "validation loss = 8.603761974133944\n",
      "Batch 18 / 157\n",
      "training loss = 8.527996063232422\n",
      "validation loss = 8.525077418277139\n",
      "Batch 19 / 157\n",
      "training loss = 8.431873321533203\n",
      "validation loss = 8.44790940535696\n",
      "Batch 20 / 157\n",
      "training loss = 8.4306640625\n",
      "validation loss = 8.375331075567948\n",
      "Batch 21 / 157\n",
      "training loss = 8.323031425476074\n",
      "validation loss = 8.310430978473864\n",
      "Batch 22 / 157\n",
      "training loss = 8.353270530700684\n",
      "validation loss = 8.24680192847001\n",
      "Batch 23 / 157\n",
      "training loss = 8.309859275817871\n",
      "validation loss = 8.191814221833882\n",
      "Batch 24 / 157\n",
      "training loss = 8.141392707824707\n",
      "validation loss = 8.129580322064852\n",
      "Batch 25 / 157\n",
      "training loss = 7.969823837280273\n",
      "validation loss = 8.080356271643387\n",
      "Batch 26 / 157\n",
      "training loss = 8.19190502166748\n",
      "validation loss = 8.03000337199161\n",
      "Batch 27 / 157\n",
      "training loss = 7.993532657623291\n",
      "validation loss = 7.9849191966809725\n",
      "Batch 28 / 157\n",
      "training loss = 7.858325958251953\n",
      "validation loss = 7.940635681152344\n",
      "Batch 29 / 157\n",
      "training loss = 7.908863544464111\n",
      "validation loss = 7.901236960762425\n",
      "Batch 30 / 157\n",
      "training loss = 7.827920436859131\n",
      "validation loss = 7.872655542273271\n",
      "Batch 31 / 157\n",
      "training loss = 7.731293201446533\n",
      "validation loss = 7.830037644034938\n",
      "Batch 32 / 157\n",
      "training loss = 7.716090679168701\n",
      "validation loss = 7.798082803425036\n",
      "Batch 33 / 157\n",
      "training loss = 7.845400333404541\n",
      "validation loss = 7.76939394599513\n",
      "Batch 34 / 157\n",
      "training loss = 7.835757732391357\n",
      "validation loss = 7.7509771146272355\n",
      "Batch 35 / 157\n",
      "training loss = 7.668920040130615\n",
      "validation loss = 7.723245394857306\n",
      "Batch 36 / 157\n",
      "training loss = 7.844470024108887\n",
      "validation loss = 7.697845634661223\n",
      "Batch 37 / 157\n",
      "training loss = 7.692710876464844\n",
      "validation loss = 7.678313205116673\n",
      "Batch 38 / 157\n",
      "training loss = 7.541693687438965\n",
      "validation loss = 7.670492548691599\n",
      "Batch 39 / 157\n",
      "training loss = 7.718660354614258\n",
      "validation loss = 7.6528221431531405\n",
      "Batch 40 / 157\n",
      "training loss = 7.7003865242004395\n",
      "validation loss = 7.640692208942614\n",
      "Batch 41 / 157\n",
      "training loss = 7.544399738311768\n",
      "validation loss = 7.633667870571739\n",
      "Batch 42 / 157\n",
      "training loss = 7.581884384155273\n",
      "validation loss = 7.6216151839808415\n",
      "Batch 43 / 157\n",
      "training loss = 7.542538642883301\n",
      "validation loss = 7.611873902772603\n",
      "Batch 44 / 157\n",
      "training loss = 7.633971691131592\n",
      "validation loss = 7.609832788768568\n",
      "Batch 45 / 157\n",
      "training loss = 7.490920066833496\n",
      "validation loss = 7.605449977673982\n",
      "Batch 46 / 157\n",
      "training loss = 7.655007839202881\n",
      "validation loss = 7.601476619118138\n",
      "Batch 47 / 157\n",
      "training loss = 7.513667106628418\n",
      "validation loss = 7.594825443468596\n",
      "Batch 48 / 157\n",
      "training loss = 7.68947172164917\n",
      "validation loss = 7.593436266246595\n",
      "Batch 49 / 157\n",
      "training loss = 7.443982124328613\n",
      "validation loss = 7.5978446759675675\n",
      "Batch 50 / 157\n",
      "training loss = 7.600991725921631\n",
      "validation loss = 7.592565938046104\n",
      "Batch 51 / 157\n",
      "training loss = 7.3841023445129395\n",
      "validation loss = 7.589822342521266\n",
      "Batch 52 / 157\n",
      "training loss = 7.461137771606445\n",
      "validation loss = 7.59608436885633\n",
      "Batch 53 / 157\n",
      "training loss = 7.488181114196777\n",
      "validation loss = 7.60804725948133\n",
      "Batch 54 / 157\n",
      "training loss = 7.438864231109619\n",
      "validation loss = 7.592600094644647\n",
      "Batch 55 / 157\n",
      "training loss = 7.593634128570557\n",
      "validation loss = 7.586892880891499\n",
      "Batch 56 / 157\n",
      "training loss = 7.577916145324707\n",
      "validation loss = 7.57897750954879\n",
      "Batch 57 / 157\n",
      "training loss = 7.543387413024902\n",
      "validation loss = 7.57494246332269\n",
      "Batch 58 / 157\n",
      "training loss = 7.652145862579346\n",
      "validation loss = 7.572790547421104\n",
      "Batch 59 / 157\n",
      "training loss = 7.614384174346924\n",
      "validation loss = 7.577338795912893\n",
      "Batch 60 / 157\n",
      "training loss = 7.56799840927124\n",
      "validation loss = 7.575956796344958\n",
      "Batch 61 / 157\n",
      "training loss = 7.726235866546631\n",
      "validation loss = 7.5669425914162085\n",
      "Batch 62 / 157\n",
      "training loss = 7.424196243286133\n",
      "validation loss = 7.560672483946147\n",
      "Batch 63 / 157\n",
      "training loss = 7.5315775871276855\n",
      "validation loss = 7.557798937747353\n",
      "Batch 64 / 157\n",
      "training loss = 7.619359493255615\n",
      "validation loss = 7.555333087318822\n",
      "Batch 65 / 157\n",
      "training loss = 7.463159561157227\n",
      "validation loss = 7.546902280104788\n",
      "Batch 66 / 157\n",
      "training loss = 7.261523246765137\n",
      "validation loss = 7.547001010493228\n",
      "Batch 67 / 157\n",
      "training loss = 7.435981273651123\n",
      "validation loss = 7.538396709843686\n",
      "Batch 68 / 157\n",
      "training loss = 7.572185516357422\n",
      "validation loss = 7.537041563736765\n",
      "Batch 69 / 157\n",
      "training loss = 7.771634578704834\n",
      "validation loss = 7.554429606387489\n",
      "Batch 70 / 157\n",
      "training loss = 7.444327354431152\n",
      "validation loss = 7.546170937387567\n",
      "Batch 71 / 157\n",
      "training loss = 7.424611568450928\n",
      "validation loss = 7.54379139448467\n",
      "Batch 72 / 157\n",
      "training loss = 7.578765869140625\n",
      "validation loss = 7.526317496048777\n",
      "Batch 73 / 157\n",
      "training loss = 7.457233905792236\n",
      "validation loss = 7.516265467593544\n",
      "Batch 74 / 157\n",
      "training loss = 7.357661724090576\n",
      "validation loss = 7.518829546476665\n",
      "Batch 75 / 157\n",
      "training loss = 7.4866943359375\n",
      "validation loss = 7.518060081883481\n",
      "Batch 76 / 157\n",
      "training loss = 7.613589763641357\n",
      "validation loss = 7.515399782281173\n",
      "Batch 77 / 157\n",
      "training loss = 8.136048316955566\n",
      "validation loss = 7.51909697683234\n",
      "Batch 78 / 157\n",
      "training loss = 7.316054821014404\n",
      "validation loss = 7.506086249100535\n",
      "Batch 79 / 157\n",
      "training loss = 7.298182010650635\n",
      "validation loss = 7.506008625030518\n",
      "Batch 80 / 157\n",
      "training loss = 7.720123767852783\n",
      "validation loss = 7.496936296161852\n",
      "Batch 81 / 157\n",
      "training loss = 7.333768367767334\n",
      "validation loss = 7.49238365574887\n",
      "Batch 82 / 157\n",
      "training loss = 7.493259906768799\n",
      "validation loss = 7.489397099143581\n",
      "Batch 83 / 157\n",
      "training loss = 7.787200450897217\n",
      "validation loss = 7.4845272365369295\n",
      "Batch 84 / 157\n",
      "training loss = 7.324851989746094\n",
      "validation loss = 7.477719557912726\n",
      "Batch 85 / 157\n",
      "training loss = 7.581601142883301\n",
      "validation loss = 7.469136614548533\n",
      "Batch 86 / 157\n",
      "training loss = 7.318950176239014\n",
      "validation loss = 7.477006109137284\n",
      "Batch 87 / 157\n",
      "training loss = 7.637502670288086\n",
      "validation loss = 7.46899948622051\n",
      "Batch 88 / 157\n",
      "training loss = 7.417901515960693\n",
      "validation loss = 7.462783738186485\n",
      "Batch 89 / 157\n",
      "training loss = 7.368391513824463\n",
      "validation loss = 7.462006870069001\n",
      "Batch 90 / 157\n",
      "training loss = 7.331544876098633\n",
      "validation loss = 7.459222768482409\n",
      "Batch 91 / 157\n",
      "training loss = 7.642506122589111\n",
      "validation loss = 7.452358120366147\n",
      "Batch 92 / 157\n",
      "training loss = 7.223855972290039\n",
      "validation loss = 7.448756895567241\n",
      "Batch 93 / 157\n",
      "training loss = 7.422820568084717\n",
      "validation loss = 7.440662384033203\n",
      "Batch 94 / 157\n",
      "training loss = 7.532196521759033\n",
      "validation loss = 7.441776752471924\n",
      "Batch 95 / 157\n",
      "training loss = 7.427041053771973\n",
      "validation loss = 7.433220963729055\n",
      "Batch 96 / 157\n",
      "training loss = 7.5366387367248535\n",
      "validation loss = 7.432885521336606\n",
      "Batch 97 / 157\n",
      "training loss = 7.589672565460205\n",
      "validation loss = 7.429182403966\n",
      "Batch 98 / 157\n",
      "training loss = 7.622396469116211\n",
      "validation loss = 7.424758760552657\n",
      "Batch 99 / 157\n",
      "training loss = 7.530797004699707\n",
      "validation loss = 7.422055746379652\n",
      "Batch 100 / 157\n",
      "training loss = 7.729314804077148\n",
      "validation loss = 7.425854406858745\n",
      "Batch 101 / 157\n",
      "training loss = 7.340450286865234\n",
      "validation loss = 7.431656435916298\n",
      "Batch 102 / 157\n",
      "training loss = 7.4482831954956055\n",
      "validation loss = 7.4137319263659025\n",
      "Batch 103 / 157\n",
      "training loss = 7.338201522827148\n",
      "validation loss = 7.400505065917969\n",
      "Batch 104 / 157\n",
      "training loss = 7.3080596923828125\n",
      "validation loss = 7.4046140219035905\n",
      "Batch 105 / 157\n",
      "training loss = 7.4579644203186035\n",
      "validation loss = 7.40442449168155\n",
      "Batch 106 / 157\n",
      "training loss = 7.320319652557373\n",
      "validation loss = 7.393330900292647\n",
      "Batch 107 / 157\n",
      "training loss = 7.4670729637146\n",
      "validation loss = 7.387788948259856\n",
      "Batch 108 / 157\n",
      "training loss = 7.365853786468506\n",
      "validation loss = 7.389115684910824\n",
      "Batch 109 / 157\n",
      "training loss = 7.345534324645996\n",
      "validation loss = 7.386741889150519\n",
      "Batch 110 / 157\n",
      "training loss = 7.244316101074219\n",
      "validation loss = 7.382247874611302\n",
      "Batch 111 / 157\n",
      "training loss = 7.306839942932129\n",
      "validation loss = 7.375068965711091\n",
      "Batch 112 / 157\n",
      "training loss = 7.319606781005859\n",
      "validation loss = 7.365034781004253\n",
      "Batch 113 / 157\n",
      "training loss = 7.579056262969971\n",
      "validation loss = 7.370696268583599\n",
      "Batch 114 / 157\n",
      "training loss = 7.408001899719238\n",
      "validation loss = 7.36317589408473\n",
      "Batch 115 / 157\n",
      "training loss = 7.385528564453125\n",
      "validation loss = 7.360415283002351\n",
      "Batch 116 / 157\n",
      "training loss = 7.291626453399658\n",
      "validation loss = 7.35158089587563\n",
      "Batch 117 / 157\n",
      "training loss = 7.379589080810547\n",
      "validation loss = 7.34590839084826\n",
      "Batch 118 / 157\n",
      "training loss = 7.289389610290527\n",
      "validation loss = 7.34807202690526\n",
      "Batch 119 / 157\n",
      "training loss = 7.282444477081299\n",
      "validation loss = 7.345585321125231\n",
      "Batch 120 / 157\n",
      "training loss = 7.230698108673096\n",
      "validation loss = 7.348210359874525\n",
      "Batch 121 / 157\n",
      "training loss = 7.472563743591309\n",
      "validation loss = 7.336668641943681\n",
      "Batch 122 / 157\n",
      "training loss = 7.324009418487549\n",
      "validation loss = 7.337507950632196\n",
      "Batch 123 / 157\n",
      "training loss = 7.511754035949707\n",
      "validation loss = 7.332928030114425\n",
      "Batch 124 / 157\n",
      "training loss = 7.199387550354004\n",
      "validation loss = 7.323308819218686\n",
      "Batch 125 / 157\n",
      "training loss = 7.495186805725098\n",
      "validation loss = 7.325376159266422\n",
      "Batch 126 / 157\n",
      "training loss = 7.2536940574646\n",
      "validation loss = 7.32632935674567\n",
      "Batch 127 / 157\n",
      "training loss = 7.444235324859619\n",
      "validation loss = 7.325897191700182\n",
      "Batch 128 / 157\n",
      "training loss = 7.422136306762695\n",
      "validation loss = 7.317160606384277\n",
      "Batch 129 / 157\n",
      "training loss = 7.443969249725342\n",
      "validation loss = 7.314024021751003\n",
      "Batch 130 / 157\n",
      "training loss = 7.242713928222656\n",
      "validation loss = 7.31370042499743\n",
      "Batch 131 / 157\n",
      "training loss = 7.234532356262207\n",
      "validation loss = 7.312592380925229\n",
      "Batch 132 / 157\n",
      "training loss = 7.582491874694824\n",
      "validation loss = 7.311207369754189\n",
      "Batch 133 / 157\n",
      "training loss = 7.544265270233154\n",
      "validation loss = 7.303047983269942\n",
      "Batch 134 / 157\n",
      "training loss = 7.1215291023254395\n",
      "validation loss = 7.300859074843557\n",
      "Batch 135 / 157\n",
      "training loss = 7.125163555145264\n",
      "validation loss = 7.29742589749788\n",
      "Batch 136 / 157\n",
      "training loss = 7.012558460235596\n",
      "validation loss = 7.293082688984118\n",
      "Batch 137 / 157\n",
      "training loss = 7.26774263381958\n",
      "validation loss = 7.292885956011321\n",
      "Batch 138 / 157\n",
      "training loss = 7.417110919952393\n",
      "validation loss = 7.281212580831427\n",
      "Batch 139 / 157\n",
      "training loss = 7.527462482452393\n",
      "validation loss = 7.290316054695531\n",
      "Batch 140 / 157\n",
      "training loss = 7.386116027832031\n",
      "validation loss = 7.286747631273772\n",
      "Batch 141 / 157\n",
      "training loss = 7.528858661651611\n",
      "validation loss = 7.294038220455772\n",
      "Batch 142 / 157\n",
      "training loss = 6.880251884460449\n",
      "validation loss = 7.296029868878816\n",
      "Batch 143 / 157\n",
      "training loss = 7.203293800354004\n",
      "validation loss = 7.29248541279843\n",
      "Batch 144 / 157\n",
      "training loss = 7.469559192657471\n",
      "validation loss = 7.2867676584344165\n",
      "Batch 145 / 157\n",
      "training loss = 6.957110404968262\n",
      "validation loss = 7.288601674531636\n",
      "Batch 146 / 157\n",
      "training loss = 7.406227111816406\n",
      "validation loss = 7.275511365187795\n",
      "Batch 147 / 157\n",
      "training loss = 7.427027702331543\n",
      "validation loss = 7.283435972113359\n",
      "Batch 148 / 157\n",
      "training loss = 7.505893707275391\n",
      "validation loss = 7.279662483616879\n",
      "Batch 149 / 157\n",
      "training loss = 7.348761558532715\n",
      "validation loss = 7.273643744619269\n",
      "Batch 150 / 157\n",
      "training loss = 7.291136741638184\n",
      "validation loss = 7.2744242517571704\n",
      "Batch 151 / 157\n",
      "training loss = 7.162041664123535\n",
      "validation loss = 7.271512382908871\n",
      "Batch 152 / 157\n",
      "training loss = 7.333723545074463\n",
      "validation loss = 7.27369321020026\n",
      "Batch 153 / 157\n",
      "training loss = 7.282289505004883\n",
      "validation loss = 7.273646781319066\n",
      "Batch 154 / 157\n",
      "training loss = 7.283183574676514\n",
      "validation loss = 7.267717712803891\n",
      "Batch 155 / 157\n",
      "training loss = 7.573115825653076\n",
      "validation loss = 7.259413719177246\n",
      "Batch 156 / 157\n",
      "training loss = 7.12823486328125\n",
      "validation loss = 7.255676144047787\n",
      "Batch 157 / 157\n",
      "training loss = 7.134988784790039\n",
      "validation loss = 7.251032979864823\n",
      "Average training loss: 7.74\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.361259460449219\n",
      "validation loss = 10.298287542242752\n",
      "Batch 2 / 157\n",
      "training loss = 10.307756423950195\n",
      "validation loss = 10.193703199687757\n",
      "Batch 3 / 157\n",
      "training loss = 10.194408416748047\n",
      "validation loss = 10.105014048124614\n",
      "Batch 4 / 157\n",
      "training loss = 10.098917007446289\n",
      "validation loss = 10.008272421987433\n",
      "Batch 5 / 157\n",
      "training loss = 10.009038925170898\n",
      "validation loss = 9.906190571032072\n",
      "Batch 6 / 157\n",
      "training loss = 9.89923095703125\n",
      "validation loss = 9.80131314930163\n",
      "Batch 7 / 157\n",
      "training loss = 9.791267395019531\n",
      "validation loss = 9.693331668251439\n",
      "Batch 8 / 157\n",
      "training loss = 9.676019668579102\n",
      "validation loss = 9.582856931184468\n",
      "Batch 9 / 157\n",
      "training loss = 9.54557991027832\n",
      "validation loss = 9.474088769210013\n",
      "Batch 10 / 157\n",
      "training loss = 9.510658264160156\n",
      "validation loss = 9.366138558638724\n",
      "Batch 11 / 157\n",
      "training loss = 9.448951721191406\n",
      "validation loss = 9.257282608433774\n",
      "Batch 12 / 157\n",
      "training loss = 9.356605529785156\n",
      "validation loss = 9.149789458826968\n",
      "Batch 13 / 157\n",
      "training loss = 9.169207572937012\n",
      "validation loss = 9.044336318969727\n",
      "Batch 14 / 157\n",
      "training loss = 8.988215446472168\n",
      "validation loss = 8.943117593464098\n",
      "Batch 15 / 157\n",
      "training loss = 9.0418701171875\n",
      "validation loss = 8.843958553514982\n",
      "Batch 16 / 157\n",
      "training loss = 8.799034118652344\n",
      "validation loss = 8.749194195396022\n",
      "Batch 17 / 157\n",
      "training loss = 8.75468921661377\n",
      "validation loss = 8.65867449107923\n",
      "Batch 18 / 157\n",
      "training loss = 8.7138032913208\n",
      "validation loss = 8.568134558828254\n",
      "Batch 19 / 157\n",
      "training loss = 8.559782028198242\n",
      "validation loss = 8.487115307858115\n",
      "Batch 20 / 157\n",
      "training loss = 8.609061241149902\n",
      "validation loss = 8.41109185469778\n",
      "Batch 21 / 157\n",
      "training loss = 8.400651931762695\n",
      "validation loss = 8.339096119529323\n",
      "Batch 22 / 157\n",
      "training loss = 8.344439506530762\n",
      "validation loss = 8.268661749990363\n",
      "Batch 23 / 157\n",
      "training loss = 8.277579307556152\n",
      "validation loss = 8.20631604445608\n",
      "Batch 24 / 157\n",
      "training loss = 8.355522155761719\n",
      "validation loss = 8.146040339218942\n",
      "Batch 25 / 157\n",
      "training loss = 8.065994262695312\n",
      "validation loss = 8.098233599411813\n",
      "Batch 26 / 157\n",
      "training loss = 8.00552749633789\n",
      "validation loss = 8.048818789030376\n",
      "Batch 27 / 157\n",
      "training loss = 8.159361839294434\n",
      "validation loss = 8.008732093007941\n",
      "Batch 28 / 157\n",
      "training loss = 8.115426063537598\n",
      "validation loss = 7.967767037843403\n",
      "Batch 29 / 157\n",
      "training loss = 7.833454132080078\n",
      "validation loss = 7.935955750314813\n",
      "Batch 30 / 157\n",
      "training loss = 7.965914726257324\n",
      "validation loss = 7.908560075257954\n",
      "Batch 31 / 157\n",
      "training loss = 7.828070640563965\n",
      "validation loss = 7.879717902133339\n",
      "Batch 32 / 157\n",
      "training loss = 7.999944686889648\n",
      "validation loss = 7.856264591217041\n",
      "Batch 33 / 157\n",
      "training loss = 7.87086296081543\n",
      "validation loss = 7.836366929506001\n",
      "Batch 34 / 157\n",
      "training loss = 8.226799011230469\n",
      "validation loss = 7.824884464866237\n",
      "Batch 35 / 157\n",
      "training loss = 7.638675212860107\n",
      "validation loss = 7.809064137308221\n",
      "Batch 36 / 157\n",
      "training loss = 8.112584114074707\n",
      "validation loss = 7.797933829458136\n",
      "Batch 37 / 157\n",
      "training loss = 7.715968608856201\n",
      "validation loss = 7.790524156470048\n",
      "Batch 38 / 157\n",
      "training loss = 7.799306869506836\n",
      "validation loss = 7.779376381321957\n",
      "Batch 39 / 157\n",
      "training loss = 7.738058567047119\n",
      "validation loss = 7.7724054487128\n",
      "Batch 40 / 157\n",
      "training loss = 7.564752101898193\n",
      "validation loss = 7.770011600695159\n",
      "Batch 41 / 157\n",
      "training loss = 7.901147842407227\n",
      "validation loss = 7.756323312458239\n",
      "Batch 42 / 157\n",
      "training loss = 7.817477703094482\n",
      "validation loss = 7.743607947700902\n",
      "Batch 43 / 157\n",
      "training loss = 7.6779656410217285\n",
      "validation loss = 7.731594662917288\n",
      "Batch 44 / 157\n",
      "training loss = 8.146805763244629\n",
      "validation loss = 7.746476374174419\n",
      "Batch 45 / 157\n",
      "training loss = 7.686498641967773\n",
      "validation loss = 7.731225967407227\n",
      "Batch 46 / 157\n",
      "training loss = 7.69529914855957\n",
      "validation loss = 7.7140603818391495\n",
      "Batch 47 / 157\n",
      "training loss = 7.597631454467773\n",
      "validation loss = 7.709589958190918\n",
      "Batch 48 / 157\n",
      "training loss = 7.669996738433838\n",
      "validation loss = 7.714674071261757\n",
      "Batch 49 / 157\n",
      "training loss = 7.772147178649902\n",
      "validation loss = 7.711802131251285\n",
      "Batch 50 / 157\n",
      "training loss = 7.687291145324707\n",
      "validation loss = 7.704474624834563\n",
      "Batch 51 / 157\n",
      "training loss = 7.630366802215576\n",
      "validation loss = 7.6927847611276725\n",
      "Batch 52 / 157\n",
      "training loss = 8.134724617004395\n",
      "validation loss = 7.684539016924407\n",
      "Batch 53 / 157\n",
      "training loss = 7.757585525512695\n",
      "validation loss = 7.687755735296952\n",
      "Batch 54 / 157\n",
      "training loss = 7.806391716003418\n",
      "validation loss = 7.7018485822175675\n",
      "Batch 55 / 157\n",
      "training loss = 7.88380765914917\n",
      "validation loss = 7.687849170283267\n",
      "Batch 56 / 157\n",
      "training loss = 7.504514694213867\n",
      "validation loss = 7.6692682316428735\n",
      "Batch 57 / 157\n",
      "training loss = 7.72206974029541\n",
      "validation loss = 7.661540859623959\n",
      "Batch 58 / 157\n",
      "training loss = 7.408365249633789\n",
      "validation loss = 7.670751220301578\n",
      "Batch 59 / 157\n",
      "training loss = 7.367631435394287\n",
      "validation loss = 7.675794927697432\n",
      "Batch 60 / 157\n",
      "training loss = 7.684008598327637\n",
      "validation loss = 7.676849164460835\n",
      "Batch 61 / 157\n",
      "training loss = 7.984570026397705\n",
      "validation loss = 7.665193256578948\n",
      "Batch 62 / 157\n",
      "training loss = 7.750354290008545\n",
      "validation loss = 7.6562244013736125\n",
      "Batch 63 / 157\n",
      "training loss = 7.8163628578186035\n",
      "validation loss = 7.6501076597916455\n",
      "Batch 64 / 157\n",
      "training loss = 7.858378887176514\n",
      "validation loss = 7.6561463757565145\n",
      "Batch 65 / 157\n",
      "training loss = 7.565333843231201\n",
      "validation loss = 7.654603380905955\n",
      "Batch 66 / 157\n",
      "training loss = 7.496401786804199\n",
      "validation loss = 7.637331686521831\n",
      "Batch 67 / 157\n",
      "training loss = 7.468028545379639\n",
      "validation loss = 7.627929612209923\n",
      "Batch 68 / 157\n",
      "training loss = 7.590595722198486\n",
      "validation loss = 7.622067953410902\n",
      "Batch 69 / 157\n",
      "training loss = 7.6337890625\n",
      "validation loss = 7.617372312043843\n",
      "Batch 70 / 157\n",
      "training loss = 7.31356954574585\n",
      "validation loss = 7.617788992430034\n",
      "Batch 71 / 157\n",
      "training loss = 8.013162612915039\n",
      "validation loss = 7.621822357177734\n",
      "Batch 72 / 157\n",
      "training loss = 7.7655863761901855\n",
      "validation loss = 7.621083309775905\n",
      "Batch 73 / 157\n",
      "training loss = 7.648004055023193\n",
      "validation loss = 7.624571750038548\n",
      "Batch 74 / 157\n",
      "training loss = 7.739202499389648\n",
      "validation loss = 7.605155919727526\n",
      "Batch 75 / 157\n",
      "training loss = 7.540705680847168\n",
      "validation loss = 7.611177017814235\n",
      "Batch 76 / 157\n",
      "training loss = 7.360705375671387\n",
      "validation loss = 7.619824007937782\n",
      "Batch 77 / 157\n",
      "training loss = 7.575809955596924\n",
      "validation loss = 7.614142543391178\n",
      "Batch 78 / 157\n",
      "training loss = 7.3337931632995605\n",
      "validation loss = 7.613641789084987\n",
      "Batch 79 / 157\n",
      "training loss = 7.6596808433532715\n",
      "validation loss = 7.593745708465576\n",
      "Batch 80 / 157\n",
      "training loss = 7.14877986907959\n",
      "validation loss = 7.5885028086210555\n",
      "Batch 81 / 157\n",
      "training loss = 7.43367338180542\n",
      "validation loss = 7.574216842651367\n",
      "Batch 82 / 157\n",
      "training loss = 7.760495185852051\n",
      "validation loss = 7.592922085209897\n",
      "Batch 83 / 157\n",
      "training loss = 7.547497272491455\n",
      "validation loss = 7.588289436541106\n",
      "Batch 84 / 157\n",
      "training loss = 7.643150806427002\n",
      "validation loss = 7.572533481999447\n",
      "Batch 85 / 157\n",
      "training loss = 7.51115083694458\n",
      "validation loss = 7.576213711186459\n",
      "Batch 86 / 157\n",
      "training loss = 8.038193702697754\n",
      "validation loss = 7.566061546928005\n",
      "Batch 87 / 157\n",
      "training loss = 7.6807074546813965\n",
      "validation loss = 7.558903292605751\n",
      "Batch 88 / 157\n",
      "training loss = 7.681258678436279\n",
      "validation loss = 7.555340164586117\n",
      "Batch 89 / 157\n",
      "training loss = 7.4986958503723145\n",
      "validation loss = 7.552488803863525\n",
      "Batch 90 / 157\n",
      "training loss = 7.119280815124512\n",
      "validation loss = 7.550693085319118\n",
      "Batch 91 / 157\n",
      "training loss = 7.563408851623535\n",
      "validation loss = 7.543385656256425\n",
      "Batch 92 / 157\n",
      "training loss = 7.727454662322998\n",
      "validation loss = 7.5429052804645735\n",
      "Batch 93 / 157\n",
      "training loss = 7.429521083831787\n",
      "validation loss = 7.535259547986482\n",
      "Batch 94 / 157\n",
      "training loss = 7.505695819854736\n",
      "validation loss = 7.534456679695531\n",
      "Batch 95 / 157\n",
      "training loss = 7.258663177490234\n",
      "validation loss = 7.528321366561086\n",
      "Batch 96 / 157\n",
      "training loss = 7.432553291320801\n",
      "validation loss = 7.525784542686061\n",
      "Batch 97 / 157\n",
      "training loss = 7.28907585144043\n",
      "validation loss = 7.523069557390715\n",
      "Batch 98 / 157\n",
      "training loss = 7.583050727844238\n",
      "validation loss = 7.512084157843339\n",
      "Batch 99 / 157\n",
      "training loss = 7.675494194030762\n",
      "validation loss = 7.517409174065841\n",
      "Batch 100 / 157\n",
      "training loss = 7.473926067352295\n",
      "validation loss = 7.515738587630422\n",
      "Batch 101 / 157\n",
      "training loss = 7.308608055114746\n",
      "validation loss = 7.504206230765895\n",
      "Batch 102 / 157\n",
      "training loss = 7.580374240875244\n",
      "validation loss = 7.509540306894403\n",
      "Batch 103 / 157\n",
      "training loss = 7.443159103393555\n",
      "validation loss = 7.50406205026727\n",
      "Batch 104 / 157\n",
      "training loss = 7.771949291229248\n",
      "validation loss = 7.487275173789577\n",
      "Batch 105 / 157\n",
      "training loss = 7.471676349639893\n",
      "validation loss = 7.4876912267584546\n",
      "Batch 106 / 157\n",
      "training loss = 7.470573902130127\n",
      "validation loss = 7.488756706840114\n",
      "Batch 107 / 157\n",
      "training loss = 7.217371940612793\n",
      "validation loss = 7.467762595728824\n",
      "Batch 108 / 157\n",
      "training loss = 7.504398345947266\n",
      "validation loss = 7.46030676992316\n",
      "Batch 109 / 157\n",
      "training loss = 7.276635646820068\n",
      "validation loss = 7.465267407266717\n",
      "Batch 110 / 157\n",
      "training loss = 8.145435333251953\n",
      "validation loss = 7.457377985904091\n",
      "Batch 111 / 157\n",
      "training loss = 7.435008525848389\n",
      "validation loss = 7.447822093963623\n",
      "Batch 112 / 157\n",
      "training loss = 7.1346893310546875\n",
      "validation loss = 7.448962562962582\n",
      "Batch 113 / 157\n",
      "training loss = 7.66252326965332\n",
      "validation loss = 7.443735925774825\n",
      "Batch 114 / 157\n",
      "training loss = 7.2983880043029785\n",
      "validation loss = 7.432316253059788\n",
      "Batch 115 / 157\n",
      "training loss = 7.599661827087402\n",
      "validation loss = 7.426131574731124\n",
      "Batch 116 / 157\n",
      "training loss = 7.5262956619262695\n",
      "validation loss = 7.4254959006058545\n",
      "Batch 117 / 157\n",
      "training loss = 7.426543712615967\n",
      "validation loss = 7.418673339642976\n",
      "Batch 118 / 157\n",
      "training loss = 7.623555660247803\n",
      "validation loss = 7.4154638742145735\n",
      "Batch 119 / 157\n",
      "training loss = 7.431490421295166\n",
      "validation loss = 7.421876154447856\n",
      "Batch 120 / 157\n",
      "training loss = 7.049105167388916\n",
      "validation loss = 7.403064250946045\n",
      "Batch 121 / 157\n",
      "training loss = 7.214516639709473\n",
      "validation loss = 7.399683475494385\n",
      "Batch 122 / 157\n",
      "training loss = 7.45311164855957\n",
      "validation loss = 7.395557503951223\n",
      "Batch 123 / 157\n",
      "training loss = 7.524174690246582\n",
      "validation loss = 7.388632121839021\n",
      "Batch 124 / 157\n",
      "training loss = 7.080774784088135\n",
      "validation loss = 7.386058280342503\n",
      "Batch 125 / 157\n",
      "training loss = 7.35654354095459\n",
      "validation loss = 7.382855314957468\n",
      "Batch 126 / 157\n",
      "training loss = 7.120617866516113\n",
      "validation loss = 7.373973620565314\n",
      "Batch 127 / 157\n",
      "training loss = 7.665811538696289\n",
      "validation loss = 7.374310142115543\n",
      "Batch 128 / 157\n",
      "training loss = 7.231997966766357\n",
      "validation loss = 7.376074816051283\n",
      "Batch 129 / 157\n",
      "training loss = 7.225774765014648\n",
      "validation loss = 7.363469650870876\n",
      "Batch 130 / 157\n",
      "training loss = 7.306879043579102\n",
      "validation loss = 7.356917657350239\n",
      "Batch 131 / 157\n",
      "training loss = 7.301025390625\n",
      "validation loss = 7.356808863188091\n",
      "Batch 132 / 157\n",
      "training loss = 7.31459903717041\n",
      "validation loss = 7.354855738188091\n",
      "Batch 133 / 157\n",
      "training loss = 7.200035095214844\n",
      "validation loss = 7.346684455871582\n",
      "Batch 134 / 157\n",
      "training loss = 7.675594806671143\n",
      "validation loss = 7.343550054650557\n",
      "Batch 135 / 157\n",
      "training loss = 7.340606212615967\n",
      "validation loss = 7.331878461335835\n",
      "Batch 136 / 157\n",
      "training loss = 7.56304931640625\n",
      "validation loss = 7.33523446635196\n",
      "Batch 137 / 157\n",
      "training loss = 7.455265045166016\n",
      "validation loss = 7.325245355304919\n",
      "Batch 138 / 157\n",
      "training loss = 7.276561737060547\n",
      "validation loss = 7.3272859673751025\n",
      "Batch 139 / 157\n",
      "training loss = 7.364132404327393\n",
      "validation loss = 7.322242034109015\n",
      "Batch 140 / 157\n",
      "training loss = 7.347780227661133\n",
      "validation loss = 7.329560681393272\n",
      "Batch 141 / 157\n",
      "training loss = 7.3489909172058105\n",
      "validation loss = 7.316420755888286\n",
      "Batch 142 / 157\n",
      "training loss = 7.276646137237549\n",
      "validation loss = 7.311200869710822\n",
      "Batch 143 / 157\n",
      "training loss = 7.181353569030762\n",
      "validation loss = 7.318860756723504\n",
      "Batch 144 / 157\n",
      "training loss = 7.226896286010742\n",
      "validation loss = 7.313548489620811\n",
      "Batch 145 / 157\n",
      "training loss = 7.483928203582764\n",
      "validation loss = 7.297936665384393\n",
      "Batch 146 / 157\n",
      "training loss = 7.334282398223877\n",
      "validation loss = 7.299202316685727\n",
      "Batch 147 / 157\n",
      "training loss = 7.2204909324646\n",
      "validation loss = 7.298926428744667\n",
      "Batch 148 / 157\n",
      "training loss = 6.8964762687683105\n",
      "validation loss = 7.30048458199752\n",
      "Batch 149 / 157\n",
      "training loss = 7.396399021148682\n",
      "validation loss = 7.299569682071083\n",
      "Batch 150 / 157\n",
      "training loss = 7.167998790740967\n",
      "validation loss = 7.290137366244667\n",
      "Batch 151 / 157\n",
      "training loss = 7.498599529266357\n",
      "validation loss = 7.289564735011051\n",
      "Batch 152 / 157\n",
      "training loss = 7.844691276550293\n",
      "validation loss = 7.305120593623111\n",
      "Batch 153 / 157\n",
      "training loss = 7.3851213455200195\n",
      "validation loss = 7.293882319801732\n",
      "Batch 154 / 157\n",
      "training loss = 6.768894195556641\n",
      "validation loss = 7.278384258872585\n",
      "Batch 155 / 157\n",
      "training loss = 7.29436731338501\n",
      "validation loss = 7.318309633355391\n",
      "Batch 156 / 157\n",
      "training loss = 7.647737979888916\n",
      "validation loss = 7.29689211594431\n",
      "Batch 157 / 157\n",
      "training loss = 6.7059221267700195\n",
      "validation loss = 7.284515380859375\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.374829292297363\n",
      "validation loss = 10.29070698587518\n",
      "Batch 2 / 157\n",
      "training loss = 10.289206504821777\n",
      "validation loss = 10.1699084231728\n",
      "Batch 3 / 157\n",
      "training loss = 10.171314239501953\n",
      "validation loss = 10.055205495733963\n",
      "Batch 4 / 157\n",
      "training loss = 10.067768096923828\n",
      "validation loss = 9.936007449501439\n",
      "Batch 5 / 157\n",
      "training loss = 9.947151184082031\n",
      "validation loss = 9.813777120489823\n",
      "Batch 6 / 157\n",
      "training loss = 9.812830924987793\n",
      "validation loss = 9.691630965784976\n",
      "Batch 7 / 157\n",
      "training loss = 9.684401512145996\n",
      "validation loss = 9.57069236353824\n",
      "Batch 8 / 157\n",
      "training loss = 9.546394348144531\n",
      "validation loss = 9.452098746048776\n",
      "Batch 9 / 157\n",
      "training loss = 9.447210311889648\n",
      "validation loss = 9.335518887168483\n",
      "Batch 10 / 157\n",
      "training loss = 9.250411987304688\n",
      "validation loss = 9.221211533797415\n",
      "Batch 11 / 157\n",
      "training loss = 9.212313652038574\n",
      "validation loss = 9.10698393771523\n",
      "Batch 12 / 157\n",
      "training loss = 9.079212188720703\n",
      "validation loss = 9.001616477966309\n",
      "Batch 13 / 157\n",
      "training loss = 8.921172142028809\n",
      "validation loss = 8.896245052939967\n",
      "Batch 14 / 157\n",
      "training loss = 8.884522438049316\n",
      "validation loss = 8.79694256029631\n",
      "Batch 15 / 157\n",
      "training loss = 8.84850025177002\n",
      "validation loss = 8.704402722810444\n",
      "Batch 16 / 157\n",
      "training loss = 8.625345230102539\n",
      "validation loss = 8.614174993414627\n",
      "Batch 17 / 157\n",
      "training loss = 8.585897445678711\n",
      "validation loss = 8.530440179925217\n",
      "Batch 18 / 157\n",
      "training loss = 8.459477424621582\n",
      "validation loss = 8.450916892603823\n",
      "Batch 19 / 157\n",
      "training loss = 8.470699310302734\n",
      "validation loss = 8.376448279932925\n",
      "Batch 20 / 157\n",
      "training loss = 8.463356971740723\n",
      "validation loss = 8.306039759987279\n",
      "Batch 21 / 157\n",
      "training loss = 8.35913372039795\n",
      "validation loss = 8.24153262690494\n",
      "Batch 22 / 157\n",
      "training loss = 8.273472785949707\n",
      "validation loss = 8.185498538770174\n",
      "Batch 23 / 157\n",
      "training loss = 8.120697021484375\n",
      "validation loss = 8.127844107778449\n",
      "Batch 24 / 157\n",
      "training loss = 8.156941413879395\n",
      "validation loss = 8.072764823311253\n",
      "Batch 25 / 157\n",
      "training loss = 8.035521507263184\n",
      "validation loss = 8.026476082048918\n",
      "Batch 26 / 157\n",
      "training loss = 7.990884780883789\n",
      "validation loss = 7.983049744053891\n",
      "Batch 27 / 157\n",
      "training loss = 8.049690246582031\n",
      "validation loss = 7.941576430672093\n",
      "Batch 28 / 157\n",
      "training loss = 8.083251953125\n",
      "validation loss = 7.906374379208214\n",
      "Batch 29 / 157\n",
      "training loss = 7.837445259094238\n",
      "validation loss = 7.873908168391178\n",
      "Batch 30 / 157\n",
      "training loss = 7.900051593780518\n",
      "validation loss = 7.839826885022615\n",
      "Batch 31 / 157\n",
      "training loss = 8.034421920776367\n",
      "validation loss = 7.811513699983296\n",
      "Batch 32 / 157\n",
      "training loss = 7.750514507293701\n",
      "validation loss = 7.796030948036595\n",
      "Batch 33 / 157\n",
      "training loss = 8.011910438537598\n",
      "validation loss = 7.766103443346526\n",
      "Batch 34 / 157\n",
      "training loss = 7.86814546585083\n",
      "validation loss = 7.744566641355815\n",
      "Batch 35 / 157\n",
      "training loss = 7.989802360534668\n",
      "validation loss = 7.728661888524106\n",
      "Batch 36 / 157\n",
      "training loss = 7.565208911895752\n",
      "validation loss = 7.71215127643786\n",
      "Batch 37 / 157\n",
      "training loss = 7.803565502166748\n",
      "validation loss = 7.700935690026534\n",
      "Batch 38 / 157\n",
      "training loss = 7.792431831359863\n",
      "validation loss = 7.686895445773476\n",
      "Batch 39 / 157\n",
      "training loss = 7.485241413116455\n",
      "validation loss = 7.67899919811048\n",
      "Batch 40 / 157\n",
      "training loss = 7.718084812164307\n",
      "validation loss = 7.670071777544524\n",
      "Batch 41 / 157\n",
      "training loss = 7.8240580558776855\n",
      "validation loss = 7.663265529431794\n",
      "Batch 42 / 157\n",
      "training loss = 7.8245110511779785\n",
      "validation loss = 7.655670693046169\n",
      "Batch 43 / 157\n",
      "training loss = 7.640116214752197\n",
      "validation loss = 7.649500646089253\n",
      "Batch 44 / 157\n",
      "training loss = 7.402495384216309\n",
      "validation loss = 7.642393639213161\n",
      "Batch 45 / 157\n",
      "training loss = 7.689398765563965\n",
      "validation loss = 7.640373004110236\n",
      "Batch 46 / 157\n",
      "training loss = 7.62345027923584\n",
      "validation loss = 7.632838148819773\n",
      "Batch 47 / 157\n",
      "training loss = 7.666080474853516\n",
      "validation loss = 7.6322704867312785\n",
      "Batch 48 / 157\n",
      "training loss = 7.716257095336914\n",
      "validation loss = 7.633365329943206\n",
      "Batch 49 / 157\n",
      "training loss = 7.527171611785889\n",
      "validation loss = 7.62797380748548\n",
      "Batch 50 / 157\n",
      "training loss = 7.523892402648926\n",
      "validation loss = 7.626812432941637\n",
      "Batch 51 / 157\n",
      "training loss = 7.505886077880859\n",
      "validation loss = 7.6264540521722095\n",
      "Batch 52 / 157\n",
      "training loss = 7.611794948577881\n",
      "validation loss = 7.629032210299843\n",
      "Batch 53 / 157\n",
      "training loss = 7.714746952056885\n",
      "validation loss = 7.624206944515831\n",
      "Batch 54 / 157\n",
      "training loss = 7.617627143859863\n",
      "validation loss = 7.619753360748291\n",
      "Batch 55 / 157\n",
      "training loss = 7.635368347167969\n",
      "validation loss = 7.615455225894325\n",
      "Batch 56 / 157\n",
      "training loss = 7.536595344543457\n",
      "validation loss = 7.614192887356407\n",
      "Batch 57 / 157\n",
      "training loss = 7.608957290649414\n",
      "validation loss = 7.6157002449035645\n",
      "Batch 58 / 157\n",
      "training loss = 7.6051177978515625\n",
      "validation loss = 7.612111166903847\n",
      "Batch 59 / 157\n",
      "training loss = 7.526180267333984\n",
      "validation loss = 7.612364718788548\n",
      "Batch 60 / 157\n",
      "training loss = 7.692981719970703\n",
      "validation loss = 7.608652892865632\n",
      "Batch 61 / 157\n",
      "training loss = 7.562155246734619\n",
      "validation loss = 7.604334203820479\n",
      "Batch 62 / 157\n",
      "training loss = 7.779501914978027\n",
      "validation loss = 7.600419094688014\n",
      "Batch 63 / 157\n",
      "training loss = 7.561171531677246\n",
      "validation loss = 7.5984845412404916\n",
      "Batch 64 / 157\n",
      "training loss = 7.674210071563721\n",
      "validation loss = 7.6005504005833675\n",
      "Batch 65 / 157\n",
      "training loss = 7.815115451812744\n",
      "validation loss = 7.601567218178197\n",
      "Batch 66 / 157\n",
      "training loss = 7.419732570648193\n",
      "validation loss = 7.587421793686716\n",
      "Batch 67 / 157\n",
      "training loss = 7.659188747406006\n",
      "validation loss = 7.585946835969624\n",
      "Batch 68 / 157\n",
      "training loss = 7.689458847045898\n",
      "validation loss = 7.582512654756245\n",
      "Batch 69 / 157\n",
      "training loss = 7.836974620819092\n",
      "validation loss = 7.577404197893645\n",
      "Batch 70 / 157\n",
      "training loss = 7.808352947235107\n",
      "validation loss = 7.577911703210128\n",
      "Batch 71 / 157\n",
      "training loss = 7.5292134284973145\n",
      "validation loss = 7.575100597582366\n",
      "Batch 72 / 157\n",
      "training loss = 7.299163341522217\n",
      "validation loss = 7.567118017297042\n",
      "Batch 73 / 157\n",
      "training loss = 7.907888889312744\n",
      "validation loss = 7.566826218052914\n",
      "Batch 74 / 157\n",
      "training loss = 7.720392227172852\n",
      "validation loss = 7.569782382563541\n",
      "Batch 75 / 157\n",
      "training loss = 7.561324596405029\n",
      "validation loss = 7.565446477187307\n",
      "Batch 76 / 157\n",
      "training loss = 7.620869159698486\n",
      "validation loss = 7.555245349281712\n",
      "Batch 77 / 157\n",
      "training loss = 7.411389350891113\n",
      "validation loss = 7.551078972063567\n",
      "Batch 78 / 157\n",
      "training loss = 7.580808639526367\n",
      "validation loss = 7.550893306732178\n",
      "Batch 79 / 157\n",
      "training loss = 8.103155136108398\n",
      "validation loss = 7.550288928182502\n",
      "Batch 80 / 157\n",
      "training loss = 7.570867538452148\n",
      "validation loss = 7.545813987129613\n",
      "Batch 81 / 157\n",
      "training loss = 7.765204906463623\n",
      "validation loss = 7.540282625901072\n",
      "Batch 82 / 157\n",
      "training loss = 7.790985584259033\n",
      "validation loss = 7.54440598738821\n",
      "Batch 83 / 157\n",
      "training loss = 7.583582401275635\n",
      "validation loss = 7.538264751434326\n",
      "Batch 84 / 157\n",
      "training loss = 7.748289585113525\n",
      "validation loss = 7.535932716570403\n",
      "Batch 85 / 157\n",
      "training loss = 7.3827433586120605\n",
      "validation loss = 7.528134722458689\n",
      "Batch 86 / 157\n",
      "training loss = 7.679902076721191\n",
      "validation loss = 7.524018940172698\n",
      "Batch 87 / 157\n",
      "training loss = 7.453497886657715\n",
      "validation loss = 7.518190283524363\n",
      "Batch 88 / 157\n",
      "training loss = 7.71504020690918\n",
      "validation loss = 7.52199461585597\n",
      "Batch 89 / 157\n",
      "training loss = 7.605910778045654\n",
      "validation loss = 7.523142137025532\n",
      "Batch 90 / 157\n",
      "training loss = 7.507653713226318\n",
      "validation loss = 7.513783479991712\n",
      "Batch 91 / 157\n",
      "training loss = 7.345098972320557\n",
      "validation loss = 7.506002451244154\n",
      "Batch 92 / 157\n",
      "training loss = 7.5322771072387695\n",
      "validation loss = 7.509084224700928\n",
      "Batch 93 / 157\n",
      "training loss = 7.3560261726379395\n",
      "validation loss = 7.5066956218920255\n",
      "Batch 94 / 157\n",
      "training loss = 7.680631160736084\n",
      "validation loss = 7.4956720502753\n",
      "Batch 95 / 157\n",
      "training loss = 7.573897361755371\n",
      "validation loss = 7.499193919332404\n",
      "Batch 96 / 157\n",
      "training loss = 7.712067604064941\n",
      "validation loss = 7.499087183099044\n",
      "Batch 97 / 157\n",
      "training loss = 7.638115406036377\n",
      "validation loss = 7.493703641389546\n",
      "Batch 98 / 157\n",
      "training loss = 7.531791687011719\n",
      "validation loss = 7.48443068956074\n",
      "Batch 99 / 157\n",
      "training loss = 7.634218692779541\n",
      "validation loss = 7.476039811184532\n",
      "Batch 100 / 157\n",
      "training loss = 7.5754194259643555\n",
      "validation loss = 7.475066812414872\n",
      "Batch 101 / 157\n",
      "training loss = 7.555375576019287\n",
      "validation loss = 7.473251066709819\n",
      "Batch 102 / 157\n",
      "training loss = 7.687720775604248\n",
      "validation loss = 7.4735165646201684\n",
      "Batch 103 / 157\n",
      "training loss = 7.875743389129639\n",
      "validation loss = 7.463071396476344\n",
      "Batch 104 / 157\n",
      "training loss = 7.3742804527282715\n",
      "validation loss = 7.457824280387477\n",
      "Batch 105 / 157\n",
      "training loss = 7.7152862548828125\n",
      "validation loss = 7.461208192925704\n",
      "Batch 106 / 157\n",
      "training loss = 7.450695991516113\n",
      "validation loss = 7.464683683294999\n",
      "Batch 107 / 157\n",
      "training loss = 7.384013652801514\n",
      "validation loss = 7.451143038900275\n",
      "Batch 108 / 157\n",
      "training loss = 7.312313079833984\n",
      "validation loss = 7.448910763389186\n",
      "Batch 109 / 157\n",
      "training loss = 7.5458292961120605\n",
      "validation loss = 7.448954356344123\n",
      "Batch 110 / 157\n",
      "training loss = 7.655588626861572\n",
      "validation loss = 7.444385277597528\n",
      "Batch 111 / 157\n",
      "training loss = 7.306808948516846\n",
      "validation loss = 7.440878140298944\n",
      "Batch 112 / 157\n",
      "training loss = 7.366854190826416\n",
      "validation loss = 7.440183689719753\n",
      "Batch 113 / 157\n",
      "training loss = 7.529321193695068\n",
      "validation loss = 7.4375434925681665\n",
      "Batch 114 / 157\n",
      "training loss = 7.455569267272949\n",
      "validation loss = 7.432445475929661\n",
      "Batch 115 / 157\n",
      "training loss = 7.625142574310303\n",
      "validation loss = 7.4313845132526595\n",
      "Batch 116 / 157\n",
      "training loss = 7.61015510559082\n",
      "validation loss = 7.432336932734439\n",
      "Batch 117 / 157\n",
      "training loss = 7.429789066314697\n",
      "validation loss = 7.426447366413317\n",
      "Batch 118 / 157\n",
      "training loss = 7.528997421264648\n",
      "validation loss = 7.425543709805138\n",
      "Batch 119 / 157\n",
      "training loss = 7.64552640914917\n",
      "validation loss = 7.421502464695981\n",
      "Batch 120 / 157\n",
      "training loss = 7.38637113571167\n",
      "validation loss = 7.4180961408113175\n",
      "Batch 121 / 157\n",
      "training loss = 7.451186180114746\n",
      "validation loss = 7.4149548129031535\n",
      "Batch 122 / 157\n",
      "training loss = 7.3299689292907715\n",
      "validation loss = 7.412257345099198\n",
      "Batch 123 / 157\n",
      "training loss = 7.276584625244141\n",
      "validation loss = 7.409445135216964\n",
      "Batch 124 / 157\n",
      "training loss = 7.136672496795654\n",
      "validation loss = 7.412226275393837\n",
      "Batch 125 / 157\n",
      "training loss = 7.527108192443848\n",
      "validation loss = 7.407712534854286\n",
      "Batch 126 / 157\n",
      "training loss = 7.300070762634277\n",
      "validation loss = 7.407149189396908\n",
      "Batch 127 / 157\n",
      "training loss = 7.122491359710693\n",
      "validation loss = 7.402589948553788\n",
      "Batch 128 / 157\n",
      "training loss = 7.287453651428223\n",
      "validation loss = 7.400176976856432\n",
      "Batch 129 / 157\n",
      "training loss = 7.578289985656738\n",
      "validation loss = 7.397732408423173\n",
      "Batch 130 / 157\n",
      "training loss = 7.279577732086182\n",
      "validation loss = 7.390548655861302\n",
      "Batch 131 / 157\n",
      "training loss = 7.3361711502075195\n",
      "validation loss = 7.392678612156918\n",
      "Batch 132 / 157\n",
      "training loss = 7.280682563781738\n",
      "validation loss = 7.392269009038022\n",
      "Batch 133 / 157\n",
      "training loss = 7.288260459899902\n",
      "validation loss = 7.394506981498317\n",
      "Batch 134 / 157\n",
      "training loss = 7.590706825256348\n",
      "validation loss = 7.392880740918611\n",
      "Batch 135 / 157\n",
      "training loss = 7.506598949432373\n",
      "validation loss = 7.388148006639983\n",
      "Batch 136 / 157\n",
      "training loss = 7.43646764755249\n",
      "validation loss = 7.384499047931872\n",
      "Batch 137 / 157\n",
      "training loss = 7.226650238037109\n",
      "validation loss = 7.3827006189446704\n",
      "Batch 138 / 157\n",
      "training loss = 7.381799221038818\n",
      "validation loss = 7.380211855235853\n",
      "Batch 139 / 157\n",
      "training loss = 7.303243637084961\n",
      "validation loss = 7.381101156535902\n",
      "Batch 140 / 157\n",
      "training loss = 7.3693366050720215\n",
      "validation loss = 7.3758078123393815\n",
      "Batch 141 / 157\n",
      "training loss = 7.79022216796875\n",
      "validation loss = 7.371484806663112\n",
      "Batch 142 / 157\n",
      "training loss = 7.409614086151123\n",
      "validation loss = 7.3773731683429915\n",
      "Batch 143 / 157\n",
      "training loss = 7.300769329071045\n",
      "validation loss = 7.3746547197040755\n",
      "Batch 144 / 157\n",
      "training loss = 7.321068286895752\n",
      "validation loss = 7.371773418627288\n",
      "Batch 145 / 157\n",
      "training loss = 7.542600631713867\n",
      "validation loss = 7.369292459989849\n",
      "Batch 146 / 157\n",
      "training loss = 7.386558532714844\n",
      "validation loss = 7.369276950233861\n",
      "Batch 147 / 157\n",
      "training loss = 7.410632133483887\n",
      "validation loss = 7.362256878300717\n",
      "Batch 148 / 157\n",
      "training loss = 7.533343315124512\n",
      "validation loss = 7.363120129233913\n",
      "Batch 149 / 157\n",
      "training loss = 7.521737098693848\n",
      "validation loss = 7.365581487354479\n",
      "Batch 150 / 157\n",
      "training loss = 7.457180976867676\n",
      "validation loss = 7.3583424216822575\n",
      "Batch 151 / 157\n",
      "training loss = 7.359299659729004\n",
      "validation loss = 7.3611040115356445\n",
      "Batch 152 / 157\n",
      "training loss = 7.28565788269043\n",
      "validation loss = 7.351343807421233\n",
      "Batch 153 / 157\n",
      "training loss = 7.561160564422607\n",
      "validation loss = 7.351113846427516\n",
      "Batch 154 / 157\n",
      "training loss = 7.242668151855469\n",
      "validation loss = 7.349942809657047\n",
      "Batch 155 / 157\n",
      "training loss = 7.359032154083252\n",
      "validation loss = 7.345227768546657\n",
      "Batch 156 / 157\n",
      "training loss = 7.235381603240967\n",
      "validation loss = 7.347978265661943\n",
      "Batch 157 / 157\n",
      "training loss = 7.291358470916748\n",
      "validation loss = 7.339658034475226\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.372288703918457\n",
      "validation loss = 10.287995388633327\n",
      "Batch 2 / 157\n",
      "training loss = 10.276494026184082\n",
      "validation loss = 10.154533787777549\n",
      "Batch 3 / 157\n",
      "training loss = 10.156509399414062\n",
      "validation loss = 10.032015398928994\n",
      "Batch 4 / 157\n",
      "training loss = 10.03610897064209\n",
      "validation loss = 9.909403901351126\n",
      "Batch 5 / 157\n",
      "training loss = 9.913982391357422\n",
      "validation loss = 9.786072931791606\n",
      "Batch 6 / 157\n",
      "training loss = 9.770731925964355\n",
      "validation loss = 9.663193050183748\n",
      "Batch 7 / 157\n",
      "training loss = 9.61962890625\n",
      "validation loss = 9.542928796065482\n",
      "Batch 8 / 157\n",
      "training loss = 9.530364990234375\n",
      "validation loss = 9.425825420178866\n",
      "Batch 9 / 157\n",
      "training loss = 9.410638809204102\n",
      "validation loss = 9.312031545137105\n",
      "Batch 10 / 157\n",
      "training loss = 9.317581176757812\n",
      "validation loss = 9.198625263414884\n",
      "Batch 11 / 157\n",
      "training loss = 9.17939567565918\n",
      "validation loss = 9.091727708515368\n",
      "Batch 12 / 157\n",
      "training loss = 9.15786075592041\n",
      "validation loss = 8.987459885446649\n",
      "Batch 13 / 157\n",
      "training loss = 8.972800254821777\n",
      "validation loss = 8.890938608269943\n",
      "Batch 14 / 157\n",
      "training loss = 8.875712394714355\n",
      "validation loss = 8.796265652305202\n",
      "Batch 15 / 157\n",
      "training loss = 8.776849746704102\n",
      "validation loss = 8.708592565436112\n",
      "Batch 16 / 157\n",
      "training loss = 8.687457084655762\n",
      "validation loss = 8.620004955090975\n",
      "Batch 17 / 157\n",
      "training loss = 8.588601112365723\n",
      "validation loss = 8.539576480263158\n",
      "Batch 18 / 157\n",
      "training loss = 8.374242782592773\n",
      "validation loss = 8.461221092625669\n",
      "Batch 19 / 157\n",
      "training loss = 8.509953498840332\n",
      "validation loss = 8.389598344501696\n",
      "Batch 20 / 157\n",
      "training loss = 8.293888092041016\n",
      "validation loss = 8.321551674290708\n",
      "Batch 21 / 157\n",
      "training loss = 8.764427185058594\n",
      "validation loss = 8.256712110419022\n",
      "Batch 22 / 157\n",
      "training loss = 8.318885803222656\n",
      "validation loss = 8.200685300325093\n",
      "Batch 23 / 157\n",
      "training loss = 8.202162742614746\n",
      "validation loss = 8.138635685569362\n",
      "Batch 24 / 157\n",
      "training loss = 8.54572582244873\n",
      "validation loss = 8.090425491333008\n",
      "Batch 25 / 157\n",
      "training loss = 8.005568504333496\n",
      "validation loss = 8.042736128756875\n",
      "Batch 26 / 157\n",
      "training loss = 8.09294319152832\n",
      "validation loss = 7.993239954898232\n",
      "Batch 27 / 157\n",
      "training loss = 8.032496452331543\n",
      "validation loss = 7.949551005112498\n",
      "Batch 28 / 157\n",
      "training loss = 7.879411697387695\n",
      "validation loss = 7.913201332092285\n",
      "Batch 29 / 157\n",
      "training loss = 7.8588972091674805\n",
      "validation loss = 7.869448762190969\n",
      "Batch 30 / 157\n",
      "training loss = 8.409017562866211\n",
      "validation loss = 7.836798090683787\n",
      "Batch 31 / 157\n",
      "training loss = 7.79676628112793\n",
      "validation loss = 7.81569222400063\n",
      "Batch 32 / 157\n",
      "training loss = 7.791010856628418\n",
      "validation loss = 7.789011327843917\n",
      "Batch 33 / 157\n",
      "training loss = 7.93925142288208\n",
      "validation loss = 7.76020622253418\n",
      "Batch 34 / 157\n",
      "training loss = 7.437081813812256\n",
      "validation loss = 7.732120011982165\n",
      "Batch 35 / 157\n",
      "training loss = 7.606839656829834\n",
      "validation loss = 7.714818904274388\n",
      "Batch 36 / 157\n",
      "training loss = 7.598793983459473\n",
      "validation loss = 7.693501346989682\n",
      "Batch 37 / 157\n",
      "training loss = 7.587907791137695\n",
      "validation loss = 7.675245510904412\n",
      "Batch 38 / 157\n",
      "training loss = 7.544020652770996\n",
      "validation loss = 7.662678392309892\n",
      "Batch 39 / 157\n",
      "training loss = 7.586313724517822\n",
      "validation loss = 7.651621442092092\n",
      "Batch 40 / 157\n",
      "training loss = 7.570178508758545\n",
      "validation loss = 7.644818657322934\n",
      "Batch 41 / 157\n",
      "training loss = 7.9416117668151855\n",
      "validation loss = 7.639250905890214\n",
      "Batch 42 / 157\n",
      "training loss = 7.681751728057861\n",
      "validation loss = 7.628243220479865\n",
      "Batch 43 / 157\n",
      "training loss = 7.424675941467285\n",
      "validation loss = 7.621051035429302\n",
      "Batch 44 / 157\n",
      "training loss = 7.49725866317749\n",
      "validation loss = 7.613755502198872\n",
      "Batch 45 / 157\n",
      "training loss = 7.803470134735107\n",
      "validation loss = 7.608421526457134\n",
      "Batch 46 / 157\n",
      "training loss = 7.506542205810547\n",
      "validation loss = 7.6074538230896\n",
      "Batch 47 / 157\n",
      "training loss = 7.369761943817139\n",
      "validation loss = 7.599928077898528\n",
      "Batch 48 / 157\n",
      "training loss = 7.590904712677002\n",
      "validation loss = 7.599951267242432\n",
      "Batch 49 / 157\n",
      "training loss = 7.656050205230713\n",
      "validation loss = 7.596841586263556\n",
      "Batch 50 / 157\n",
      "training loss = 7.3771209716796875\n",
      "validation loss = 7.600765830592105\n",
      "Batch 51 / 157\n",
      "training loss = 7.5164666175842285\n",
      "validation loss = 7.597866610476845\n",
      "Batch 52 / 157\n",
      "training loss = 7.574513912200928\n",
      "validation loss = 7.594270354823062\n",
      "Batch 53 / 157\n",
      "training loss = 7.658039569854736\n",
      "validation loss = 7.591840819308632\n",
      "Batch 54 / 157\n",
      "training loss = 7.6313629150390625\n",
      "validation loss = 7.5931653474506575\n",
      "Batch 55 / 157\n",
      "training loss = 7.342394828796387\n",
      "validation loss = 7.5869223695052295\n",
      "Batch 56 / 157\n",
      "training loss = 7.657522201538086\n",
      "validation loss = 7.59328556060791\n",
      "Batch 57 / 157\n",
      "training loss = 7.276692867279053\n",
      "validation loss = 7.585770682284706\n",
      "Batch 58 / 157\n",
      "training loss = 7.367207050323486\n",
      "validation loss = 7.595352775172183\n",
      "Batch 59 / 157\n",
      "training loss = 7.6520161628723145\n",
      "validation loss = 7.5918020951120475\n",
      "Batch 60 / 157\n",
      "training loss = 7.486445426940918\n",
      "validation loss = 7.594743854121158\n",
      "Batch 61 / 157\n",
      "training loss = 7.707988739013672\n",
      "validation loss = 7.584627327166106\n",
      "Batch 62 / 157\n",
      "training loss = 7.683415412902832\n",
      "validation loss = 7.5841243894476635\n",
      "Batch 63 / 157\n",
      "training loss = 7.620988845825195\n",
      "validation loss = 7.589316895133571\n",
      "Batch 64 / 157\n",
      "training loss = 7.612813472747803\n",
      "validation loss = 7.592930291828356\n",
      "Batch 65 / 157\n",
      "training loss = 7.581897735595703\n",
      "validation loss = 7.575732933847528\n",
      "Batch 66 / 157\n",
      "training loss = 7.561115741729736\n",
      "validation loss = 7.577327351821096\n",
      "Batch 67 / 157\n",
      "training loss = 7.416508674621582\n",
      "validation loss = 7.572194626456813\n",
      "Batch 68 / 157\n",
      "training loss = 7.667057991027832\n",
      "validation loss = 7.576632901241905\n",
      "Batch 69 / 157\n",
      "training loss = 7.7872443199157715\n",
      "validation loss = 7.571441550003855\n",
      "Batch 70 / 157\n",
      "training loss = 7.588214874267578\n",
      "validation loss = 7.565211195694773\n",
      "Batch 71 / 157\n",
      "training loss = 7.41546106338501\n",
      "validation loss = 7.563217614826403\n",
      "Batch 72 / 157\n",
      "training loss = 7.555195331573486\n",
      "validation loss = 7.564842500184712\n",
      "Batch 73 / 157\n",
      "training loss = 7.441794395446777\n",
      "validation loss = 7.560714596196225\n",
      "Batch 74 / 157\n",
      "training loss = 7.435201644897461\n",
      "validation loss = 7.557298986535323\n",
      "Batch 75 / 157\n",
      "training loss = 7.474315166473389\n",
      "validation loss = 7.5510882578398055\n",
      "Batch 76 / 157\n",
      "training loss = 7.585488319396973\n",
      "validation loss = 7.550940036773682\n",
      "Batch 77 / 157\n",
      "training loss = 7.420017719268799\n",
      "validation loss = 7.547207556272808\n",
      "Batch 78 / 157\n",
      "training loss = 7.559699535369873\n",
      "validation loss = 7.544628846017938\n",
      "Batch 79 / 157\n",
      "training loss = 7.5952653884887695\n",
      "validation loss = 7.541964480751439\n",
      "Batch 80 / 157\n",
      "training loss = 7.5303568840026855\n",
      "validation loss = 7.544619986885472\n",
      "Batch 81 / 157\n",
      "training loss = 7.5859456062316895\n",
      "validation loss = 7.537564679196007\n",
      "Batch 82 / 157\n",
      "training loss = 7.568449974060059\n",
      "validation loss = 7.5367969713713\n",
      "Batch 83 / 157\n",
      "training loss = 7.52018928527832\n",
      "validation loss = 7.531744254262824\n",
      "Batch 84 / 157\n",
      "training loss = 7.525933742523193\n",
      "validation loss = 7.532033343064158\n",
      "Batch 85 / 157\n",
      "training loss = 7.486782550811768\n",
      "validation loss = 7.523567249900417\n",
      "Batch 86 / 157\n",
      "training loss = 7.485784530639648\n",
      "validation loss = 7.524931405719958\n",
      "Batch 87 / 157\n",
      "training loss = 7.674397945404053\n",
      "validation loss = 7.518206194827431\n",
      "Batch 88 / 157\n",
      "training loss = 7.598820209503174\n",
      "validation loss = 7.519528138010125\n",
      "Batch 89 / 157\n",
      "training loss = 7.54411506652832\n",
      "validation loss = 7.5107405060216\n",
      "Batch 90 / 157\n",
      "training loss = 7.513439655303955\n",
      "validation loss = 7.506453338422273\n",
      "Batch 91 / 157\n",
      "training loss = 7.386914253234863\n",
      "validation loss = 7.50310744737324\n",
      "Batch 92 / 157\n",
      "training loss = 7.472842216491699\n",
      "validation loss = 7.49646006132427\n",
      "Batch 93 / 157\n",
      "training loss = 7.599041938781738\n",
      "validation loss = 7.49201784635845\n",
      "Batch 94 / 157\n",
      "training loss = 7.427048683166504\n",
      "validation loss = 7.487363162793611\n",
      "Batch 95 / 157\n",
      "training loss = 7.314901351928711\n",
      "validation loss = 7.488267672689338\n",
      "Batch 96 / 157\n",
      "training loss = 7.3210129737854\n",
      "validation loss = 7.481691410667018\n",
      "Batch 97 / 157\n",
      "training loss = 7.5454792976379395\n",
      "validation loss = 7.479266367460552\n",
      "Batch 98 / 157\n",
      "training loss = 7.435479164123535\n",
      "validation loss = 7.46997416646857\n",
      "Batch 99 / 157\n",
      "training loss = 7.532163619995117\n",
      "validation loss = 7.463685688219573\n",
      "Batch 100 / 157\n",
      "training loss = 7.533858776092529\n",
      "validation loss = 7.467143460323936\n",
      "Batch 101 / 157\n",
      "training loss = 7.568056106567383\n",
      "validation loss = 7.471560302533601\n",
      "Batch 102 / 157\n",
      "training loss = 7.377650737762451\n",
      "validation loss = 7.46121348832783\n",
      "Batch 103 / 157\n",
      "training loss = 7.331241130828857\n",
      "validation loss = 7.456936710759213\n",
      "Batch 104 / 157\n",
      "training loss = 7.699699878692627\n",
      "validation loss = 7.454943506341231\n",
      "Batch 105 / 157\n",
      "training loss = 7.72852087020874\n",
      "validation loss = 7.448660850524902\n",
      "Batch 106 / 157\n",
      "training loss = 7.619684219360352\n",
      "validation loss = 7.453382868515818\n",
      "Batch 107 / 157\n",
      "training loss = 7.671706199645996\n",
      "validation loss = 7.454541507520173\n",
      "Batch 108 / 157\n",
      "training loss = 7.207959175109863\n",
      "validation loss = 7.456592007687218\n",
      "Batch 109 / 157\n",
      "training loss = 7.41269063949585\n",
      "validation loss = 7.442830939041941\n",
      "Batch 110 / 157\n",
      "training loss = 7.451560020446777\n",
      "validation loss = 7.432994591562371\n",
      "Batch 111 / 157\n",
      "training loss = 7.342017650604248\n",
      "validation loss = 7.437185538442511\n",
      "Batch 112 / 157\n",
      "training loss = 7.533871173858643\n",
      "validation loss = 7.444140986392372\n",
      "Batch 113 / 157\n",
      "training loss = 7.495044231414795\n",
      "validation loss = 7.436333555924265\n",
      "Batch 114 / 157\n",
      "training loss = 7.474944114685059\n",
      "validation loss = 7.419874668121338\n",
      "Batch 115 / 157\n",
      "training loss = 7.493810176849365\n",
      "validation loss = 7.414090708682411\n",
      "Batch 116 / 157\n",
      "training loss = 7.446291923522949\n",
      "validation loss = 7.4244459804735685\n",
      "Batch 117 / 157\n",
      "training loss = 7.430056095123291\n",
      "validation loss = 7.416128033085873\n",
      "Batch 118 / 157\n",
      "training loss = 7.378558158874512\n",
      "validation loss = 7.415677974098607\n",
      "Batch 119 / 157\n",
      "training loss = 7.423971652984619\n",
      "validation loss = 7.415274268702457\n",
      "Batch 120 / 157\n",
      "training loss = 7.358898639678955\n",
      "validation loss = 7.410787055366917\n",
      "Batch 121 / 157\n",
      "training loss = 7.334934234619141\n",
      "validation loss = 7.406738707893773\n",
      "Batch 122 / 157\n",
      "training loss = 7.6023335456848145\n",
      "validation loss = 7.3995082503870915\n",
      "Batch 123 / 157\n",
      "training loss = 7.415411949157715\n",
      "validation loss = 7.403875401145534\n",
      "Batch 124 / 157\n",
      "training loss = 7.391317844390869\n",
      "validation loss = 7.397909591072484\n",
      "Batch 125 / 157\n",
      "training loss = 7.5004448890686035\n",
      "validation loss = 7.3979465835972835\n",
      "Batch 126 / 157\n",
      "training loss = 7.3506011962890625\n",
      "validation loss = 7.396087571194298\n",
      "Batch 127 / 157\n",
      "training loss = 7.452128887176514\n",
      "validation loss = 7.391055433373702\n",
      "Batch 128 / 157\n",
      "training loss = 7.441822528839111\n",
      "validation loss = 7.391200994190417\n",
      "Batch 129 / 157\n",
      "training loss = 7.473257541656494\n",
      "validation loss = 7.387290603236148\n",
      "Batch 130 / 157\n",
      "training loss = 7.565925121307373\n",
      "validation loss = 7.382631703426964\n",
      "Batch 131 / 157\n",
      "training loss = 7.456068515777588\n",
      "validation loss = 7.3791038111636515\n",
      "Batch 132 / 157\n",
      "training loss = 7.501574993133545\n",
      "validation loss = 7.382256583163612\n",
      "Batch 133 / 157\n",
      "training loss = 7.487870216369629\n",
      "validation loss = 7.373091923563104\n",
      "Batch 134 / 157\n",
      "training loss = 7.293496608734131\n",
      "validation loss = 7.369110483872263\n",
      "Batch 135 / 157\n",
      "training loss = 7.472021102905273\n",
      "validation loss = 7.36959329404329\n",
      "Batch 136 / 157\n",
      "training loss = 7.182821750640869\n",
      "validation loss = 7.365042385302092\n",
      "Batch 137 / 157\n",
      "training loss = 7.352334022521973\n",
      "validation loss = 7.362181588223106\n",
      "Batch 138 / 157\n",
      "training loss = 7.357495307922363\n",
      "validation loss = 7.359921605963456\n",
      "Batch 139 / 157\n",
      "training loss = 7.104105472564697\n",
      "validation loss = 7.359049270027562\n",
      "Batch 140 / 157\n",
      "training loss = 7.369985103607178\n",
      "validation loss = 7.355752769269441\n",
      "Batch 141 / 157\n",
      "training loss = 7.375619888305664\n",
      "validation loss = 7.354854332773309\n",
      "Batch 142 / 157\n",
      "training loss = 7.4724836349487305\n",
      "validation loss = 7.346572348946019\n",
      "Batch 143 / 157\n",
      "training loss = 7.355154037475586\n",
      "validation loss = 7.35069089186819\n",
      "Batch 144 / 157\n",
      "training loss = 7.3444504737854\n",
      "validation loss = 7.345232135371158\n",
      "Batch 145 / 157\n",
      "training loss = 7.526489734649658\n",
      "validation loss = 7.3395685396696395\n",
      "Batch 146 / 157\n",
      "training loss = 6.944126605987549\n",
      "validation loss = 7.332298002744976\n",
      "Batch 147 / 157\n",
      "training loss = 7.455905437469482\n",
      "validation loss = 7.334647981744063\n",
      "Batch 148 / 157\n",
      "training loss = 7.3565497398376465\n",
      "validation loss = 7.32895060589439\n",
      "Batch 149 / 157\n",
      "training loss = 7.262356281280518\n",
      "validation loss = 7.326386451721191\n",
      "Batch 150 / 157\n",
      "training loss = 7.376600742340088\n",
      "validation loss = 7.320352303354364\n",
      "Batch 151 / 157\n",
      "training loss = 7.333237648010254\n",
      "validation loss = 7.328933766013698\n",
      "Batch 152 / 157\n",
      "training loss = 7.17207145690918\n",
      "validation loss = 7.322272426203678\n",
      "Batch 153 / 157\n",
      "training loss = 7.296395778656006\n",
      "validation loss = 7.317064611535323\n",
      "Batch 154 / 157\n",
      "training loss = 7.272392272949219\n",
      "validation loss = 7.311084144993832\n",
      "Batch 155 / 157\n",
      "training loss = 7.034977912902832\n",
      "validation loss = 7.311990562238191\n",
      "Batch 156 / 157\n",
      "training loss = 7.146869659423828\n",
      "validation loss = 7.311214120764482\n",
      "Batch 157 / 157\n",
      "training loss = 7.296544075012207\n",
      "validation loss = 7.306932599920976\n",
      "Average training loss: 7.76\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.38083553314209\n",
      "validation loss = 10.305549621582031\n",
      "Batch 2 / 157\n",
      "training loss = 10.302502632141113\n",
      "validation loss = 10.212122164274517\n",
      "Batch 3 / 157\n",
      "training loss = 10.224546432495117\n",
      "validation loss = 10.11746823160272\n",
      "Batch 4 / 157\n",
      "training loss = 10.158601760864258\n",
      "validation loss = 10.02100648378071\n",
      "Batch 5 / 157\n",
      "training loss = 10.008801460266113\n",
      "validation loss = 9.917020697342721\n",
      "Batch 6 / 157\n",
      "training loss = 9.933576583862305\n",
      "validation loss = 9.81357152838456\n",
      "Batch 7 / 157\n",
      "training loss = 9.767801284790039\n",
      "validation loss = 9.706950790003726\n",
      "Batch 8 / 157\n",
      "training loss = 9.7329740524292\n",
      "validation loss = 9.601930317125822\n",
      "Batch 9 / 157\n",
      "training loss = 9.638175010681152\n",
      "validation loss = 9.49508059652228\n",
      "Batch 10 / 157\n",
      "training loss = 9.500164031982422\n",
      "validation loss = 9.388053944236354\n",
      "Batch 11 / 157\n",
      "training loss = 9.33200454711914\n",
      "validation loss = 9.279470242952046\n",
      "Batch 12 / 157\n",
      "training loss = 9.270657539367676\n",
      "validation loss = 9.178105755856162\n",
      "Batch 13 / 157\n",
      "training loss = 9.212664604187012\n",
      "validation loss = 9.076263528121146\n",
      "Batch 14 / 157\n",
      "training loss = 9.222978591918945\n",
      "validation loss = 8.975154625742059\n",
      "Batch 15 / 157\n",
      "training loss = 8.906848907470703\n",
      "validation loss = 8.878243446350098\n",
      "Batch 16 / 157\n",
      "training loss = 8.837506294250488\n",
      "validation loss = 8.785725543373509\n",
      "Batch 17 / 157\n",
      "training loss = 8.52391242980957\n",
      "validation loss = 8.696679767809416\n",
      "Batch 18 / 157\n",
      "training loss = 8.740734100341797\n",
      "validation loss = 8.608084528069748\n",
      "Batch 19 / 157\n",
      "training loss = 8.63447380065918\n",
      "validation loss = 8.523098443683825\n",
      "Batch 20 / 157\n",
      "training loss = 8.465790748596191\n",
      "validation loss = 8.444462475023771\n",
      "Batch 21 / 157\n",
      "training loss = 8.273072242736816\n",
      "validation loss = 8.371206584729647\n",
      "Batch 22 / 157\n",
      "training loss = 8.565016746520996\n",
      "validation loss = 8.306378966883608\n",
      "Batch 23 / 157\n",
      "training loss = 8.346193313598633\n",
      "validation loss = 8.239521101901406\n",
      "Batch 24 / 157\n",
      "training loss = 8.130024909973145\n",
      "validation loss = 8.181007284867135\n",
      "Batch 25 / 157\n",
      "training loss = 8.33255672454834\n",
      "validation loss = 8.133561360208612\n",
      "Batch 26 / 157\n",
      "training loss = 7.899359703063965\n",
      "validation loss = 8.08304954829969\n",
      "Batch 27 / 157\n",
      "training loss = 7.943059921264648\n",
      "validation loss = 8.041346851148104\n",
      "Batch 28 / 157\n",
      "training loss = 8.242975234985352\n",
      "validation loss = 8.00450739107634\n",
      "Batch 29 / 157\n",
      "training loss = 7.908102512359619\n",
      "validation loss = 7.968589180394223\n",
      "Batch 30 / 157\n",
      "training loss = 8.124417304992676\n",
      "validation loss = 7.93766084470247\n",
      "Batch 31 / 157\n",
      "training loss = 7.612273693084717\n",
      "validation loss = 7.9143952570463485\n",
      "Batch 32 / 157\n",
      "training loss = 7.894582748413086\n",
      "validation loss = 7.8882543162295695\n",
      "Batch 33 / 157\n",
      "training loss = 7.821686744689941\n",
      "validation loss = 7.873253847423353\n",
      "Batch 34 / 157\n",
      "training loss = 7.5035600662231445\n",
      "validation loss = 7.850887323680677\n",
      "Batch 35 / 157\n",
      "training loss = 7.6765546798706055\n",
      "validation loss = 7.841553788436086\n",
      "Batch 36 / 157\n",
      "training loss = 7.784404277801514\n",
      "validation loss = 7.8352151669953995\n",
      "Batch 37 / 157\n",
      "training loss = 7.936118125915527\n",
      "validation loss = 7.8157259539554\n",
      "Batch 38 / 157\n",
      "training loss = 7.771079063415527\n",
      "validation loss = 7.8142454247725635\n",
      "Batch 39 / 157\n",
      "training loss = 7.921815872192383\n",
      "validation loss = 7.809553974553158\n",
      "Batch 40 / 157\n",
      "training loss = 7.5823869705200195\n",
      "validation loss = 7.794975330955104\n",
      "Batch 41 / 157\n",
      "training loss = 8.09682559967041\n",
      "validation loss = 7.790004855708072\n",
      "Batch 42 / 157\n",
      "training loss = 7.665656089782715\n",
      "validation loss = 7.776513877667878\n",
      "Batch 43 / 157\n",
      "training loss = 7.435169696807861\n",
      "validation loss = 7.765606980574758\n",
      "Batch 44 / 157\n",
      "training loss = 7.651628017425537\n",
      "validation loss = 7.758534858101292\n",
      "Batch 45 / 157\n",
      "training loss = 7.627874374389648\n",
      "validation loss = 7.753144891638505\n",
      "Batch 46 / 157\n",
      "training loss = 7.478498935699463\n",
      "validation loss = 7.749971866607666\n",
      "Batch 47 / 157\n",
      "training loss = 7.57012939453125\n",
      "validation loss = 7.753885043294806\n",
      "Batch 48 / 157\n",
      "training loss = 7.431142330169678\n",
      "validation loss = 7.748217482315867\n",
      "Batch 49 / 157\n",
      "training loss = 7.720524311065674\n",
      "validation loss = 7.740721677478991\n",
      "Batch 50 / 157\n",
      "training loss = 7.617975234985352\n",
      "validation loss = 7.731939767536364\n",
      "Batch 51 / 157\n",
      "training loss = 7.479546546936035\n",
      "validation loss = 7.719268045927349\n",
      "Batch 52 / 157\n",
      "training loss = 7.732423305511475\n",
      "validation loss = 7.714711239463405\n",
      "Batch 53 / 157\n",
      "training loss = 7.735222339630127\n",
      "validation loss = 7.706826837439286\n",
      "Batch 54 / 157\n",
      "training loss = 7.850107669830322\n",
      "validation loss = 7.700564735814145\n",
      "Batch 55 / 157\n",
      "training loss = 7.514309883117676\n",
      "validation loss = 7.698015564366391\n",
      "Batch 56 / 157\n",
      "training loss = 7.711948394775391\n",
      "validation loss = 7.701008344951429\n",
      "Batch 57 / 157\n",
      "training loss = 7.7786865234375\n",
      "validation loss = 7.683833398317036\n",
      "Batch 58 / 157\n",
      "training loss = 7.441166400909424\n",
      "validation loss = 7.68136127371537\n",
      "Batch 59 / 157\n",
      "training loss = 8.00510311126709\n",
      "validation loss = 7.680105661091051\n",
      "Batch 60 / 157\n",
      "training loss = 7.653619289398193\n",
      "validation loss = 7.67108164335552\n",
      "Batch 61 / 157\n",
      "training loss = 7.6922688484191895\n",
      "validation loss = 7.663956566860802\n",
      "Batch 62 / 157\n",
      "training loss = 7.7125372886657715\n",
      "validation loss = 7.659674569180138\n",
      "Batch 63 / 157\n",
      "training loss = 7.669299602508545\n",
      "validation loss = 7.649921542719791\n",
      "Batch 64 / 157\n",
      "training loss = 7.777088165283203\n",
      "validation loss = 7.645328873082211\n",
      "Batch 65 / 157\n",
      "training loss = 7.548017978668213\n",
      "validation loss = 7.643971618853118\n",
      "Batch 66 / 157\n",
      "training loss = 8.017024040222168\n",
      "validation loss = 7.655539035797119\n",
      "Batch 67 / 157\n",
      "training loss = 7.6697998046875\n",
      "validation loss = 7.629250476234837\n",
      "Batch 68 / 157\n",
      "training loss = 7.512965202331543\n",
      "validation loss = 7.640751211266768\n",
      "Batch 69 / 157\n",
      "training loss = 7.645169734954834\n",
      "validation loss = 7.636490545774761\n",
      "Batch 70 / 157\n",
      "training loss = 7.765786647796631\n",
      "validation loss = 7.6441959079943205\n",
      "Batch 71 / 157\n",
      "training loss = 7.630768775939941\n",
      "validation loss = 7.637078887537906\n",
      "Batch 72 / 157\n",
      "training loss = 7.729907035827637\n",
      "validation loss = 7.609772983350251\n",
      "Batch 73 / 157\n",
      "training loss = 7.638713359832764\n",
      "validation loss = 7.617230314957468\n",
      "Batch 74 / 157\n",
      "training loss = 7.584853649139404\n",
      "validation loss = 7.614023710552015\n",
      "Batch 75 / 157\n",
      "training loss = 7.634517669677734\n",
      "validation loss = 7.614382794028835\n",
      "Batch 76 / 157\n",
      "training loss = 7.364555835723877\n",
      "validation loss = 7.610924469797235\n",
      "Batch 77 / 157\n",
      "training loss = 7.709769248962402\n",
      "validation loss = 7.60907180685746\n",
      "Batch 78 / 157\n",
      "training loss = 8.02613639831543\n",
      "validation loss = 7.600798079842015\n",
      "Batch 79 / 157\n",
      "training loss = 7.619367599487305\n",
      "validation loss = 7.597695651807283\n",
      "Batch 80 / 157\n",
      "training loss = 7.703043460845947\n",
      "validation loss = 7.600648478457802\n",
      "Batch 81 / 157\n",
      "training loss = 7.524846076965332\n",
      "validation loss = 7.591717920805278\n",
      "Batch 82 / 157\n",
      "training loss = 7.8102498054504395\n",
      "validation loss = 7.581625662351909\n",
      "Batch 83 / 157\n",
      "training loss = 7.436197757720947\n",
      "validation loss = 7.574818109211169\n",
      "Batch 84 / 157\n",
      "training loss = 7.70551061630249\n",
      "validation loss = 7.5694358976263745\n",
      "Batch 85 / 157\n",
      "training loss = 7.656108856201172\n",
      "validation loss = 7.568866428576018\n",
      "Batch 86 / 157\n",
      "training loss = 7.481372356414795\n",
      "validation loss = 7.561072776192113\n",
      "Batch 87 / 157\n",
      "training loss = 7.152446746826172\n",
      "validation loss = 7.554892088237562\n",
      "Batch 88 / 157\n",
      "training loss = 7.639766216278076\n",
      "validation loss = 7.552766900313528\n",
      "Batch 89 / 157\n",
      "training loss = 7.419857025146484\n",
      "validation loss = 7.5430405767340405\n",
      "Batch 90 / 157\n",
      "training loss = 7.3574442863464355\n",
      "validation loss = 7.536229610443115\n",
      "Batch 91 / 157\n",
      "training loss = 7.468533515930176\n",
      "validation loss = 7.5387352140326245\n",
      "Batch 92 / 157\n",
      "training loss = 7.686848163604736\n",
      "validation loss = 7.539422110507362\n",
      "Batch 93 / 157\n",
      "training loss = 7.354244709014893\n",
      "validation loss = 7.522465680774889\n",
      "Batch 94 / 157\n",
      "training loss = 7.511023998260498\n",
      "validation loss = 7.525791444276509\n",
      "Batch 95 / 157\n",
      "training loss = 7.555612564086914\n",
      "validation loss = 7.521087244937294\n",
      "Batch 96 / 157\n",
      "training loss = 7.689075946807861\n",
      "validation loss = 7.503484173824913\n",
      "Batch 97 / 157\n",
      "training loss = 7.637797832489014\n",
      "validation loss = 7.495991280204372\n",
      "Batch 98 / 157\n",
      "training loss = 7.151909351348877\n",
      "validation loss = 7.495443569986444\n",
      "Batch 99 / 157\n",
      "training loss = 7.5541839599609375\n",
      "validation loss = 7.4894003115202255\n",
      "Batch 100 / 157\n",
      "training loss = 7.737560749053955\n",
      "validation loss = 7.484498676500823\n",
      "Batch 101 / 157\n",
      "training loss = 7.352803707122803\n",
      "validation loss = 7.475634876050447\n",
      "Batch 102 / 157\n",
      "training loss = 7.223386764526367\n",
      "validation loss = 7.475195633737664\n",
      "Batch 103 / 157\n",
      "training loss = 7.545820713043213\n",
      "validation loss = 7.46191940809551\n",
      "Batch 104 / 157\n",
      "training loss = 7.363559246063232\n",
      "validation loss = 7.4610996748271745\n",
      "Batch 105 / 157\n",
      "training loss = 7.593796253204346\n",
      "validation loss = 7.467729593578138\n",
      "Batch 106 / 157\n",
      "training loss = 7.399510860443115\n",
      "validation loss = 7.456899366880718\n",
      "Batch 107 / 157\n",
      "training loss = 7.697497844696045\n",
      "validation loss = 7.454469229045667\n",
      "Batch 108 / 157\n",
      "training loss = 7.274510383605957\n",
      "validation loss = 7.444544415724905\n",
      "Batch 109 / 157\n",
      "training loss = 7.3541460037231445\n",
      "validation loss = 7.437928475831685\n",
      "Batch 110 / 157\n",
      "training loss = 7.941184043884277\n",
      "validation loss = 7.453672258477462\n",
      "Batch 111 / 157\n",
      "training loss = 7.552855491638184\n",
      "validation loss = 7.433962646283601\n",
      "Batch 112 / 157\n",
      "training loss = 7.218873977661133\n",
      "validation loss = 7.427989357396176\n",
      "Batch 113 / 157\n",
      "training loss = 7.497364521026611\n",
      "validation loss = 7.42025633862144\n",
      "Batch 114 / 157\n",
      "training loss = 7.401122570037842\n",
      "validation loss = 7.414259107489335\n",
      "Batch 115 / 157\n",
      "training loss = 7.7917680740356445\n",
      "validation loss = 7.417555683537533\n",
      "Batch 116 / 157\n",
      "training loss = 7.4852399826049805\n",
      "validation loss = 7.439084705553557\n",
      "Batch 117 / 157\n",
      "training loss = 7.566016674041748\n",
      "validation loss = 7.413746482447574\n",
      "Batch 118 / 157\n",
      "training loss = 7.275838851928711\n",
      "validation loss = 7.391875543092427\n",
      "Batch 119 / 157\n",
      "training loss = 7.313957691192627\n",
      "validation loss = 7.408036809218557\n",
      "Batch 120 / 157\n",
      "training loss = 7.4818830490112305\n",
      "validation loss = 7.403243441330759\n",
      "Batch 121 / 157\n",
      "training loss = 7.272648334503174\n",
      "validation loss = 7.390649971209075\n",
      "Batch 122 / 157\n",
      "training loss = 7.232811450958252\n",
      "validation loss = 7.383751417461195\n",
      "Batch 123 / 157\n",
      "training loss = 7.411619663238525\n",
      "validation loss = 7.3721342839692765\n",
      "Batch 124 / 157\n",
      "training loss = 7.243839740753174\n",
      "validation loss = 7.3732419515910905\n",
      "Batch 125 / 157\n",
      "training loss = 7.563837051391602\n",
      "validation loss = 7.37507805071379\n",
      "Batch 126 / 157\n",
      "training loss = 7.246541500091553\n",
      "validation loss = 7.372388613851447\n",
      "Batch 127 / 157\n",
      "training loss = 7.365569591522217\n",
      "validation loss = 7.367120366347463\n",
      "Batch 128 / 157\n",
      "training loss = 7.261114120483398\n",
      "validation loss = 7.367313711266768\n",
      "Batch 129 / 157\n",
      "training loss = 7.234067440032959\n",
      "validation loss = 7.3638269273858326\n",
      "Batch 130 / 157\n",
      "training loss = 7.682544708251953\n",
      "validation loss = 7.356303817347476\n",
      "Batch 131 / 157\n",
      "training loss = 7.47136116027832\n",
      "validation loss = 7.3549175011484245\n",
      "Batch 132 / 157\n",
      "training loss = 7.278656482696533\n",
      "validation loss = 7.350086061578048\n",
      "Batch 133 / 157\n",
      "training loss = 7.348161697387695\n",
      "validation loss = 7.347876674250553\n",
      "Batch 134 / 157\n",
      "training loss = 7.193095684051514\n",
      "validation loss = 7.344101479178981\n",
      "Batch 135 / 157\n",
      "training loss = 7.237730503082275\n",
      "validation loss = 7.336261523397345\n",
      "Batch 136 / 157\n",
      "training loss = 7.137159824371338\n",
      "validation loss = 7.357641069512618\n",
      "Batch 137 / 157\n",
      "training loss = 7.422265529632568\n",
      "validation loss = 7.355190904516923\n",
      "Batch 138 / 157\n",
      "training loss = 7.167809009552002\n",
      "validation loss = 7.340731093758031\n",
      "Batch 139 / 157\n",
      "training loss = 7.4869914054870605\n",
      "validation loss = 7.323978148008647\n",
      "Batch 140 / 157\n",
      "training loss = 7.165447235107422\n",
      "validation loss = 7.335662917086952\n",
      "Batch 141 / 157\n",
      "training loss = 7.4738287925720215\n",
      "validation loss = 7.342295772150943\n",
      "Batch 142 / 157\n",
      "training loss = 7.229753017425537\n",
      "validation loss = 7.331796646118164\n",
      "Batch 143 / 157\n",
      "training loss = 7.382617473602295\n",
      "validation loss = 7.3196314259579305\n",
      "Batch 144 / 157\n",
      "training loss = 7.719545364379883\n",
      "validation loss = 7.31679808466058\n",
      "Batch 145 / 157\n",
      "training loss = 7.317723751068115\n",
      "validation loss = 7.3127789748342416\n",
      "Batch 146 / 157\n",
      "training loss = 7.189035415649414\n",
      "validation loss = 7.3123212111623666\n",
      "Batch 147 / 157\n",
      "training loss = 7.324212551116943\n",
      "validation loss = 7.314628852041144\n",
      "Batch 148 / 157\n",
      "training loss = 7.300169944763184\n",
      "validation loss = 7.306500585455644\n",
      "Batch 149 / 157\n",
      "training loss = 7.159744739532471\n",
      "validation loss = 7.303101740385356\n",
      "Batch 150 / 157\n",
      "training loss = 7.535480499267578\n",
      "validation loss = 7.307662386643259\n",
      "Batch 151 / 157\n",
      "training loss = 7.283552169799805\n",
      "validation loss = 7.297677617324026\n",
      "Batch 152 / 157\n",
      "training loss = 7.274539470672607\n",
      "validation loss = 7.30605431606895\n",
      "Batch 153 / 157\n",
      "training loss = 7.087133407592773\n",
      "validation loss = 7.30497932434082\n",
      "Batch 154 / 157\n",
      "training loss = 7.174181938171387\n",
      "validation loss = 7.2991031345568205\n",
      "Batch 155 / 157\n",
      "training loss = 7.334139347076416\n",
      "validation loss = 7.289489319449977\n",
      "Batch 156 / 157\n",
      "training loss = 7.333470344543457\n",
      "validation loss = 7.288474032753392\n",
      "Batch 157 / 157\n",
      "training loss = 7.173569202423096\n",
      "validation loss = 7.292742929960552\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.378961563110352\n",
      "validation loss = 10.337667916950426\n",
      "Batch 2 / 157\n",
      "training loss = 10.335576057434082\n",
      "validation loss = 10.208526611328125\n",
      "Batch 3 / 157\n",
      "training loss = 10.201891899108887\n",
      "validation loss = 10.094406981217233\n",
      "Batch 4 / 157\n",
      "training loss = 10.076583862304688\n",
      "validation loss = 9.980969880756579\n",
      "Batch 5 / 157\n",
      "training loss = 9.98107624053955\n",
      "validation loss = 9.868421303598504\n",
      "Batch 6 / 157\n",
      "training loss = 9.845632553100586\n",
      "validation loss = 9.752301768252725\n",
      "Batch 7 / 157\n",
      "training loss = 9.791123390197754\n",
      "validation loss = 9.636646672299033\n",
      "Batch 8 / 157\n",
      "training loss = 9.63053035736084\n",
      "validation loss = 9.52265739440918\n",
      "Batch 9 / 157\n",
      "training loss = 9.524113655090332\n",
      "validation loss = 9.40956933874833\n",
      "Batch 10 / 157\n",
      "training loss = 9.410595893859863\n",
      "validation loss = 9.298063880518862\n",
      "Batch 11 / 157\n",
      "training loss = 9.264081954956055\n",
      "validation loss = 9.192207135652241\n",
      "Batch 12 / 157\n",
      "training loss = 9.127128601074219\n",
      "validation loss = 9.08588228727642\n",
      "Batch 13 / 157\n",
      "training loss = 9.027083396911621\n",
      "validation loss = 8.985689062821237\n",
      "Batch 14 / 157\n",
      "training loss = 8.990214347839355\n",
      "validation loss = 8.887860549123664\n",
      "Batch 15 / 157\n",
      "training loss = 8.860443115234375\n",
      "validation loss = 8.796623280173854\n",
      "Batch 16 / 157\n",
      "training loss = 8.806166648864746\n",
      "validation loss = 8.706507532220138\n",
      "Batch 17 / 157\n",
      "training loss = 8.74858570098877\n",
      "validation loss = 8.620880327726665\n",
      "Batch 18 / 157\n",
      "training loss = 8.5155029296875\n",
      "validation loss = 8.5406393251921\n",
      "Batch 19 / 157\n",
      "training loss = 8.695472717285156\n",
      "validation loss = 8.465248760424162\n",
      "Batch 20 / 157\n",
      "training loss = 8.579720497131348\n",
      "validation loss = 8.394388349432694\n",
      "Batch 21 / 157\n",
      "training loss = 8.579575538635254\n",
      "validation loss = 8.325740011114823\n",
      "Batch 22 / 157\n",
      "training loss = 8.28298568725586\n",
      "validation loss = 8.26517973448101\n",
      "Batch 23 / 157\n",
      "training loss = 8.154312133789062\n",
      "validation loss = 8.203818923548647\n",
      "Batch 24 / 157\n",
      "training loss = 8.371200561523438\n",
      "validation loss = 8.148544713070518\n",
      "Batch 25 / 157\n",
      "training loss = 8.227424621582031\n",
      "validation loss = 8.100423561899285\n",
      "Batch 26 / 157\n",
      "training loss = 8.062256813049316\n",
      "validation loss = 8.049436694697329\n",
      "Batch 27 / 157\n",
      "training loss = 8.134648323059082\n",
      "validation loss = 8.007009681902433\n",
      "Batch 28 / 157\n",
      "training loss = 7.884419918060303\n",
      "validation loss = 7.9672193025287825\n",
      "Batch 29 / 157\n",
      "training loss = 7.961403846740723\n",
      "validation loss = 7.928461727343108\n",
      "Batch 30 / 157\n",
      "training loss = 7.855525970458984\n",
      "validation loss = 7.892440570028205\n",
      "Batch 31 / 157\n",
      "training loss = 7.831020355224609\n",
      "validation loss = 7.861401708502519\n",
      "Batch 32 / 157\n",
      "training loss = 8.17996597290039\n",
      "validation loss = 7.829838878230045\n",
      "Batch 33 / 157\n",
      "training loss = 7.8850789070129395\n",
      "validation loss = 7.805812785499974\n",
      "Batch 34 / 157\n",
      "training loss = 7.798381805419922\n",
      "validation loss = 7.781285687496788\n",
      "Batch 35 / 157\n",
      "training loss = 7.959194660186768\n",
      "validation loss = 7.755179430309095\n",
      "Batch 36 / 157\n",
      "training loss = 7.690232753753662\n",
      "validation loss = 7.738881487595408\n",
      "Batch 37 / 157\n",
      "training loss = 7.558221817016602\n",
      "validation loss = 7.720045340688605\n",
      "Batch 38 / 157\n",
      "training loss = 7.889967441558838\n",
      "validation loss = 7.712195371326647\n",
      "Batch 39 / 157\n",
      "training loss = 7.778990745544434\n",
      "validation loss = 7.689303347938939\n",
      "Batch 40 / 157\n",
      "training loss = 7.870817184448242\n",
      "validation loss = 7.681665746789229\n",
      "Batch 41 / 157\n",
      "training loss = 7.567380905151367\n",
      "validation loss = 7.675271511077881\n",
      "Batch 42 / 157\n",
      "training loss = 7.73166036605835\n",
      "validation loss = 7.6651858279579566\n",
      "Batch 43 / 157\n",
      "training loss = 7.551384449005127\n",
      "validation loss = 7.658764061174895\n",
      "Batch 44 / 157\n",
      "training loss = 7.652785778045654\n",
      "validation loss = 7.653957417136745\n",
      "Batch 45 / 157\n",
      "training loss = 7.820024013519287\n",
      "validation loss = 7.647137190166273\n",
      "Batch 46 / 157\n",
      "training loss = 7.716301441192627\n",
      "validation loss = 7.643961329209177\n",
      "Batch 47 / 157\n",
      "training loss = 7.592926502227783\n",
      "validation loss = 7.636795370202315\n",
      "Batch 48 / 157\n",
      "training loss = 7.623096466064453\n",
      "validation loss = 7.631671805130808\n",
      "Batch 49 / 157\n",
      "training loss = 7.647749423980713\n",
      "validation loss = 7.6284275556865495\n",
      "Batch 50 / 157\n",
      "training loss = 7.5957841873168945\n",
      "validation loss = 7.63248902872989\n",
      "Batch 51 / 157\n",
      "training loss = 7.670871257781982\n",
      "validation loss = 7.625353587301154\n",
      "Batch 52 / 157\n",
      "training loss = 7.572807788848877\n",
      "validation loss = 7.621344315378289\n",
      "Batch 53 / 157\n",
      "training loss = 7.818603038787842\n",
      "validation loss = 7.622951206408049\n",
      "Batch 54 / 157\n",
      "training loss = 7.930438041687012\n",
      "validation loss = 7.624126183359246\n",
      "Batch 55 / 157\n",
      "training loss = 7.332606315612793\n",
      "validation loss = 7.619788822374846\n",
      "Batch 56 / 157\n",
      "training loss = 7.667529582977295\n",
      "validation loss = 7.613907864219264\n",
      "Batch 57 / 157\n",
      "training loss = 7.728218078613281\n",
      "validation loss = 7.611484828748201\n",
      "Batch 58 / 157\n",
      "training loss = 7.532278537750244\n",
      "validation loss = 7.613639605672736\n",
      "Batch 59 / 157\n",
      "training loss = 7.539326190948486\n",
      "validation loss = 7.611150766673841\n",
      "Batch 60 / 157\n",
      "training loss = 7.721883296966553\n",
      "validation loss = 7.606827183773643\n",
      "Batch 61 / 157\n",
      "training loss = 7.565210342407227\n",
      "validation loss = 7.609774288378264\n",
      "Batch 62 / 157\n",
      "training loss = 7.606602668762207\n",
      "validation loss = 7.599669908222399\n",
      "Batch 63 / 157\n",
      "training loss = 7.570106029510498\n",
      "validation loss = 7.599531801123368\n",
      "Batch 64 / 157\n",
      "training loss = 7.903710842132568\n",
      "validation loss = 7.596246644070274\n",
      "Batch 65 / 157\n",
      "training loss = 7.691980838775635\n",
      "validation loss = 7.596748904178017\n",
      "Batch 66 / 157\n",
      "training loss = 7.406789302825928\n",
      "validation loss = 7.583875229484157\n",
      "Batch 67 / 157\n",
      "training loss = 7.537995338439941\n",
      "validation loss = 7.580700096331145\n",
      "Batch 68 / 157\n",
      "training loss = 7.613945007324219\n",
      "validation loss = 7.58132603293971\n",
      "Batch 69 / 157\n",
      "training loss = 7.70147180557251\n",
      "validation loss = 7.581838708174856\n",
      "Batch 70 / 157\n",
      "training loss = 7.718164443969727\n",
      "validation loss = 7.574711096914191\n",
      "Batch 71 / 157\n",
      "training loss = 7.645033359527588\n",
      "validation loss = 7.572386139317563\n",
      "Batch 72 / 157\n",
      "training loss = 7.5573859214782715\n",
      "validation loss = 7.566836708470395\n",
      "Batch 73 / 157\n",
      "training loss = 7.495307922363281\n",
      "validation loss = 7.560376117103978\n",
      "Batch 74 / 157\n",
      "training loss = 7.288939952850342\n",
      "validation loss = 7.55863638928062\n",
      "Batch 75 / 157\n",
      "training loss = 7.54851770401001\n",
      "validation loss = 7.557180781113474\n",
      "Batch 76 / 157\n",
      "training loss = 7.401142597198486\n",
      "validation loss = 7.554788589477539\n",
      "Batch 77 / 157\n",
      "training loss = 7.5774736404418945\n",
      "validation loss = 7.55019147772538\n",
      "Batch 78 / 157\n",
      "training loss = 7.651839256286621\n",
      "validation loss = 7.5407817740189405\n",
      "Batch 79 / 157\n",
      "training loss = 7.630071640014648\n",
      "validation loss = 7.5400263886702685\n",
      "Batch 80 / 157\n",
      "training loss = 7.579244136810303\n",
      "validation loss = 7.537505927838777\n",
      "Batch 81 / 157\n",
      "training loss = 7.595351219177246\n",
      "validation loss = 7.537320488377621\n",
      "Batch 82 / 157\n",
      "training loss = 7.552877902984619\n",
      "validation loss = 7.525465463337145\n",
      "Batch 83 / 157\n",
      "training loss = 7.720484733581543\n",
      "validation loss = 7.518940875404759\n",
      "Batch 84 / 157\n",
      "training loss = 7.605171203613281\n",
      "validation loss = 7.5190074318333675\n",
      "Batch 85 / 157\n",
      "training loss = 7.584141254425049\n",
      "validation loss = 7.512504577636719\n",
      "Batch 86 / 157\n",
      "training loss = 7.641457557678223\n",
      "validation loss = 7.50922797855578\n",
      "Batch 87 / 157\n",
      "training loss = 7.62415075302124\n",
      "validation loss = 7.506829086102937\n",
      "Batch 88 / 157\n",
      "training loss = 7.464043617248535\n",
      "validation loss = 7.504858167547929\n",
      "Batch 89 / 157\n",
      "training loss = 7.499293804168701\n",
      "validation loss = 7.49719436545121\n",
      "Batch 90 / 157\n",
      "training loss = 7.63066291809082\n",
      "validation loss = 7.495979233791954\n",
      "Batch 91 / 157\n",
      "training loss = 7.301266670227051\n",
      "validation loss = 7.490337095762554\n",
      "Batch 92 / 157\n",
      "training loss = 7.285869121551514\n",
      "validation loss = 7.492498975051077\n",
      "Batch 93 / 157\n",
      "training loss = 7.8107829093933105\n",
      "validation loss = 7.490142521105315\n",
      "Batch 94 / 157\n",
      "training loss = 7.26564884185791\n",
      "validation loss = 7.4835053494102075\n",
      "Batch 95 / 157\n",
      "training loss = 7.559494972229004\n",
      "validation loss = 7.478479134409051\n",
      "Batch 96 / 157\n",
      "training loss = 7.63923978805542\n",
      "validation loss = 7.480296310625579\n",
      "Batch 97 / 157\n",
      "training loss = 7.5332512855529785\n",
      "validation loss = 7.470190826215242\n",
      "Batch 98 / 157\n",
      "training loss = 7.541262626647949\n",
      "validation loss = 7.477270728663394\n",
      "Batch 99 / 157\n",
      "training loss = 7.466209411621094\n",
      "validation loss = 7.467428859911467\n",
      "Batch 100 / 157\n",
      "training loss = 7.495381832122803\n",
      "validation loss = 7.45755524384348\n",
      "Batch 101 / 157\n",
      "training loss = 7.4441375732421875\n",
      "validation loss = 7.452160132558722\n",
      "Batch 102 / 157\n",
      "training loss = 7.925868511199951\n",
      "validation loss = 7.454741678739849\n",
      "Batch 103 / 157\n",
      "training loss = 7.500557899475098\n",
      "validation loss = 7.455533429196007\n",
      "Batch 104 / 157\n",
      "training loss = 7.503046035766602\n",
      "validation loss = 7.443619326541298\n",
      "Batch 105 / 157\n",
      "training loss = 7.443992614746094\n",
      "validation loss = 7.443967467860172\n",
      "Batch 106 / 157\n",
      "training loss = 7.501601219177246\n",
      "validation loss = 7.4422984625163835\n",
      "Batch 107 / 157\n",
      "training loss = 7.4615478515625\n",
      "validation loss = 7.4390648038763745\n",
      "Batch 108 / 157\n",
      "training loss = 7.407399654388428\n",
      "validation loss = 7.434464078200491\n",
      "Batch 109 / 157\n",
      "training loss = 7.388485431671143\n",
      "validation loss = 7.4266868892468905\n",
      "Batch 110 / 157\n",
      "training loss = 7.357395648956299\n",
      "validation loss = 7.420957615501003\n",
      "Batch 111 / 157\n",
      "training loss = 7.4498748779296875\n",
      "validation loss = 7.4227212855690405\n",
      "Batch 112 / 157\n",
      "training loss = 7.401455402374268\n",
      "validation loss = 7.422483644987407\n",
      "Batch 113 / 157\n",
      "training loss = 7.244356632232666\n",
      "validation loss = 7.415243901704487\n",
      "Batch 114 / 157\n",
      "training loss = 7.555895805358887\n",
      "validation loss = 7.410225843128405\n",
      "Batch 115 / 157\n",
      "training loss = 7.412493705749512\n",
      "validation loss = 7.412307237323962\n",
      "Batch 116 / 157\n",
      "training loss = 7.272326946258545\n",
      "validation loss = 7.406462117245323\n",
      "Batch 117 / 157\n",
      "training loss = 7.4482340812683105\n",
      "validation loss = 7.405369181382029\n",
      "Batch 118 / 157\n",
      "training loss = 7.435976982116699\n",
      "validation loss = 7.403756091469212\n",
      "Batch 119 / 157\n",
      "training loss = 7.291353225708008\n",
      "validation loss = 7.400997136768542\n",
      "Batch 120 / 157\n",
      "training loss = 7.333155155181885\n",
      "validation loss = 7.40083092137387\n",
      "Batch 121 / 157\n",
      "training loss = 7.583700180053711\n",
      "validation loss = 7.40371581127769\n",
      "Batch 122 / 157\n",
      "training loss = 7.38319730758667\n",
      "validation loss = 7.396535371479235\n",
      "Batch 123 / 157\n",
      "training loss = 7.339359283447266\n",
      "validation loss = 7.392168245817485\n",
      "Batch 124 / 157\n",
      "training loss = 7.641600131988525\n",
      "validation loss = 7.395351685975728\n",
      "Batch 125 / 157\n",
      "training loss = 7.571271896362305\n",
      "validation loss = 7.39312397806268\n",
      "Batch 126 / 157\n",
      "training loss = 7.414956569671631\n",
      "validation loss = 7.392456732298198\n",
      "Batch 127 / 157\n",
      "training loss = 7.78012228012085\n",
      "validation loss = 7.389014495046515\n",
      "Batch 128 / 157\n",
      "training loss = 7.71205997467041\n",
      "validation loss = 7.395330504367226\n",
      "Batch 129 / 157\n",
      "training loss = 7.447688579559326\n",
      "validation loss = 7.384403103276303\n",
      "Batch 130 / 157\n",
      "training loss = 7.2170209884643555\n",
      "validation loss = 7.3800633832028035\n",
      "Batch 131 / 157\n",
      "training loss = 7.375893592834473\n",
      "validation loss = 7.379365770440352\n",
      "Batch 132 / 157\n",
      "training loss = 7.359307765960693\n",
      "validation loss = 7.382643147518761\n",
      "Batch 133 / 157\n",
      "training loss = 7.66703462600708\n",
      "validation loss = 7.370988619954963\n",
      "Batch 134 / 157\n",
      "training loss = 7.587272644042969\n",
      "validation loss = 7.36610598313181\n",
      "Batch 135 / 157\n",
      "training loss = 7.63947057723999\n",
      "validation loss = 7.374677306727359\n",
      "Batch 136 / 157\n",
      "training loss = 7.36044454574585\n",
      "validation loss = 7.380187134993704\n",
      "Batch 137 / 157\n",
      "training loss = 7.282149791717529\n",
      "validation loss = 7.3609394023292944\n",
      "Batch 138 / 157\n",
      "training loss = 7.288527965545654\n",
      "validation loss = 7.3603191124765495\n",
      "Batch 139 / 157\n",
      "training loss = 7.089128494262695\n",
      "validation loss = 7.364558345393131\n",
      "Batch 140 / 157\n",
      "training loss = 7.411920070648193\n",
      "validation loss = 7.370635911038048\n",
      "Batch 141 / 157\n",
      "training loss = 7.499754905700684\n",
      "validation loss = 7.3632684757834985\n",
      "Batch 142 / 157\n",
      "training loss = 7.2450761795043945\n",
      "validation loss = 7.362003803253174\n",
      "Batch 143 / 157\n",
      "training loss = 7.250708103179932\n",
      "validation loss = 7.356299877166748\n",
      "Batch 144 / 157\n",
      "training loss = 7.011262893676758\n",
      "validation loss = 7.353374832554867\n",
      "Batch 145 / 157\n",
      "training loss = 7.012747287750244\n",
      "validation loss = 7.337610495717902\n",
      "Batch 146 / 157\n",
      "training loss = 7.2509989738464355\n",
      "validation loss = 7.337776334662187\n",
      "Batch 147 / 157\n",
      "training loss = 7.494670867919922\n",
      "validation loss = 7.343360499331825\n",
      "Batch 148 / 157\n",
      "training loss = 7.398259162902832\n",
      "validation loss = 7.341786961806448\n",
      "Batch 149 / 157\n",
      "training loss = 7.4459052085876465\n",
      "validation loss = 7.335522375608745\n",
      "Batch 150 / 157\n",
      "training loss = 7.501403331756592\n",
      "validation loss = 7.337267373737536\n",
      "Batch 151 / 157\n",
      "training loss = 7.362629413604736\n",
      "validation loss = 7.3380028072156405\n",
      "Batch 152 / 157\n",
      "training loss = 7.337467193603516\n",
      "validation loss = 7.324750799881785\n",
      "Batch 153 / 157\n",
      "training loss = 7.469260215759277\n",
      "validation loss = 7.327033820905183\n",
      "Batch 154 / 157\n",
      "training loss = 7.43623161315918\n",
      "validation loss = 7.32677866283216\n",
      "Batch 155 / 157\n",
      "training loss = 7.420181751251221\n",
      "validation loss = 7.324981087132504\n",
      "Batch 156 / 157\n",
      "training loss = 7.369959831237793\n",
      "validation loss = 7.326640982376902\n",
      "Batch 157 / 157\n",
      "training loss = 7.263565540313721\n",
      "validation loss = 7.32666560223228\n",
      "Average training loss: 7.81\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.370993614196777\n",
      "validation loss = 10.30433554398386\n",
      "Batch 2 / 157\n",
      "training loss = 10.312747955322266\n",
      "validation loss = 10.17285402197587\n",
      "Batch 3 / 157\n",
      "training loss = 10.17448902130127\n",
      "validation loss = 10.054770369278756\n",
      "Batch 4 / 157\n",
      "training loss = 10.063658714294434\n",
      "validation loss = 9.93756063360917\n",
      "Batch 5 / 157\n",
      "training loss = 9.948885917663574\n",
      "validation loss = 9.817680358886719\n",
      "Batch 6 / 157\n",
      "training loss = 9.775979042053223\n",
      "validation loss = 9.699990222328587\n",
      "Batch 7 / 157\n",
      "training loss = 9.679604530334473\n",
      "validation loss = 9.581891712389494\n",
      "Batch 8 / 157\n",
      "training loss = 9.550453186035156\n",
      "validation loss = 9.466024448997096\n",
      "Batch 9 / 157\n",
      "training loss = 9.421056747436523\n",
      "validation loss = 9.35249830547132\n",
      "Batch 10 / 157\n",
      "training loss = 9.298186302185059\n",
      "validation loss = 9.241653392189427\n",
      "Batch 11 / 157\n",
      "training loss = 9.246618270874023\n",
      "validation loss = 9.13690898292943\n",
      "Batch 12 / 157\n",
      "training loss = 9.204761505126953\n",
      "validation loss = 9.032658376191792\n",
      "Batch 13 / 157\n",
      "training loss = 8.971453666687012\n",
      "validation loss = 8.933112646404066\n",
      "Batch 14 / 157\n",
      "training loss = 8.893804550170898\n",
      "validation loss = 8.834812415273566\n",
      "Batch 15 / 157\n",
      "training loss = 8.880495071411133\n",
      "validation loss = 8.745341652318052\n",
      "Batch 16 / 157\n",
      "training loss = 8.70345401763916\n",
      "validation loss = 8.655622482299805\n",
      "Batch 17 / 157\n",
      "training loss = 8.649109840393066\n",
      "validation loss = 8.572607893692819\n",
      "Batch 18 / 157\n",
      "training loss = 8.57349681854248\n",
      "validation loss = 8.494578110544305\n",
      "Batch 19 / 157\n",
      "training loss = 8.467681884765625\n",
      "validation loss = 8.42084729044061\n",
      "Batch 20 / 157\n",
      "training loss = 8.422490119934082\n",
      "validation loss = 8.353392199466104\n",
      "Batch 21 / 157\n",
      "training loss = 8.271227836608887\n",
      "validation loss = 8.285378958049574\n",
      "Batch 22 / 157\n",
      "training loss = 8.106781005859375\n",
      "validation loss = 8.222197080913343\n",
      "Batch 23 / 157\n",
      "training loss = 8.197440147399902\n",
      "validation loss = 8.161517645183363\n",
      "Batch 24 / 157\n",
      "training loss = 8.205284118652344\n",
      "validation loss = 8.110143661499023\n",
      "Batch 25 / 157\n",
      "training loss = 8.144411087036133\n",
      "validation loss = 8.058522550683273\n",
      "Batch 26 / 157\n",
      "training loss = 8.277064323425293\n",
      "validation loss = 8.013908762680856\n",
      "Batch 27 / 157\n",
      "training loss = 7.960839748382568\n",
      "validation loss = 7.963473269813939\n",
      "Batch 28 / 157\n",
      "training loss = 7.942025661468506\n",
      "validation loss = 7.928221878252532\n",
      "Batch 29 / 157\n",
      "training loss = 7.895517826080322\n",
      "validation loss = 7.889267946544447\n",
      "Batch 30 / 157\n",
      "training loss = 7.774278163909912\n",
      "validation loss = 7.850543549186305\n",
      "Batch 31 / 157\n",
      "training loss = 7.885432720184326\n",
      "validation loss = 7.81528733905993\n",
      "Batch 32 / 157\n",
      "training loss = 7.875715255737305\n",
      "validation loss = 7.788737271961413\n",
      "Batch 33 / 157\n",
      "training loss = 7.621810436248779\n",
      "validation loss = 7.7591922659623\n",
      "Batch 34 / 157\n",
      "training loss = 7.678248405456543\n",
      "validation loss = 7.734768139688592\n",
      "Batch 35 / 157\n",
      "training loss = 7.640753746032715\n",
      "validation loss = 7.715769491697612\n",
      "Batch 36 / 157\n",
      "training loss = 7.490065574645996\n",
      "validation loss = 7.698066084008468\n",
      "Batch 37 / 157\n",
      "training loss = 7.709350109100342\n",
      "validation loss = 7.668282910397179\n",
      "Batch 38 / 157\n",
      "training loss = 7.7526655197143555\n",
      "validation loss = 7.660707875301964\n",
      "Batch 39 / 157\n",
      "training loss = 7.530717372894287\n",
      "validation loss = 7.659814483241031\n",
      "Batch 40 / 157\n",
      "training loss = 7.592530727386475\n",
      "validation loss = 7.634540633151405\n",
      "Batch 41 / 157\n",
      "training loss = 7.739530563354492\n",
      "validation loss = 7.620217222916453\n",
      "Batch 42 / 157\n",
      "training loss = 7.640920639038086\n",
      "validation loss = 7.6158039193404345\n",
      "Batch 43 / 157\n",
      "training loss = 7.664966106414795\n",
      "validation loss = 7.60811364023309\n",
      "Batch 44 / 157\n",
      "training loss = 7.621982097625732\n",
      "validation loss = 7.607734956239399\n",
      "Batch 45 / 157\n",
      "training loss = 7.591209411621094\n",
      "validation loss = 7.607305175379703\n",
      "Batch 46 / 157\n",
      "training loss = 7.74877405166626\n",
      "validation loss = 7.595795505925229\n",
      "Batch 47 / 157\n",
      "training loss = 7.511002540588379\n",
      "validation loss = 7.5968418623271745\n",
      "Batch 48 / 157\n",
      "training loss = 7.6332502365112305\n",
      "validation loss = 7.5966563726726335\n",
      "Batch 49 / 157\n",
      "training loss = 7.801853656768799\n",
      "validation loss = 7.595979439584832\n",
      "Batch 50 / 157\n",
      "training loss = 7.549987316131592\n",
      "validation loss = 7.592082349877608\n",
      "Batch 51 / 157\n",
      "training loss = 7.481570720672607\n",
      "validation loss = 7.587029607672441\n",
      "Batch 52 / 157\n",
      "training loss = 7.545426368713379\n",
      "validation loss = 7.583472578149093\n",
      "Batch 53 / 157\n",
      "training loss = 7.530159950256348\n",
      "validation loss = 7.590834592518053\n",
      "Batch 54 / 157\n",
      "training loss = 7.760315418243408\n",
      "validation loss = 7.58594294598228\n",
      "Batch 55 / 157\n",
      "training loss = 7.816600322723389\n",
      "validation loss = 7.583387625844855\n",
      "Batch 56 / 157\n",
      "training loss = 7.471275806427002\n",
      "validation loss = 7.578931206151059\n",
      "Batch 57 / 157\n",
      "training loss = 7.348655700683594\n",
      "validation loss = 7.577525490208676\n",
      "Batch 58 / 157\n",
      "training loss = 7.568192481994629\n",
      "validation loss = 7.569612829308761\n",
      "Batch 59 / 157\n",
      "training loss = 7.5622124671936035\n",
      "validation loss = 7.567553470009251\n",
      "Batch 60 / 157\n",
      "training loss = 7.389828205108643\n",
      "validation loss = 7.562102393100136\n",
      "Batch 61 / 157\n",
      "training loss = 7.583033561706543\n",
      "validation loss = 7.560427213969984\n",
      "Batch 62 / 157\n",
      "training loss = 7.54497766494751\n",
      "validation loss = 7.561667015678005\n",
      "Batch 63 / 157\n",
      "training loss = 7.666164875030518\n",
      "validation loss = 7.556055721483733\n",
      "Batch 64 / 157\n",
      "training loss = 7.344845771789551\n",
      "validation loss = 7.555044726321571\n",
      "Batch 65 / 157\n",
      "training loss = 7.576512813568115\n",
      "validation loss = 7.542542909321032\n",
      "Batch 66 / 157\n",
      "training loss = 7.704734802246094\n",
      "validation loss = 7.553605456101267\n",
      "Batch 67 / 157\n",
      "training loss = 7.497805118560791\n",
      "validation loss = 7.536888423718904\n",
      "Batch 68 / 157\n",
      "training loss = 7.323704242706299\n",
      "validation loss = 7.532331491771497\n",
      "Batch 69 / 157\n",
      "training loss = 7.598312854766846\n",
      "validation loss = 7.5359726203115365\n",
      "Batch 70 / 157\n",
      "training loss = 7.390199661254883\n",
      "validation loss = 7.542631701419228\n",
      "Batch 71 / 157\n",
      "training loss = 7.894693851470947\n",
      "validation loss = 7.528391411429958\n",
      "Batch 72 / 157\n",
      "training loss = 7.531037330627441\n",
      "validation loss = 7.515068104392604\n",
      "Batch 73 / 157\n",
      "training loss = 7.9493021965026855\n",
      "validation loss = 7.52298106645283\n",
      "Batch 74 / 157\n",
      "training loss = 8.089773178100586\n",
      "validation loss = 7.5557990074157715\n",
      "Batch 75 / 157\n",
      "training loss = 7.297878265380859\n",
      "validation loss = 7.537801315909938\n",
      "Batch 76 / 157\n",
      "training loss = 7.513847351074219\n",
      "validation loss = 7.5192006763659025\n",
      "Batch 77 / 157\n",
      "training loss = 7.544224739074707\n",
      "validation loss = 7.520492127067165\n",
      "Batch 78 / 157\n",
      "training loss = 7.538483142852783\n",
      "validation loss = 7.533349714781108\n",
      "Batch 79 / 157\n",
      "training loss = 7.533517837524414\n",
      "validation loss = 7.541895766007273\n",
      "Batch 80 / 157\n",
      "training loss = 7.349474906921387\n",
      "validation loss = 7.532532917825799\n",
      "Batch 81 / 157\n",
      "training loss = 7.5693254470825195\n",
      "validation loss = 7.537356502131412\n",
      "Batch 82 / 157\n",
      "training loss = 7.823660850524902\n",
      "validation loss = 7.526436278694554\n",
      "Batch 83 / 157\n",
      "training loss = 7.451693534851074\n",
      "validation loss = 7.525716304779053\n",
      "Batch 84 / 157\n",
      "training loss = 7.659090995788574\n",
      "validation loss = 7.51621319118299\n",
      "Batch 85 / 157\n",
      "training loss = 7.399151802062988\n",
      "validation loss = 7.506509404433401\n",
      "Batch 86 / 157\n",
      "training loss = 7.609290599822998\n",
      "validation loss = 7.485228061676025\n",
      "Batch 87 / 157\n",
      "training loss = 7.598386764526367\n",
      "validation loss = 7.485395908355713\n",
      "Batch 88 / 157\n",
      "training loss = 7.289554119110107\n",
      "validation loss = 7.481817119999936\n",
      "Batch 89 / 157\n",
      "training loss = 7.561797142028809\n",
      "validation loss = 7.475788392518696\n",
      "Batch 90 / 157\n",
      "training loss = 7.68682336807251\n",
      "validation loss = 7.479560375213623\n",
      "Batch 91 / 157\n",
      "training loss = 7.555616855621338\n",
      "validation loss = 7.464517568287096\n",
      "Batch 92 / 157\n",
      "training loss = 7.495909690856934\n",
      "validation loss = 7.4633840259752775\n",
      "Batch 93 / 157\n",
      "training loss = 7.505368709564209\n",
      "validation loss = 7.4522680734333235\n",
      "Batch 94 / 157\n",
      "training loss = 7.388185024261475\n",
      "validation loss = 7.451642287404914\n",
      "Batch 95 / 157\n",
      "training loss = 7.651371479034424\n",
      "validation loss = 7.448725047864412\n",
      "Batch 96 / 157\n",
      "training loss = 7.541667938232422\n",
      "validation loss = 7.446124704260575\n",
      "Batch 97 / 157\n",
      "training loss = 7.352179050445557\n",
      "validation loss = 7.449338059676321\n",
      "Batch 98 / 157\n",
      "training loss = 7.4074015617370605\n",
      "validation loss = 7.442860126495361\n",
      "Batch 99 / 157\n",
      "training loss = 7.489038467407227\n",
      "validation loss = 7.441812816419099\n",
      "Batch 100 / 157\n",
      "training loss = 7.508208274841309\n",
      "validation loss = 7.441580897883365\n",
      "Batch 101 / 157\n",
      "training loss = 7.3414692878723145\n",
      "validation loss = 7.4304249663102\n",
      "Batch 102 / 157\n",
      "training loss = 7.255744934082031\n",
      "validation loss = 7.429443585245233\n",
      "Batch 103 / 157\n",
      "training loss = 7.386595249176025\n",
      "validation loss = 7.428770793111701\n",
      "Batch 104 / 157\n",
      "training loss = 7.5185675621032715\n",
      "validation loss = 7.425334127325761\n",
      "Batch 105 / 157\n",
      "training loss = 7.690970420837402\n",
      "validation loss = 7.429354993920577\n",
      "Batch 106 / 157\n",
      "training loss = 7.320492744445801\n",
      "validation loss = 7.418588211661891\n",
      "Batch 107 / 157\n",
      "training loss = 7.418184280395508\n",
      "validation loss = 7.420080561386912\n",
      "Batch 108 / 157\n",
      "training loss = 7.428025722503662\n",
      "validation loss = 7.41596962276258\n",
      "Batch 109 / 157\n",
      "training loss = 7.32234001159668\n",
      "validation loss = 7.414816755997507\n",
      "Batch 110 / 157\n",
      "training loss = 7.457002639770508\n",
      "validation loss = 7.402992675178929\n",
      "Batch 111 / 157\n",
      "training loss = 7.338063716888428\n",
      "validation loss = 7.402899064515767\n",
      "Batch 112 / 157\n",
      "training loss = 7.418572902679443\n",
      "validation loss = 7.399445433365671\n",
      "Batch 113 / 157\n",
      "training loss = 7.851047039031982\n",
      "validation loss = 7.400039221111097\n",
      "Batch 114 / 157\n",
      "training loss = 7.3052978515625\n",
      "validation loss = 7.395093466106214\n",
      "Batch 115 / 157\n",
      "training loss = 7.333581447601318\n",
      "validation loss = 7.396453029231021\n",
      "Batch 116 / 157\n",
      "training loss = 7.453614234924316\n",
      "validation loss = 7.3916825244301245\n",
      "Batch 117 / 157\n",
      "training loss = 7.340764045715332\n",
      "validation loss = 7.3874773728220084\n",
      "Batch 118 / 157\n",
      "training loss = 7.353640556335449\n",
      "validation loss = 7.382384475908782\n",
      "Batch 119 / 157\n",
      "training loss = 7.462743759155273\n",
      "validation loss = 7.385112335807399\n",
      "Batch 120 / 157\n",
      "training loss = 7.484804630279541\n",
      "validation loss = 7.370408409520199\n",
      "Batch 121 / 157\n",
      "training loss = 7.314166069030762\n",
      "validation loss = 7.377616355293675\n",
      "Batch 122 / 157\n",
      "training loss = 7.329588413238525\n",
      "validation loss = 7.375075741818077\n",
      "Batch 123 / 157\n",
      "training loss = 7.228430271148682\n",
      "validation loss = 7.368795921928005\n",
      "Batch 124 / 157\n",
      "training loss = 7.007796287536621\n",
      "validation loss = 7.359404438420346\n",
      "Batch 125 / 157\n",
      "training loss = 7.28622579574585\n",
      "validation loss = 7.36745053843448\n",
      "Batch 126 / 157\n",
      "training loss = 7.395556449890137\n",
      "validation loss = 7.3636879669992545\n",
      "Batch 127 / 157\n",
      "training loss = 7.278403282165527\n",
      "validation loss = 7.362665703422145\n",
      "Batch 128 / 157\n",
      "training loss = 7.265437602996826\n",
      "validation loss = 7.344180458470395\n",
      "Batch 129 / 157\n",
      "training loss = 7.177517414093018\n",
      "validation loss = 7.351037753255744\n",
      "Batch 130 / 157\n",
      "training loss = 7.252655029296875\n",
      "validation loss = 7.348402675829436\n",
      "Batch 131 / 157\n",
      "training loss = 7.487659931182861\n",
      "validation loss = 7.34507041228445\n",
      "Batch 132 / 157\n",
      "training loss = 7.457669734954834\n",
      "validation loss = 7.347491113763106\n",
      "Batch 133 / 157\n",
      "training loss = 7.372058868408203\n",
      "validation loss = 7.342080216658743\n",
      "Batch 134 / 157\n",
      "training loss = 7.415295600891113\n",
      "validation loss = 7.342652421248586\n",
      "Batch 135 / 157\n",
      "training loss = 7.0491437911987305\n",
      "validation loss = 7.329770288969341\n",
      "Batch 136 / 157\n",
      "training loss = 7.229719161987305\n",
      "validation loss = 7.335525136244924\n",
      "Batch 137 / 157\n",
      "training loss = 7.248106479644775\n",
      "validation loss = 7.327192733162327\n",
      "Batch 138 / 157\n",
      "training loss = 7.279684543609619\n",
      "validation loss = 7.324003119217722\n",
      "Batch 139 / 157\n",
      "training loss = 7.291713237762451\n",
      "validation loss = 7.3252599866766674\n",
      "Batch 140 / 157\n",
      "training loss = 7.3129754066467285\n",
      "validation loss = 7.315677567532188\n",
      "Batch 141 / 157\n",
      "training loss = 7.3325300216674805\n",
      "validation loss = 7.321753928535863\n",
      "Batch 142 / 157\n",
      "training loss = 7.238717079162598\n",
      "validation loss = 7.319192158548455\n",
      "Batch 143 / 157\n",
      "training loss = 7.339017868041992\n",
      "validation loss = 7.312231841840242\n",
      "Batch 144 / 157\n",
      "training loss = 7.575357437133789\n",
      "validation loss = 7.311153411865234\n",
      "Batch 145 / 157\n",
      "training loss = 7.2938618659973145\n",
      "validation loss = 7.315444067904823\n",
      "Batch 146 / 157\n",
      "training loss = 7.300422191619873\n",
      "validation loss = 7.313712621990003\n",
      "Batch 147 / 157\n",
      "training loss = 7.3622894287109375\n",
      "validation loss = 7.305085031609786\n",
      "Batch 148 / 157\n",
      "training loss = 7.149997234344482\n",
      "validation loss = 7.304987003928737\n",
      "Batch 149 / 157\n",
      "training loss = 7.260858535766602\n",
      "validation loss = 7.314152516816792\n",
      "Batch 150 / 157\n",
      "training loss = 7.404236316680908\n",
      "validation loss = 7.302566001289769\n",
      "Batch 151 / 157\n",
      "training loss = 7.404059410095215\n",
      "validation loss = 7.301730582588597\n",
      "Batch 152 / 157\n",
      "training loss = 7.224363803863525\n",
      "validation loss = 7.292878025456479\n",
      "Batch 153 / 157\n",
      "training loss = 7.4027276039123535\n",
      "validation loss = 7.288446551875064\n",
      "Batch 154 / 157\n",
      "training loss = 7.032093048095703\n",
      "validation loss = 7.288259104678505\n",
      "Batch 155 / 157\n",
      "training loss = 7.153875827789307\n",
      "validation loss = 7.296514285238166\n",
      "Batch 156 / 157\n",
      "training loss = 7.4421892166137695\n",
      "validation loss = 7.292668869620876\n",
      "Batch 157 / 157\n",
      "training loss = 7.407525539398193\n",
      "validation loss = 7.277042087755706\n",
      "Average training loss: 7.75\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.394533157348633\n",
      "validation loss = 10.199946252923263\n",
      "Batch 2 / 157\n",
      "training loss = 10.21310043334961\n",
      "validation loss = 9.819549660933646\n",
      "Batch 3 / 157\n",
      "training loss = 9.862884521484375\n",
      "validation loss = 9.453529357910156\n",
      "Batch 4 / 157\n",
      "training loss = 9.402180671691895\n",
      "validation loss = 9.114867110001413\n",
      "Batch 5 / 157\n",
      "training loss = 9.014881134033203\n",
      "validation loss = 8.788502040662264\n",
      "Batch 6 / 157\n",
      "training loss = 8.650157928466797\n",
      "validation loss = 8.496765839426141\n",
      "Batch 7 / 157\n",
      "training loss = 8.614544868469238\n",
      "validation loss = 8.26440103430497\n",
      "Batch 8 / 157\n",
      "training loss = 8.114296913146973\n",
      "validation loss = 8.126614821584601\n",
      "Batch 9 / 157\n",
      "training loss = 8.126908302307129\n",
      "validation loss = 8.070164354223953\n",
      "Batch 10 / 157\n",
      "training loss = 8.475546836853027\n",
      "validation loss = 8.08384488758288\n",
      "Batch 11 / 157\n",
      "training loss = 8.175331115722656\n",
      "validation loss = 8.11937555513884\n",
      "Batch 12 / 157\n",
      "training loss = 7.9580841064453125\n",
      "validation loss = 8.139176343616686\n",
      "Batch 13 / 157\n",
      "training loss = 7.951090335845947\n",
      "validation loss = 8.1010194075735\n",
      "Batch 14 / 157\n",
      "training loss = 7.711730003356934\n",
      "validation loss = 8.009645913776598\n",
      "Batch 15 / 157\n",
      "training loss = 7.823649883270264\n",
      "validation loss = 7.972543339980276\n",
      "Batch 16 / 157\n",
      "training loss = 8.270647048950195\n",
      "validation loss = 8.05361368781642\n",
      "Batch 17 / 157\n",
      "training loss = 7.971867561340332\n",
      "validation loss = 7.956455356196353\n",
      "Batch 18 / 157\n",
      "training loss = 7.944498062133789\n",
      "validation loss = 7.963197105809262\n",
      "Batch 19 / 157\n",
      "training loss = 8.187031745910645\n",
      "validation loss = 7.984870132647063\n",
      "Batch 20 / 157\n",
      "training loss = 7.9500017166137695\n",
      "validation loss = 7.977948615425511\n",
      "Batch 21 / 157\n",
      "training loss = 8.144904136657715\n",
      "validation loss = 7.945113006391023\n",
      "Batch 22 / 157\n",
      "training loss = 7.988921165466309\n",
      "validation loss = 7.910593835931075\n",
      "Batch 23 / 157\n",
      "training loss = 8.421146392822266\n",
      "validation loss = 7.909039924019261\n",
      "Batch 24 / 157\n",
      "training loss = 7.627328395843506\n",
      "validation loss = 7.926350919823897\n",
      "Batch 25 / 157\n",
      "training loss = 7.734122276306152\n",
      "validation loss = 7.914437921423661\n",
      "Batch 26 / 157\n",
      "training loss = 7.808806419372559\n",
      "validation loss = 7.907256879304585\n",
      "Batch 27 / 157\n",
      "training loss = 8.000872611999512\n",
      "validation loss = 7.9083730547051685\n",
      "Batch 28 / 157\n",
      "training loss = 8.369473457336426\n",
      "validation loss = 7.913601222791169\n",
      "Batch 29 / 157\n",
      "training loss = 8.05593204498291\n",
      "validation loss = 7.897504505358245\n",
      "Batch 30 / 157\n",
      "training loss = 7.935041904449463\n",
      "validation loss = 7.880563936735454\n",
      "Batch 31 / 157\n",
      "training loss = 7.995187759399414\n",
      "validation loss = 7.880297510247481\n",
      "Batch 32 / 157\n",
      "training loss = 7.671800136566162\n",
      "validation loss = 7.8597286374945385\n",
      "Batch 33 / 157\n",
      "training loss = 7.884899139404297\n",
      "validation loss = 7.827340828744989\n",
      "Batch 34 / 157\n",
      "training loss = 7.978609085083008\n",
      "validation loss = 7.809162666923122\n",
      "Batch 35 / 157\n",
      "training loss = 7.8394622802734375\n",
      "validation loss = 7.777956761811909\n",
      "Batch 36 / 157\n",
      "training loss = 7.801845073699951\n",
      "validation loss = 7.764807324660452\n",
      "Batch 37 / 157\n",
      "training loss = 7.861146926879883\n",
      "validation loss = 7.744923190066689\n",
      "Batch 38 / 157\n",
      "training loss = 7.392688751220703\n",
      "validation loss = 7.791466537274812\n",
      "Batch 39 / 157\n",
      "training loss = 7.778778076171875\n",
      "validation loss = 7.748310490658409\n",
      "Batch 40 / 157\n",
      "training loss = 7.83584451675415\n",
      "validation loss = 7.738893308137593\n",
      "Batch 41 / 157\n",
      "training loss = 7.721756935119629\n",
      "validation loss = 7.769491923482795\n",
      "Batch 42 / 157\n",
      "training loss = 8.027612686157227\n",
      "validation loss = 7.730211759868421\n",
      "Batch 43 / 157\n",
      "training loss = 7.9296488761901855\n",
      "validation loss = 7.757315284327457\n",
      "Batch 44 / 157\n",
      "training loss = 7.972205638885498\n",
      "validation loss = 7.73480897200735\n",
      "Batch 45 / 157\n",
      "training loss = 7.781684398651123\n",
      "validation loss = 7.722847210733514\n",
      "Batch 46 / 157\n",
      "training loss = 7.863300323486328\n",
      "validation loss = 7.7390563613490055\n",
      "Batch 47 / 157\n",
      "training loss = 7.699450969696045\n",
      "validation loss = 7.720768777947677\n",
      "Batch 48 / 157\n",
      "training loss = 7.520261764526367\n",
      "validation loss = 7.71185124547858\n",
      "Batch 49 / 157\n",
      "training loss = 7.695681095123291\n",
      "validation loss = 7.704008554157458\n",
      "Batch 50 / 157\n",
      "training loss = 7.8031511306762695\n",
      "validation loss = 7.714465216586464\n",
      "Batch 51 / 157\n",
      "training loss = 8.02503776550293\n",
      "validation loss = 7.698615099254408\n",
      "Batch 52 / 157\n",
      "training loss = 7.509018421173096\n",
      "validation loss = 7.693903220327277\n",
      "Batch 53 / 157\n",
      "training loss = 7.693923473358154\n",
      "validation loss = 7.701248721072548\n",
      "Batch 54 / 157\n",
      "training loss = 7.488045692443848\n",
      "validation loss = 7.6769334140576815\n",
      "Batch 55 / 157\n",
      "training loss = 7.847225666046143\n",
      "validation loss = 7.680706049266615\n",
      "Batch 56 / 157\n",
      "training loss = 7.6101765632629395\n",
      "validation loss = 7.673492506930702\n",
      "Batch 57 / 157\n",
      "training loss = 7.989312171936035\n",
      "validation loss = 7.651045698868601\n",
      "Batch 58 / 157\n",
      "training loss = 7.826809883117676\n",
      "validation loss = 7.635451542703729\n",
      "Batch 59 / 157\n",
      "training loss = 7.4990763664245605\n",
      "validation loss = 7.632923753638017\n",
      "Batch 60 / 157\n",
      "training loss = 7.338535785675049\n",
      "validation loss = 7.636924994619269\n",
      "Batch 61 / 157\n",
      "training loss = 7.972464084625244\n",
      "validation loss = 7.613008197985198\n",
      "Batch 62 / 157\n",
      "training loss = 7.616041660308838\n",
      "validation loss = 7.633543817620528\n",
      "Batch 63 / 157\n",
      "training loss = 7.707397937774658\n",
      "validation loss = 7.600793311470433\n",
      "Batch 64 / 157\n",
      "training loss = 7.901081562042236\n",
      "validation loss = 7.609339864630448\n",
      "Batch 65 / 157\n",
      "training loss = 7.657965183258057\n",
      "validation loss = 7.616768284847862\n",
      "Batch 66 / 157\n",
      "training loss = 7.738992691040039\n",
      "validation loss = 7.612866853412829\n",
      "Batch 67 / 157\n",
      "training loss = 7.425059795379639\n",
      "validation loss = 7.603271409084923\n",
      "Batch 68 / 157\n",
      "training loss = 7.269506454467773\n",
      "validation loss = 7.609124359331633\n",
      "Batch 69 / 157\n",
      "training loss = 7.569363594055176\n",
      "validation loss = 7.603827802758468\n",
      "Batch 70 / 157\n",
      "training loss = 7.194485187530518\n",
      "validation loss = 7.601935662721333\n",
      "Batch 71 / 157\n",
      "training loss = 7.459888935089111\n",
      "validation loss = 7.605421794088263\n",
      "Batch 72 / 157\n",
      "training loss = 7.462983131408691\n",
      "validation loss = 7.589866713473671\n",
      "Batch 73 / 157\n",
      "training loss = 7.788271427154541\n",
      "validation loss = 7.592385317149915\n",
      "Batch 74 / 157\n",
      "training loss = 7.757233142852783\n",
      "validation loss = 7.602346595964934\n",
      "Batch 75 / 157\n",
      "training loss = 7.2710652351379395\n",
      "validation loss = 7.600950968892951\n",
      "Batch 76 / 157\n",
      "training loss = 7.149502277374268\n",
      "validation loss = 7.634083145543149\n",
      "Batch 77 / 157\n",
      "training loss = 7.688863754272461\n",
      "validation loss = 7.616160568438079\n",
      "Batch 78 / 157\n",
      "training loss = 7.594489574432373\n",
      "validation loss = 7.580694474672017\n",
      "Batch 79 / 157\n",
      "training loss = 7.752048492431641\n",
      "validation loss = 7.600421303196957\n",
      "Batch 80 / 157\n",
      "training loss = 7.7967848777771\n",
      "validation loss = 7.646580394945647\n",
      "Batch 81 / 157\n",
      "training loss = 7.3325629234313965\n",
      "validation loss = 7.607937210484555\n",
      "Batch 82 / 157\n",
      "training loss = 7.873228549957275\n",
      "validation loss = 7.599120918073152\n",
      "Batch 83 / 157\n",
      "training loss = 7.630615234375\n",
      "validation loss = 7.597272722344649\n",
      "Batch 84 / 157\n",
      "training loss = 7.717301368713379\n",
      "validation loss = 7.57829277138961\n",
      "Batch 85 / 157\n",
      "training loss = 7.262912750244141\n",
      "validation loss = 7.573999179036994\n",
      "Batch 86 / 157\n",
      "training loss = 7.451972007751465\n",
      "validation loss = 7.5640060525191455\n",
      "Batch 87 / 157\n",
      "training loss = 7.436091899871826\n",
      "validation loss = 7.5650921620820695\n",
      "Batch 88 / 157\n",
      "training loss = 7.676210403442383\n",
      "validation loss = 7.554045526604903\n",
      "Batch 89 / 157\n",
      "training loss = 7.599057674407959\n",
      "validation loss = 7.548269723591051\n",
      "Batch 90 / 157\n",
      "training loss = 7.391352653503418\n",
      "validation loss = 7.549520191393401\n",
      "Batch 91 / 157\n",
      "training loss = 8.069552421569824\n",
      "validation loss = 7.54937724063271\n",
      "Batch 92 / 157\n",
      "training loss = 7.225638389587402\n",
      "validation loss = 7.53916577288979\n",
      "Batch 93 / 157\n",
      "training loss = 7.58791971206665\n",
      "validation loss = 7.533156520441959\n",
      "Batch 94 / 157\n",
      "training loss = 7.516595363616943\n",
      "validation loss = 7.535079780377839\n",
      "Batch 95 / 157\n",
      "training loss = 7.281433582305908\n",
      "validation loss = 7.539766412032278\n",
      "Batch 96 / 157\n",
      "training loss = 7.771112442016602\n",
      "validation loss = 7.5273870417946265\n",
      "Batch 97 / 157\n",
      "training loss = 7.5754499435424805\n",
      "validation loss = 7.520787088494552\n",
      "Batch 98 / 157\n",
      "training loss = 7.6160783767700195\n",
      "validation loss = 7.524442296279104\n",
      "Batch 99 / 157\n",
      "training loss = 7.685112953186035\n",
      "validation loss = 7.5120038986206055\n",
      "Batch 100 / 157\n",
      "training loss = 7.632439136505127\n",
      "validation loss = 7.51620142083419\n",
      "Batch 101 / 157\n",
      "training loss = 7.173147201538086\n",
      "validation loss = 7.525580707349275\n",
      "Batch 102 / 157\n",
      "training loss = 7.380426406860352\n",
      "validation loss = 7.530829253949617\n",
      "Batch 103 / 157\n",
      "training loss = 7.493203163146973\n",
      "validation loss = 7.512656186756335\n",
      "Batch 104 / 157\n",
      "training loss = 7.4515557289123535\n",
      "validation loss = 7.506392830296567\n",
      "Batch 105 / 157\n",
      "training loss = 7.263645648956299\n",
      "validation loss = 7.515949951974969\n",
      "Batch 106 / 157\n",
      "training loss = 7.3405256271362305\n",
      "validation loss = 7.514655113220215\n",
      "Batch 107 / 157\n",
      "training loss = 7.561697483062744\n",
      "validation loss = 7.5158692912051555\n",
      "Batch 108 / 157\n",
      "training loss = 7.641214370727539\n",
      "validation loss = 7.524331870831941\n",
      "Batch 109 / 157\n",
      "training loss = 7.673432350158691\n",
      "validation loss = 7.512570481551321\n",
      "Batch 110 / 157\n",
      "training loss = 7.070180892944336\n",
      "validation loss = 7.5113043785095215\n",
      "Batch 111 / 157\n",
      "training loss = 7.010000228881836\n",
      "validation loss = 7.505941742344906\n",
      "Batch 112 / 157\n",
      "training loss = 7.301698684692383\n",
      "validation loss = 7.51172023070486\n",
      "Batch 113 / 157\n",
      "training loss = 7.67454195022583\n",
      "validation loss = 7.508589468504253\n",
      "Batch 114 / 157\n",
      "training loss = 7.772349834442139\n",
      "validation loss = 7.505610340519955\n",
      "Batch 115 / 157\n",
      "training loss = 7.350854873657227\n",
      "validation loss = 7.4923764279014184\n",
      "Batch 116 / 157\n",
      "training loss = 7.861321449279785\n",
      "validation loss = 7.499297995316355\n",
      "Batch 117 / 157\n",
      "training loss = 7.947747707366943\n",
      "validation loss = 7.511232074938323\n",
      "Batch 118 / 157\n",
      "training loss = 7.442521572113037\n",
      "validation loss = 7.510462861312063\n",
      "Batch 119 / 157\n",
      "training loss = 7.335719108581543\n",
      "validation loss = 7.486587649897525\n",
      "Batch 120 / 157\n",
      "training loss = 7.506377220153809\n",
      "validation loss = 7.479552193691856\n",
      "Batch 121 / 157\n",
      "training loss = 7.4165120124816895\n",
      "validation loss = 7.469600627296849\n",
      "Batch 122 / 157\n",
      "training loss = 7.302155017852783\n",
      "validation loss = 7.466409783614309\n",
      "Batch 123 / 157\n",
      "training loss = 7.151777744293213\n",
      "validation loss = 7.449930366716887\n",
      "Batch 124 / 157\n",
      "training loss = 7.595492362976074\n",
      "validation loss = 7.442895864185534\n",
      "Batch 125 / 157\n",
      "training loss = 6.9547295570373535\n",
      "validation loss = 7.44717530200356\n",
      "Batch 126 / 157\n",
      "training loss = 7.515620231628418\n",
      "validation loss = 7.446296541314376\n",
      "Batch 127 / 157\n",
      "training loss = 7.2458977699279785\n",
      "validation loss = 7.439305280384264\n",
      "Batch 128 / 157\n",
      "training loss = 7.3296990394592285\n",
      "validation loss = 7.438856074684544\n",
      "Batch 129 / 157\n",
      "training loss = 7.026788234710693\n",
      "validation loss = 7.434223727176064\n",
      "Batch 130 / 157\n",
      "training loss = 7.560090065002441\n",
      "validation loss = 7.430169732947099\n",
      "Batch 131 / 157\n",
      "training loss = 7.444094657897949\n",
      "validation loss = 7.437865207069798\n",
      "Batch 132 / 157\n",
      "training loss = 7.061130046844482\n",
      "validation loss = 7.4370878119217725\n",
      "Batch 133 / 157\n",
      "training loss = 7.720325946807861\n",
      "validation loss = 7.430984722940545\n",
      "Batch 134 / 157\n",
      "training loss = 7.682046890258789\n",
      "validation loss = 7.44229238911679\n",
      "Batch 135 / 157\n",
      "training loss = 7.191402912139893\n",
      "validation loss = 7.428459543930857\n",
      "Batch 136 / 157\n",
      "training loss = 7.517434597015381\n",
      "validation loss = 7.423477298335025\n",
      "Batch 137 / 157\n",
      "training loss = 7.270814895629883\n",
      "validation loss = 7.424673080444336\n",
      "Batch 138 / 157\n",
      "training loss = 7.113514423370361\n",
      "validation loss = 7.42323310751664\n",
      "Batch 139 / 157\n",
      "training loss = 7.703071117401123\n",
      "validation loss = 7.420390681216591\n",
      "Batch 140 / 157\n",
      "training loss = 7.135174751281738\n",
      "validation loss = 7.434262350985878\n",
      "Batch 141 / 157\n",
      "training loss = 7.248233795166016\n",
      "validation loss = 7.401870551862214\n",
      "Batch 142 / 157\n",
      "training loss = 7.212752819061279\n",
      "validation loss = 7.4196548210947135\n",
      "Batch 143 / 157\n",
      "training loss = 7.584900379180908\n",
      "validation loss = 7.403640671780235\n",
      "Batch 144 / 157\n",
      "training loss = 7.3478593826293945\n",
      "validation loss = 7.409952841306987\n",
      "Batch 145 / 157\n",
      "training loss = 7.349880218505859\n",
      "validation loss = 7.410591426648591\n",
      "Batch 146 / 157\n",
      "training loss = 7.513737201690674\n",
      "validation loss = 7.402766227722168\n",
      "Batch 147 / 157\n",
      "training loss = 7.3817138671875\n",
      "validation loss = 7.403783798217773\n",
      "Batch 148 / 157\n",
      "training loss = 7.113396167755127\n",
      "validation loss = 7.411253678171258\n",
      "Batch 149 / 157\n",
      "training loss = 7.5284833908081055\n",
      "validation loss = 7.401428247752943\n",
      "Batch 150 / 157\n",
      "training loss = 7.566869735717773\n",
      "validation loss = 7.403839889325593\n",
      "Batch 151 / 157\n",
      "training loss = 7.111705780029297\n",
      "validation loss = 7.412718346244411\n",
      "Batch 152 / 157\n",
      "training loss = 6.988677024841309\n",
      "validation loss = 7.408322509966399\n",
      "Batch 153 / 157\n",
      "training loss = 7.221912860870361\n",
      "validation loss = 7.409523813348067\n",
      "Batch 154 / 157\n",
      "training loss = 7.134181499481201\n",
      "validation loss = 7.408344569959138\n",
      "Batch 155 / 157\n",
      "training loss = 7.7381086349487305\n",
      "validation loss = 7.3967287665919255\n",
      "Batch 156 / 157\n",
      "training loss = 7.284971237182617\n",
      "validation loss = 7.390184151498895\n",
      "Batch 157 / 157\n",
      "training loss = 7.559567451477051\n",
      "validation loss = 7.4002988212986995\n",
      "Average training loss: 7.69\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.385917663574219\n",
      "validation loss = 10.1768025348061\n",
      "Batch 2 / 157\n",
      "training loss = 10.161497116088867\n",
      "validation loss = 9.703312371906481\n",
      "Batch 3 / 157\n",
      "training loss = 9.711869239807129\n",
      "validation loss = 9.305637660779452\n",
      "Batch 4 / 157\n",
      "training loss = 9.28116512298584\n",
      "validation loss = 8.942648134733501\n",
      "Batch 5 / 157\n",
      "training loss = 8.945579528808594\n",
      "validation loss = 8.611051358674702\n",
      "Batch 6 / 157\n",
      "training loss = 8.60505485534668\n",
      "validation loss = 8.345494169937936\n",
      "Batch 7 / 157\n",
      "training loss = 8.402335166931152\n",
      "validation loss = 8.148137318460565\n",
      "Batch 8 / 157\n",
      "training loss = 8.334999084472656\n",
      "validation loss = 8.020287087089137\n",
      "Batch 9 / 157\n",
      "training loss = 8.068938255310059\n",
      "validation loss = 7.957504222267552\n",
      "Batch 10 / 157\n",
      "training loss = 7.926982402801514\n",
      "validation loss = 7.931353142386989\n",
      "Batch 11 / 157\n",
      "training loss = 7.7980546951293945\n",
      "validation loss = 7.930034010033858\n",
      "Batch 12 / 157\n",
      "training loss = 7.59769868850708\n",
      "validation loss = 7.938004117262991\n",
      "Batch 13 / 157\n",
      "training loss = 8.141429901123047\n",
      "validation loss = 7.9198041213186166\n",
      "Batch 14 / 157\n",
      "training loss = 8.135093688964844\n",
      "validation loss = 7.860848301335385\n",
      "Batch 15 / 157\n",
      "training loss = 7.986424446105957\n",
      "validation loss = 7.812510113967092\n",
      "Batch 16 / 157\n",
      "training loss = 7.942381858825684\n",
      "validation loss = 7.869545484844007\n",
      "Batch 17 / 157\n",
      "training loss = 7.94725227355957\n",
      "validation loss = 7.826092193001195\n",
      "Batch 18 / 157\n",
      "training loss = 7.975039005279541\n",
      "validation loss = 7.785976660879035\n",
      "Batch 19 / 157\n",
      "training loss = 7.776095867156982\n",
      "validation loss = 7.789024076963726\n",
      "Batch 20 / 157\n",
      "training loss = 7.924439430236816\n",
      "validation loss = 7.796698319284539\n",
      "Batch 21 / 157\n",
      "training loss = 7.882059574127197\n",
      "validation loss = 7.789854099876003\n",
      "Batch 22 / 157\n",
      "training loss = 7.65301513671875\n",
      "validation loss = 7.773689797050075\n",
      "Batch 23 / 157\n",
      "training loss = 7.70510721206665\n",
      "validation loss = 7.760325080470035\n",
      "Batch 24 / 157\n",
      "training loss = 8.205941200256348\n",
      "validation loss = 7.765607206444991\n",
      "Batch 25 / 157\n",
      "training loss = 7.664799690246582\n",
      "validation loss = 7.777226573542545\n",
      "Batch 26 / 157\n",
      "training loss = 7.780686855316162\n",
      "validation loss = 7.760903283169395\n",
      "Batch 27 / 157\n",
      "training loss = 7.889254093170166\n",
      "validation loss = 7.745647330033152\n",
      "Batch 28 / 157\n",
      "training loss = 7.812327861785889\n",
      "validation loss = 7.728850690942061\n",
      "Batch 29 / 157\n",
      "training loss = 7.796905040740967\n",
      "validation loss = 7.720414236972206\n",
      "Batch 30 / 157\n",
      "training loss = 7.517257213592529\n",
      "validation loss = 7.718011479628713\n",
      "Batch 31 / 157\n",
      "training loss = 8.001802444458008\n",
      "validation loss = 7.706369048670719\n",
      "Batch 32 / 157\n",
      "training loss = 7.53389310836792\n",
      "validation loss = 7.682411545201352\n",
      "Batch 33 / 157\n",
      "training loss = 7.646849632263184\n",
      "validation loss = 7.674793469278436\n",
      "Batch 34 / 157\n",
      "training loss = 7.894773483276367\n",
      "validation loss = 7.677940996069657\n",
      "Batch 35 / 157\n",
      "training loss = 7.781800270080566\n",
      "validation loss = 7.64996526115819\n",
      "Batch 36 / 157\n",
      "training loss = 7.671154022216797\n",
      "validation loss = 7.627771001113088\n",
      "Batch 37 / 157\n",
      "training loss = 7.9204936027526855\n",
      "validation loss = 7.6144610455161645\n",
      "Batch 38 / 157\n",
      "training loss = 7.365695476531982\n",
      "validation loss = 7.610740736911171\n",
      "Batch 39 / 157\n",
      "training loss = 7.864134311676025\n",
      "validation loss = 7.597346381137245\n",
      "Batch 40 / 157\n",
      "training loss = 7.666491985321045\n",
      "validation loss = 7.582183260666697\n",
      "Batch 41 / 157\n",
      "training loss = 7.566000461578369\n",
      "validation loss = 7.581441904369154\n",
      "Batch 42 / 157\n",
      "training loss = 7.841965675354004\n",
      "validation loss = 7.5864605150724715\n",
      "Batch 43 / 157\n",
      "training loss = 7.693113803863525\n",
      "validation loss = 7.580444737484581\n",
      "Batch 44 / 157\n",
      "training loss = 7.726906776428223\n",
      "validation loss = 7.559595308805767\n",
      "Batch 45 / 157\n",
      "training loss = 7.517128944396973\n",
      "validation loss = 7.563103274295204\n",
      "Batch 46 / 157\n",
      "training loss = 7.762144088745117\n",
      "validation loss = 7.569164426703202\n",
      "Batch 47 / 157\n",
      "training loss = 7.623001575469971\n",
      "validation loss = 7.557244300842285\n",
      "Batch 48 / 157\n",
      "training loss = 7.5466179847717285\n",
      "validation loss = 7.54649167311819\n",
      "Batch 49 / 157\n",
      "training loss = 7.6302313804626465\n",
      "validation loss = 7.54834350786711\n",
      "Batch 50 / 157\n",
      "training loss = 7.791281223297119\n",
      "validation loss = 7.547012253811485\n",
      "Batch 51 / 157\n",
      "training loss = 7.453816890716553\n",
      "validation loss = 7.538990924232884\n",
      "Batch 52 / 157\n",
      "training loss = 7.635543346405029\n",
      "validation loss = 7.535071548662688\n",
      "Batch 53 / 157\n",
      "training loss = 7.822021961212158\n",
      "validation loss = 7.529212650499846\n",
      "Batch 54 / 157\n",
      "training loss = 7.5638508796691895\n",
      "validation loss = 7.530362982498972\n",
      "Batch 55 / 157\n",
      "training loss = 7.392904281616211\n",
      "validation loss = 7.52524283057765\n",
      "Batch 56 / 157\n",
      "training loss = 7.569810390472412\n",
      "validation loss = 7.520030272634406\n",
      "Batch 57 / 157\n",
      "training loss = 7.573195934295654\n",
      "validation loss = 7.52160486422087\n",
      "Batch 58 / 157\n",
      "training loss = 7.561232566833496\n",
      "validation loss = 7.517760954405132\n",
      "Batch 59 / 157\n",
      "training loss = 7.401754379272461\n",
      "validation loss = 7.508682803103798\n",
      "Batch 60 / 157\n",
      "training loss = 7.391878128051758\n",
      "validation loss = 7.506581381747597\n",
      "Batch 61 / 157\n",
      "training loss = 7.499762535095215\n",
      "validation loss = 7.5061343845568205\n",
      "Batch 62 / 157\n",
      "training loss = 7.599634170532227\n",
      "validation loss = 7.504184823287161\n",
      "Batch 63 / 157\n",
      "training loss = 7.0185394287109375\n",
      "validation loss = 7.498954421595523\n",
      "Batch 64 / 157\n",
      "training loss = 7.479923725128174\n",
      "validation loss = 7.496021320945339\n",
      "Batch 65 / 157\n",
      "training loss = 7.570059776306152\n",
      "validation loss = 7.493586038288317\n",
      "Batch 66 / 157\n",
      "training loss = 7.6388325691223145\n",
      "validation loss = 7.488398627230995\n",
      "Batch 67 / 157\n",
      "training loss = 7.715458393096924\n",
      "validation loss = 7.486599445343018\n",
      "Batch 68 / 157\n",
      "training loss = 7.442881107330322\n",
      "validation loss = 7.485112792567203\n",
      "Batch 69 / 157\n",
      "training loss = 7.579160690307617\n",
      "validation loss = 7.4877425243980005\n",
      "Batch 70 / 157\n",
      "training loss = 7.60609245300293\n",
      "validation loss = 7.483558479108308\n",
      "Batch 71 / 157\n",
      "training loss = 7.406962871551514\n",
      "validation loss = 7.477119395607396\n",
      "Batch 72 / 157\n",
      "training loss = 7.3538289070129395\n",
      "validation loss = 7.469097112354479\n",
      "Batch 73 / 157\n",
      "training loss = 7.3736772537231445\n",
      "validation loss = 7.467826617391486\n",
      "Batch 74 / 157\n",
      "training loss = 7.801983833312988\n",
      "validation loss = 7.471106253172222\n",
      "Batch 75 / 157\n",
      "training loss = 7.570062160491943\n",
      "validation loss = 7.463942251707378\n",
      "Batch 76 / 157\n",
      "training loss = 7.326972484588623\n",
      "validation loss = 7.457963642321135\n",
      "Batch 77 / 157\n",
      "training loss = 7.597047328948975\n",
      "validation loss = 7.457076474239952\n",
      "Batch 78 / 157\n",
      "training loss = 7.704991340637207\n",
      "validation loss = 7.453309535980225\n",
      "Batch 79 / 157\n",
      "training loss = 7.635231971740723\n",
      "validation loss = 7.46365657605623\n",
      "Batch 80 / 157\n",
      "training loss = 7.373779773712158\n",
      "validation loss = 7.44156917772795\n",
      "Batch 81 / 157\n",
      "training loss = 7.473762035369873\n",
      "validation loss = 7.447386415381181\n",
      "Batch 82 / 157\n",
      "training loss = 7.415946006774902\n",
      "validation loss = 7.4463350396407275\n",
      "Batch 83 / 157\n",
      "training loss = 7.380832195281982\n",
      "validation loss = 7.4378426702399\n",
      "Batch 84 / 157\n",
      "training loss = 7.254702568054199\n",
      "validation loss = 7.436037239275481\n",
      "Batch 85 / 157\n",
      "training loss = 7.438858985900879\n",
      "validation loss = 7.431547215110378\n",
      "Batch 86 / 157\n",
      "training loss = 7.431631565093994\n",
      "validation loss = 7.440895682887027\n",
      "Batch 87 / 157\n",
      "training loss = 7.4081621170043945\n",
      "validation loss = 7.434873329965692\n",
      "Batch 88 / 157\n",
      "training loss = 7.637822151184082\n",
      "validation loss = 7.4345497834055045\n",
      "Batch 89 / 157\n",
      "training loss = 7.486706733703613\n",
      "validation loss = 7.432515821958843\n",
      "Batch 90 / 157\n",
      "training loss = 7.35157585144043\n",
      "validation loss = 7.430423862055728\n",
      "Batch 91 / 157\n",
      "training loss = 7.84739875793457\n",
      "validation loss = 7.432671245775725\n",
      "Batch 92 / 157\n",
      "training loss = 7.302630424499512\n",
      "validation loss = 7.4263864818372225\n",
      "Batch 93 / 157\n",
      "training loss = 7.3252272605896\n",
      "validation loss = 7.429580061059249\n",
      "Batch 94 / 157\n",
      "training loss = 7.613808631896973\n",
      "validation loss = 7.419780179073936\n",
      "Batch 95 / 157\n",
      "training loss = 7.239890098571777\n",
      "validation loss = 7.416274974220677\n",
      "Batch 96 / 157\n",
      "training loss = 7.478005409240723\n",
      "validation loss = 7.418547655406751\n",
      "Batch 97 / 157\n",
      "training loss = 7.685505390167236\n",
      "validation loss = 7.418568109211169\n",
      "Batch 98 / 157\n",
      "training loss = 7.287164688110352\n",
      "validation loss = 7.404935134084601\n",
      "Batch 99 / 157\n",
      "training loss = 7.44007682800293\n",
      "validation loss = 7.4104962599904916\n",
      "Batch 100 / 157\n",
      "training loss = 7.604770660400391\n",
      "validation loss = 7.39827269002011\n",
      "Batch 101 / 157\n",
      "training loss = 7.446887493133545\n",
      "validation loss = 7.402469233462685\n",
      "Batch 102 / 157\n",
      "training loss = 7.396475315093994\n",
      "validation loss = 7.4082081694352\n",
      "Batch 103 / 157\n",
      "training loss = 7.316966533660889\n",
      "validation loss = 7.406762650138454\n",
      "Batch 104 / 157\n",
      "training loss = 7.4118499755859375\n",
      "validation loss = 7.3986516751741105\n",
      "Batch 105 / 157\n",
      "training loss = 7.442901134490967\n",
      "validation loss = 7.403248912409732\n",
      "Batch 106 / 157\n",
      "training loss = 7.5430755615234375\n",
      "validation loss = 7.393069493143182\n",
      "Batch 107 / 157\n",
      "training loss = 7.5432891845703125\n",
      "validation loss = 7.394039254439504\n",
      "Batch 108 / 157\n",
      "training loss = 7.706300258636475\n",
      "validation loss = 7.415062829067833\n",
      "Batch 109 / 157\n",
      "training loss = 7.19651460647583\n",
      "validation loss = 7.395107595544112\n",
      "Batch 110 / 157\n",
      "training loss = 7.327548503875732\n",
      "validation loss = 7.392055310701069\n",
      "Batch 111 / 157\n",
      "training loss = 7.435158729553223\n",
      "validation loss = 7.396697044372559\n",
      "Batch 112 / 157\n",
      "training loss = 7.39424467086792\n",
      "validation loss = 7.388978958129883\n",
      "Batch 113 / 157\n",
      "training loss = 7.5442070960998535\n",
      "validation loss = 7.385146216342323\n",
      "Batch 114 / 157\n",
      "training loss = 7.530214786529541\n",
      "validation loss = 7.389141183150442\n",
      "Batch 115 / 157\n",
      "training loss = 7.352982997894287\n",
      "validation loss = 7.38181179448178\n",
      "Batch 116 / 157\n",
      "training loss = 7.28411865234375\n",
      "validation loss = 7.374463984840794\n",
      "Batch 117 / 157\n",
      "training loss = 7.230436325073242\n",
      "validation loss = 7.377011299133301\n",
      "Batch 118 / 157\n",
      "training loss = 7.409985542297363\n",
      "validation loss = 7.381309860631039\n",
      "Batch 119 / 157\n",
      "training loss = 7.723733425140381\n",
      "validation loss = 7.380458731400339\n",
      "Batch 120 / 157\n",
      "training loss = 7.272148132324219\n",
      "validation loss = 7.380117240704988\n",
      "Batch 121 / 157\n",
      "training loss = 7.396271228790283\n",
      "validation loss = 7.3760097152308415\n",
      "Batch 122 / 157\n",
      "training loss = 7.476064682006836\n",
      "validation loss = 7.370228792491712\n",
      "Batch 123 / 157\n",
      "training loss = 7.565865516662598\n",
      "validation loss = 7.373191231175473\n",
      "Batch 124 / 157\n",
      "training loss = 6.779750347137451\n",
      "validation loss = 7.385259979649594\n",
      "Batch 125 / 157\n",
      "training loss = 7.4914655685424805\n",
      "validation loss = 7.382060377221358\n",
      "Batch 126 / 157\n",
      "training loss = 7.389414310455322\n",
      "validation loss = 7.375785928023489\n",
      "Batch 127 / 157\n",
      "training loss = 7.373254776000977\n",
      "validation loss = 7.371066620475368\n",
      "Batch 128 / 157\n",
      "training loss = 7.285782814025879\n",
      "validation loss = 7.374146637163665\n",
      "Batch 129 / 157\n",
      "training loss = 7.7644453048706055\n",
      "validation loss = 7.384996941215114\n",
      "Batch 130 / 157\n",
      "training loss = 7.384442329406738\n",
      "validation loss = 7.383091223867316\n",
      "Batch 131 / 157\n",
      "training loss = 7.035330772399902\n",
      "validation loss = 7.371513366699219\n",
      "Batch 132 / 157\n",
      "training loss = 7.3926520347595215\n",
      "validation loss = 7.371817312742534\n",
      "Batch 133 / 157\n",
      "training loss = 7.3554301261901855\n",
      "validation loss = 7.3714635999579174\n",
      "Batch 134 / 157\n",
      "training loss = 7.360879898071289\n",
      "validation loss = 7.364522933959961\n",
      "Batch 135 / 157\n",
      "training loss = 7.32768440246582\n",
      "validation loss = 7.362659102992008\n",
      "Batch 136 / 157\n",
      "training loss = 7.4133734703063965\n",
      "validation loss = 7.363004358191239\n",
      "Batch 137 / 157\n",
      "training loss = 7.495071887969971\n",
      "validation loss = 7.3654810503909465\n",
      "Batch 138 / 157\n",
      "training loss = 7.46688175201416\n",
      "validation loss = 7.357234051353053\n",
      "Batch 139 / 157\n",
      "training loss = 7.4686198234558105\n",
      "validation loss = 7.356927972090872\n",
      "Batch 140 / 157\n",
      "training loss = 7.645955562591553\n",
      "validation loss = 7.358204967097232\n",
      "Batch 141 / 157\n",
      "training loss = 7.554585933685303\n",
      "validation loss = 7.356949906600149\n",
      "Batch 142 / 157\n",
      "training loss = 7.597177028656006\n",
      "validation loss = 7.344804161473324\n",
      "Batch 143 / 157\n",
      "training loss = 7.492823600769043\n",
      "validation loss = 7.348900544015985\n",
      "Batch 144 / 157\n",
      "training loss = 7.197881698608398\n",
      "validation loss = 7.342775821685791\n",
      "Batch 145 / 157\n",
      "training loss = 7.247602462768555\n",
      "validation loss = 7.329474122900712\n",
      "Batch 146 / 157\n",
      "training loss = 7.431420803070068\n",
      "validation loss = 7.334324661054109\n",
      "Batch 147 / 157\n",
      "training loss = 7.36826753616333\n",
      "validation loss = 7.331846036409077\n",
      "Batch 148 / 157\n",
      "training loss = 7.1180100440979\n",
      "validation loss = 7.331913621802079\n",
      "Batch 149 / 157\n",
      "training loss = 7.447643756866455\n",
      "validation loss = 7.327160383525648\n",
      "Batch 150 / 157\n",
      "training loss = 7.240718364715576\n",
      "validation loss = 7.324152143378007\n",
      "Batch 151 / 157\n",
      "training loss = 7.602637767791748\n",
      "validation loss = 7.321053203783538\n",
      "Batch 152 / 157\n",
      "training loss = 7.402307033538818\n",
      "validation loss = 7.321948904740183\n",
      "Batch 153 / 157\n",
      "training loss = 7.513851642608643\n",
      "validation loss = 7.321790971254048\n",
      "Batch 154 / 157\n",
      "training loss = 7.479213714599609\n",
      "validation loss = 7.331731946844804\n",
      "Batch 155 / 157\n",
      "training loss = 7.415778636932373\n",
      "validation loss = 7.327303359383031\n",
      "Batch 156 / 157\n",
      "training loss = 7.3823323249816895\n",
      "validation loss = 7.32130241394043\n",
      "Batch 157 / 157\n",
      "training loss = 7.407059192657471\n",
      "validation loss = 7.316515395515843\n",
      "Average training loss: 7.63\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.382857322692871\n",
      "validation loss = 10.174123764038086\n",
      "Batch 2 / 157\n",
      "training loss = 10.142450332641602\n",
      "validation loss = 9.63904461107756\n",
      "Batch 3 / 157\n",
      "training loss = 9.627851486206055\n",
      "validation loss = 9.22172330555163\n",
      "Batch 4 / 157\n",
      "training loss = 9.181922912597656\n",
      "validation loss = 8.859120820697985\n",
      "Batch 5 / 157\n",
      "training loss = 8.858731269836426\n",
      "validation loss = 8.551738538240132\n",
      "Batch 6 / 157\n",
      "training loss = 8.69764232635498\n",
      "validation loss = 8.302619883888646\n",
      "Batch 7 / 157\n",
      "training loss = 8.277824401855469\n",
      "validation loss = 8.10775691584537\n",
      "Batch 8 / 157\n",
      "training loss = 8.069522857666016\n",
      "validation loss = 7.971809286820261\n",
      "Batch 9 / 157\n",
      "training loss = 7.8551530838012695\n",
      "validation loss = 7.87671244771857\n",
      "Batch 10 / 157\n",
      "training loss = 7.813732147216797\n",
      "validation loss = 7.814078255703575\n",
      "Batch 11 / 157\n",
      "training loss = 7.5389509201049805\n",
      "validation loss = 7.7882709754140755\n",
      "Batch 12 / 157\n",
      "training loss = 7.698929786682129\n",
      "validation loss = 7.787713477486058\n",
      "Batch 13 / 157\n",
      "training loss = 7.697242736816406\n",
      "validation loss = 7.789611490149247\n",
      "Batch 14 / 157\n",
      "training loss = 8.043941497802734\n",
      "validation loss = 7.7564441279361125\n",
      "Batch 15 / 157\n",
      "training loss = 7.787769794464111\n",
      "validation loss = 7.724774561430278\n",
      "Batch 16 / 157\n",
      "training loss = 7.638836860656738\n",
      "validation loss = 7.743759305853593\n",
      "Batch 17 / 157\n",
      "training loss = 7.771703720092773\n",
      "validation loss = 7.717116255509226\n",
      "Batch 18 / 157\n",
      "training loss = 7.765877723693848\n",
      "validation loss = 7.705257691835103\n",
      "Batch 19 / 157\n",
      "training loss = 7.585051536560059\n",
      "validation loss = 7.711340728558992\n",
      "Batch 20 / 157\n",
      "training loss = 8.016254425048828\n",
      "validation loss = 7.6856913566589355\n",
      "Batch 21 / 157\n",
      "training loss = 7.529785633087158\n",
      "validation loss = 7.656618971573679\n",
      "Batch 22 / 157\n",
      "training loss = 7.691784858703613\n",
      "validation loss = 7.654287313160143\n",
      "Batch 23 / 157\n",
      "training loss = 7.557394027709961\n",
      "validation loss = 7.628597736358643\n",
      "Batch 24 / 157\n",
      "training loss = 7.5875630378723145\n",
      "validation loss = 7.607706069946289\n",
      "Batch 25 / 157\n",
      "training loss = 7.795711994171143\n",
      "validation loss = 7.590712773172479\n",
      "Batch 26 / 157\n",
      "training loss = 7.49454927444458\n",
      "validation loss = 7.591841471822638\n",
      "Batch 27 / 157\n",
      "training loss = 7.487975597381592\n",
      "validation loss = 7.59271072086535\n",
      "Batch 28 / 157\n",
      "training loss = 7.761984825134277\n",
      "validation loss = 7.604363667337518\n",
      "Batch 29 / 157\n",
      "training loss = 7.751739978790283\n",
      "validation loss = 7.580700723748458\n",
      "Batch 30 / 157\n",
      "training loss = 7.5353851318359375\n",
      "validation loss = 7.541066897542853\n",
      "Batch 31 / 157\n",
      "training loss = 7.259841442108154\n",
      "validation loss = 7.541481168646562\n",
      "Batch 32 / 157\n",
      "training loss = 7.496885299682617\n",
      "validation loss = 7.55740529612491\n",
      "Batch 33 / 157\n",
      "training loss = 7.360683441162109\n",
      "validation loss = 7.5385385814465975\n",
      "Batch 34 / 157\n",
      "training loss = 7.502943515777588\n",
      "validation loss = 7.516566853774221\n",
      "Batch 35 / 157\n",
      "training loss = 7.687063217163086\n",
      "validation loss = 7.504291961067601\n",
      "Batch 36 / 157\n",
      "training loss = 7.567436695098877\n",
      "validation loss = 7.504467913978978\n",
      "Batch 37 / 157\n",
      "training loss = 7.936999320983887\n",
      "validation loss = 7.495348428425036\n",
      "Batch 38 / 157\n",
      "training loss = 7.319868087768555\n",
      "validation loss = 7.474213675448769\n",
      "Batch 39 / 157\n",
      "training loss = 7.2964067459106445\n",
      "validation loss = 7.465725396808825\n",
      "Batch 40 / 157\n",
      "training loss = 7.590837478637695\n",
      "validation loss = 7.467866395649157\n",
      "Batch 41 / 157\n",
      "training loss = 7.043466091156006\n",
      "validation loss = 7.469437523892052\n",
      "Batch 42 / 157\n",
      "training loss = 7.330663204193115\n",
      "validation loss = 7.487754696293881\n",
      "Batch 43 / 157\n",
      "training loss = 7.499360084533691\n",
      "validation loss = 7.4856160314459546\n",
      "Batch 44 / 157\n",
      "training loss = 7.4828572273254395\n",
      "validation loss = 7.462788180301064\n",
      "Batch 45 / 157\n",
      "training loss = 7.434085369110107\n",
      "validation loss = 7.447344478807952\n",
      "Batch 46 / 157\n",
      "training loss = 7.654317378997803\n",
      "validation loss = 7.44373103191978\n",
      "Batch 47 / 157\n",
      "training loss = 7.342649459838867\n",
      "validation loss = 7.440918771844161\n",
      "Batch 48 / 157\n",
      "training loss = 7.359884738922119\n",
      "validation loss = 7.446781484704268\n",
      "Batch 49 / 157\n",
      "training loss = 7.496643543243408\n",
      "validation loss = 7.446306554894698\n",
      "Batch 50 / 157\n",
      "training loss = 7.426498889923096\n",
      "validation loss = 7.438139840176231\n",
      "Batch 51 / 157\n",
      "training loss = 7.517446517944336\n",
      "validation loss = 7.431679324099892\n",
      "Batch 52 / 157\n",
      "training loss = 7.440302848815918\n",
      "validation loss = 7.42253273411801\n",
      "Batch 53 / 157\n",
      "training loss = 7.65777587890625\n",
      "validation loss = 7.4142734878941585\n",
      "Batch 54 / 157\n",
      "training loss = 7.243816375732422\n",
      "validation loss = 7.4134120439228255\n",
      "Batch 55 / 157\n",
      "training loss = 7.4422149658203125\n",
      "validation loss = 7.401913391916375\n",
      "Batch 56 / 157\n",
      "training loss = 7.4296183586120605\n",
      "validation loss = 7.397332166370592\n",
      "Batch 57 / 157\n",
      "training loss = 7.20193338394165\n",
      "validation loss = 7.396500612560072\n",
      "Batch 58 / 157\n",
      "training loss = 7.565764427185059\n",
      "validation loss = 7.380782252863834\n",
      "Batch 59 / 157\n",
      "training loss = 7.267917633056641\n",
      "validation loss = 7.3842802800630265\n",
      "Batch 60 / 157\n",
      "training loss = 7.512945175170898\n",
      "validation loss = 7.3787022138896745\n",
      "Batch 61 / 157\n",
      "training loss = 7.297639846801758\n",
      "validation loss = 7.371177020825837\n",
      "Batch 62 / 157\n",
      "training loss = 7.290855884552002\n",
      "validation loss = 7.371621608734131\n",
      "Batch 63 / 157\n",
      "training loss = 7.296217918395996\n",
      "validation loss = 7.367637508793881\n",
      "Batch 64 / 157\n",
      "training loss = 7.5362162590026855\n",
      "validation loss = 7.367170283668919\n",
      "Batch 65 / 157\n",
      "training loss = 7.551421165466309\n",
      "validation loss = 7.382040199480559\n",
      "Batch 66 / 157\n",
      "training loss = 7.233757972717285\n",
      "validation loss = 7.389008973774157\n",
      "Batch 67 / 157\n",
      "training loss = 7.306821346282959\n",
      "validation loss = 7.36134840312757\n",
      "Batch 68 / 157\n",
      "training loss = 7.350379943847656\n",
      "validation loss = 7.360697821566933\n",
      "Batch 69 / 157\n",
      "training loss = 7.195359230041504\n",
      "validation loss = 7.363778842122931\n",
      "Batch 70 / 157\n",
      "training loss = 7.465597629547119\n",
      "validation loss = 7.352748619882684\n",
      "Batch 71 / 157\n",
      "training loss = 7.638015270233154\n",
      "validation loss = 7.3512835251657584\n",
      "Batch 72 / 157\n",
      "training loss = 7.289951324462891\n",
      "validation loss = 7.357550646129408\n",
      "Batch 73 / 157\n",
      "training loss = 7.287015438079834\n",
      "validation loss = 7.3464836070412085\n",
      "Batch 74 / 157\n",
      "training loss = 7.490004062652588\n",
      "validation loss = 7.340682958301745\n",
      "Batch 75 / 157\n",
      "training loss = 7.40486478805542\n",
      "validation loss = 7.354717856959293\n",
      "Batch 76 / 157\n",
      "training loss = 7.32406759262085\n",
      "validation loss = 7.34295654296875\n",
      "Batch 77 / 157\n",
      "training loss = 7.188227653503418\n",
      "validation loss = 7.331531348981355\n",
      "Batch 78 / 157\n",
      "training loss = 7.224393367767334\n",
      "validation loss = 7.333706454226845\n",
      "Batch 79 / 157\n",
      "training loss = 7.330202102661133\n",
      "validation loss = 7.335761772958856\n",
      "Batch 80 / 157\n",
      "training loss = 7.240047931671143\n",
      "validation loss = 7.332051804191188\n",
      "Batch 81 / 157\n",
      "training loss = 7.589688777923584\n",
      "validation loss = 7.315944646534167\n",
      "Batch 82 / 157\n",
      "training loss = 7.420010089874268\n",
      "validation loss = 7.3196483160320085\n",
      "Batch 83 / 157\n",
      "training loss = 7.232354164123535\n",
      "validation loss = 7.315535369672273\n",
      "Batch 84 / 157\n",
      "training loss = 7.245772838592529\n",
      "validation loss = 7.308464426743357\n",
      "Batch 85 / 157\n",
      "training loss = 7.251218318939209\n",
      "validation loss = 7.298820646185624\n",
      "Batch 86 / 157\n",
      "training loss = 7.30154275894165\n",
      "validation loss = 7.292210980465538\n",
      "Batch 87 / 157\n",
      "training loss = 7.412056922912598\n",
      "validation loss = 7.307972406086169\n",
      "Batch 88 / 157\n",
      "training loss = 7.236037254333496\n",
      "validation loss = 7.3054737291838\n",
      "Batch 89 / 157\n",
      "training loss = 7.2630391120910645\n",
      "validation loss = 7.2933425652353385\n",
      "Batch 90 / 157\n",
      "training loss = 7.521119594573975\n",
      "validation loss = 7.298201786844354\n",
      "Batch 91 / 157\n",
      "training loss = 7.35700798034668\n",
      "validation loss = 7.29376408928319\n",
      "Batch 92 / 157\n",
      "training loss = 7.279247283935547\n",
      "validation loss = 7.291774548982319\n",
      "Batch 93 / 157\n",
      "training loss = 7.463362693786621\n",
      "validation loss = 7.301147184873882\n",
      "Batch 94 / 157\n",
      "training loss = 7.489023685455322\n",
      "validation loss = 7.291986063907021\n",
      "Batch 95 / 157\n",
      "training loss = 7.431049823760986\n",
      "validation loss = 7.297191695163124\n",
      "Batch 96 / 157\n",
      "training loss = 7.252310752868652\n",
      "validation loss = 7.292545042539897\n",
      "Batch 97 / 157\n",
      "training loss = 7.447537422180176\n",
      "validation loss = 7.281340975510447\n",
      "Batch 98 / 157\n",
      "training loss = 7.143111705780029\n",
      "validation loss = 7.2803780656111865\n",
      "Batch 99 / 157\n",
      "training loss = 6.6696014404296875\n",
      "validation loss = 7.282520118512605\n",
      "Batch 100 / 157\n",
      "training loss = 7.458484649658203\n",
      "validation loss = 7.28968472229807\n",
      "Batch 101 / 157\n",
      "training loss = 7.131688594818115\n",
      "validation loss = 7.2828370646426555\n",
      "Batch 102 / 157\n",
      "training loss = 7.245970249176025\n",
      "validation loss = 7.278577252438194\n",
      "Batch 103 / 157\n",
      "training loss = 7.288549900054932\n",
      "validation loss = 7.264567776730186\n",
      "Batch 104 / 157\n",
      "training loss = 7.214117050170898\n",
      "validation loss = 7.268777395549574\n",
      "Batch 105 / 157\n",
      "training loss = 7.204401969909668\n",
      "validation loss = 7.273440536699797\n",
      "Batch 106 / 157\n",
      "training loss = 7.315370082855225\n",
      "validation loss = 7.259796569221898\n",
      "Batch 107 / 157\n",
      "training loss = 7.040185451507568\n",
      "validation loss = 7.2541584968566895\n",
      "Batch 108 / 157\n",
      "training loss = 7.306179046630859\n",
      "validation loss = 7.255980667315032\n",
      "Batch 109 / 157\n",
      "training loss = 7.260091781616211\n",
      "validation loss = 7.261812561436703\n",
      "Batch 110 / 157\n",
      "training loss = 7.235098361968994\n",
      "validation loss = 7.254523653733103\n",
      "Batch 111 / 157\n",
      "training loss = 7.743222713470459\n",
      "validation loss = 7.250716811732242\n",
      "Batch 112 / 157\n",
      "training loss = 6.964865207672119\n",
      "validation loss = 7.240802463732268\n",
      "Batch 113 / 157\n",
      "training loss = 7.303089618682861\n",
      "validation loss = 7.24136011224044\n",
      "Batch 114 / 157\n",
      "training loss = 7.420107364654541\n",
      "validation loss = 7.236541748046875\n",
      "Batch 115 / 157\n",
      "training loss = 6.987156391143799\n",
      "validation loss = 7.235813718093069\n",
      "Batch 116 / 157\n",
      "training loss = 7.407706260681152\n",
      "validation loss = 7.250119058709395\n",
      "Batch 117 / 157\n",
      "training loss = 6.8033061027526855\n",
      "validation loss = 7.240774907563862\n",
      "Batch 118 / 157\n",
      "training loss = 7.3055243492126465\n",
      "validation loss = 7.229210276352732\n",
      "Batch 119 / 157\n",
      "training loss = 7.339513778686523\n",
      "validation loss = 7.230567580775211\n",
      "Batch 120 / 157\n",
      "training loss = 7.223735809326172\n",
      "validation loss = 7.2326737203096085\n",
      "Batch 121 / 157\n",
      "training loss = 7.145656108856201\n",
      "validation loss = 7.233991924085115\n",
      "Batch 122 / 157\n",
      "training loss = 6.988101482391357\n",
      "validation loss = 7.235621602911698\n",
      "Batch 123 / 157\n",
      "training loss = 7.055730819702148\n",
      "validation loss = 7.236506989127712\n",
      "Batch 124 / 157\n",
      "training loss = 7.326951026916504\n",
      "validation loss = 7.229323362049303\n",
      "Batch 125 / 157\n",
      "training loss = 7.124045372009277\n",
      "validation loss = 7.222016635694001\n",
      "Batch 126 / 157\n",
      "training loss = 7.1455230712890625\n",
      "validation loss = 7.216208959880628\n",
      "Batch 127 / 157\n",
      "training loss = 7.509499549865723\n",
      "validation loss = 7.229336537812886\n",
      "Batch 128 / 157\n",
      "training loss = 7.325079441070557\n",
      "validation loss = 7.238277861946507\n",
      "Batch 129 / 157\n",
      "training loss = 7.565485954284668\n",
      "validation loss = 7.2377072384482934\n",
      "Batch 130 / 157\n",
      "training loss = 7.2672882080078125\n",
      "validation loss = 7.222307556553891\n",
      "Batch 131 / 157\n",
      "training loss = 6.999269008636475\n",
      "validation loss = 7.217465701856111\n",
      "Batch 132 / 157\n",
      "training loss = 7.206323623657227\n",
      "validation loss = 7.217512005253842\n",
      "Batch 133 / 157\n",
      "training loss = 7.097602844238281\n",
      "validation loss = 7.2199374500073885\n",
      "Batch 134 / 157\n",
      "training loss = 7.1730475425720215\n",
      "validation loss = 7.20562031394557\n",
      "Batch 135 / 157\n",
      "training loss = 6.693942546844482\n",
      "validation loss = 7.206193923950195\n",
      "Batch 136 / 157\n",
      "training loss = 7.1934285163879395\n",
      "validation loss = 7.207769845661364\n",
      "Batch 137 / 157\n",
      "training loss = 7.102959632873535\n",
      "validation loss = 7.19097395947105\n",
      "Batch 138 / 157\n",
      "training loss = 7.1573686599731445\n",
      "validation loss = 7.201455818979364\n",
      "Batch 139 / 157\n",
      "training loss = 7.217979907989502\n",
      "validation loss = 7.199210367704692\n",
      "Batch 140 / 157\n",
      "training loss = 7.231074810028076\n",
      "validation loss = 7.188053683230751\n",
      "Batch 141 / 157\n",
      "training loss = 7.531877040863037\n",
      "validation loss = 7.192967364662572\n",
      "Batch 142 / 157\n",
      "training loss = 7.397975921630859\n",
      "validation loss = 7.1910534908897\n",
      "Batch 143 / 157\n",
      "training loss = 7.176210880279541\n",
      "validation loss = 7.195321710486161\n",
      "Batch 144 / 157\n",
      "training loss = 7.203145980834961\n",
      "validation loss = 7.198310952437551\n",
      "Batch 145 / 157\n",
      "training loss = 7.265926837921143\n",
      "validation loss = 7.195008729633532\n",
      "Batch 146 / 157\n",
      "training loss = 6.891590595245361\n",
      "validation loss = 7.187692165374756\n",
      "Batch 147 / 157\n",
      "training loss = 7.169571876525879\n",
      "validation loss = 7.183714690961335\n",
      "Batch 148 / 157\n",
      "training loss = 7.520334720611572\n",
      "validation loss = 7.186810192308928\n",
      "Batch 149 / 157\n",
      "training loss = 7.046804904937744\n",
      "validation loss = 7.194351221385755\n",
      "Batch 150 / 157\n",
      "training loss = 7.11785364151001\n",
      "validation loss = 7.194951182917545\n",
      "Batch 151 / 157\n",
      "training loss = 7.178153991699219\n",
      "validation loss = 7.182635257118626\n",
      "Batch 152 / 157\n",
      "training loss = 7.331539154052734\n",
      "validation loss = 7.182509020755165\n",
      "Batch 153 / 157\n",
      "training loss = 7.218053817749023\n",
      "validation loss = 7.191321548662688\n",
      "Batch 154 / 157\n",
      "training loss = 6.952772617340088\n",
      "validation loss = 7.189298027440121\n",
      "Batch 155 / 157\n",
      "training loss = 7.0975565910339355\n",
      "validation loss = 7.168725816827071\n",
      "Batch 156 / 157\n",
      "training loss = 7.200803279876709\n",
      "validation loss = 7.1662772078263135\n",
      "Batch 157 / 157\n",
      "training loss = 7.24651575088501\n",
      "validation loss = 7.173583407151072\n",
      "Average training loss: 7.45\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.3997802734375\n",
      "validation loss = 10.154601799814325\n",
      "Batch 2 / 157\n",
      "training loss = 10.137269020080566\n",
      "validation loss = 9.779247534902472\n",
      "Batch 3 / 157\n",
      "training loss = 9.852855682373047\n",
      "validation loss = 9.419855418958162\n",
      "Batch 4 / 157\n",
      "training loss = 9.53447437286377\n",
      "validation loss = 9.088866183632298\n",
      "Batch 5 / 157\n",
      "training loss = 9.12763500213623\n",
      "validation loss = 8.770683087800679\n",
      "Batch 6 / 157\n",
      "training loss = 8.789088249206543\n",
      "validation loss = 8.491975884688529\n",
      "Batch 7 / 157\n",
      "training loss = 8.305031776428223\n",
      "validation loss = 8.276447547109504\n",
      "Batch 8 / 157\n",
      "training loss = 8.19522762298584\n",
      "validation loss = 8.143989010861045\n",
      "Batch 9 / 157\n",
      "training loss = 8.023834228515625\n",
      "validation loss = 8.096410374892386\n",
      "Batch 10 / 157\n",
      "training loss = 7.796603679656982\n",
      "validation loss = 8.1240732042413\n",
      "Batch 11 / 157\n",
      "training loss = 7.761764049530029\n",
      "validation loss = 8.180492074866043\n",
      "Batch 12 / 157\n",
      "training loss = 8.173521995544434\n",
      "validation loss = 8.217955137553968\n",
      "Batch 13 / 157\n",
      "training loss = 7.889847278594971\n",
      "validation loss = 8.181727861103258\n",
      "Batch 14 / 157\n",
      "training loss = 7.923273086547852\n",
      "validation loss = 8.065671243165669\n",
      "Batch 15 / 157\n",
      "training loss = 8.161802291870117\n",
      "validation loss = 7.998287727958278\n",
      "Batch 16 / 157\n",
      "training loss = 7.698133945465088\n",
      "validation loss = 8.008823746129087\n",
      "Batch 17 / 157\n",
      "training loss = 7.937261581420898\n",
      "validation loss = 7.963738516757362\n",
      "Batch 18 / 157\n",
      "training loss = 8.097163200378418\n",
      "validation loss = 7.9451469371193335\n",
      "Batch 19 / 157\n",
      "training loss = 8.077173233032227\n",
      "validation loss = 7.933300871598093\n",
      "Batch 20 / 157\n",
      "training loss = 7.852593421936035\n",
      "validation loss = 7.9072013654206925\n",
      "Batch 21 / 157\n",
      "training loss = 7.78028678894043\n",
      "validation loss = 7.883905410766602\n",
      "Batch 22 / 157\n",
      "training loss = 8.065369606018066\n",
      "validation loss = 7.861708264601858\n",
      "Batch 23 / 157\n",
      "training loss = 7.857329845428467\n",
      "validation loss = 7.863911553433067\n",
      "Batch 24 / 157\n",
      "training loss = 7.520364284515381\n",
      "validation loss = 7.832175405401933\n",
      "Batch 25 / 157\n",
      "training loss = 7.980342864990234\n",
      "validation loss = 7.851890614158229\n",
      "Batch 26 / 157\n",
      "training loss = 8.113118171691895\n",
      "validation loss = 7.836346852151971\n",
      "Batch 27 / 157\n",
      "training loss = 7.891919136047363\n",
      "validation loss = 7.79829364073904\n",
      "Batch 28 / 157\n",
      "training loss = 7.983500003814697\n",
      "validation loss = 7.808609711496453\n",
      "Batch 29 / 157\n",
      "training loss = 7.772570610046387\n",
      "validation loss = 7.805321844000566\n",
      "Batch 30 / 157\n",
      "training loss = 7.609902381896973\n",
      "validation loss = 7.76730183551186\n",
      "Batch 31 / 157\n",
      "training loss = 7.8260650634765625\n",
      "validation loss = 7.7783711583990796\n",
      "Batch 32 / 157\n",
      "training loss = 7.590872287750244\n",
      "validation loss = 7.766770086790386\n",
      "Batch 33 / 157\n",
      "training loss = 8.047418594360352\n",
      "validation loss = 7.729903221130371\n",
      "Batch 34 / 157\n",
      "training loss = 7.969084739685059\n",
      "validation loss = 7.754076254995246\n",
      "Batch 35 / 157\n",
      "training loss = 7.389008045196533\n",
      "validation loss = 7.717450066616661\n",
      "Batch 36 / 157\n",
      "training loss = 7.440403938293457\n",
      "validation loss = 7.719458153373317\n",
      "Batch 37 / 157\n",
      "training loss = 7.843140125274658\n",
      "validation loss = 7.7103606023286515\n",
      "Batch 38 / 157\n",
      "training loss = 7.592970371246338\n",
      "validation loss = 7.6822867142526725\n",
      "Batch 39 / 157\n",
      "training loss = 7.569241523742676\n",
      "validation loss = 7.688942457500257\n",
      "Batch 40 / 157\n",
      "training loss = 7.726627826690674\n",
      "validation loss = 7.669519675405402\n",
      "Batch 41 / 157\n",
      "training loss = 7.807581424713135\n",
      "validation loss = 7.661957565106843\n",
      "Batch 42 / 157\n",
      "training loss = 7.441111087799072\n",
      "validation loss = 7.683948767812629\n",
      "Batch 43 / 157\n",
      "training loss = 7.956058025360107\n",
      "validation loss = 7.665130012913754\n",
      "Batch 44 / 157\n",
      "training loss = 8.151082038879395\n",
      "validation loss = 7.701852672978451\n",
      "Batch 45 / 157\n",
      "training loss = 7.636302471160889\n",
      "validation loss = 7.673124840385036\n",
      "Batch 46 / 157\n",
      "training loss = 7.612814426422119\n",
      "validation loss = 7.644836501071327\n",
      "Batch 47 / 157\n",
      "training loss = 7.752384185791016\n",
      "validation loss = 7.661071601666902\n",
      "Batch 48 / 157\n",
      "training loss = 7.482507705688477\n",
      "validation loss = 7.659411229585347\n",
      "Batch 49 / 157\n",
      "training loss = 7.801172733306885\n",
      "validation loss = 7.633959920782792\n",
      "Batch 50 / 157\n",
      "training loss = 7.342353343963623\n",
      "validation loss = 7.639899630295603\n",
      "Batch 51 / 157\n",
      "training loss = 7.97426700592041\n",
      "validation loss = 7.699242115020752\n",
      "Batch 52 / 157\n",
      "training loss = 7.731451511383057\n",
      "validation loss = 7.643979574504652\n",
      "Batch 53 / 157\n",
      "training loss = 7.64626407623291\n",
      "validation loss = 7.636628477196944\n",
      "Batch 54 / 157\n",
      "training loss = 7.603964328765869\n",
      "validation loss = 7.669864328284013\n",
      "Batch 55 / 157\n",
      "training loss = 7.735342025756836\n",
      "validation loss = 7.643733601821096\n",
      "Batch 56 / 157\n",
      "training loss = 7.480470657348633\n",
      "validation loss = 7.615091524626079\n",
      "Batch 57 / 157\n",
      "training loss = 7.457340717315674\n",
      "validation loss = 7.612388987290232\n",
      "Batch 58 / 157\n",
      "training loss = 7.69401741027832\n",
      "validation loss = 7.622575784984388\n",
      "Batch 59 / 157\n",
      "training loss = 7.745038032531738\n",
      "validation loss = 7.591777926997135\n",
      "Batch 60 / 157\n",
      "training loss = 7.315343379974365\n",
      "validation loss = 7.589756614283512\n",
      "Batch 61 / 157\n",
      "training loss = 7.4831461906433105\n",
      "validation loss = 7.598338252619693\n",
      "Batch 62 / 157\n",
      "training loss = 7.849822521209717\n",
      "validation loss = 7.573955937435753\n",
      "Batch 63 / 157\n",
      "training loss = 7.513062477111816\n",
      "validation loss = 7.576929192793997\n",
      "Batch 64 / 157\n",
      "training loss = 7.3723344802856445\n",
      "validation loss = 7.561184004733437\n",
      "Batch 65 / 157\n",
      "training loss = 7.542506694793701\n",
      "validation loss = 7.561893136877763\n",
      "Batch 66 / 157\n",
      "training loss = 7.604888916015625\n",
      "validation loss = 7.568529555672093\n",
      "Batch 67 / 157\n",
      "training loss = 7.52128791809082\n",
      "validation loss = 7.5576810585825065\n",
      "Batch 68 / 157\n",
      "training loss = 7.623498916625977\n",
      "validation loss = 7.555155779186048\n",
      "Batch 69 / 157\n",
      "training loss = 7.5698466300964355\n",
      "validation loss = 7.570337270435534\n",
      "Batch 70 / 157\n",
      "training loss = 7.240098476409912\n",
      "validation loss = 7.541406932630037\n",
      "Batch 71 / 157\n",
      "training loss = 7.343692302703857\n",
      "validation loss = 7.5539241088064095\n",
      "Batch 72 / 157\n",
      "training loss = 7.581422805786133\n",
      "validation loss = 7.544961753644441\n",
      "Batch 73 / 157\n",
      "training loss = 7.598316192626953\n",
      "validation loss = 7.541552719316985\n",
      "Batch 74 / 157\n",
      "training loss = 7.360248565673828\n",
      "validation loss = 7.538135704241301\n",
      "Batch 75 / 157\n",
      "training loss = 7.850233554840088\n",
      "validation loss = 7.523912103552568\n",
      "Batch 76 / 157\n",
      "training loss = 7.282384395599365\n",
      "validation loss = 7.508407417096589\n",
      "Batch 77 / 157\n",
      "training loss = 7.5138678550720215\n",
      "validation loss = 7.503089302464535\n",
      "Batch 78 / 157\n",
      "training loss = 7.12502908706665\n",
      "validation loss = 7.5168251740305045\n",
      "Batch 79 / 157\n",
      "training loss = 6.61876106262207\n",
      "validation loss = 7.540728769804302\n",
      "Batch 80 / 157\n",
      "training loss = 8.03002643585205\n",
      "validation loss = 7.5152992951242545\n",
      "Batch 81 / 157\n",
      "training loss = 8.008321762084961\n",
      "validation loss = 7.545911287006579\n",
      "Batch 82 / 157\n",
      "training loss = 7.512485504150391\n",
      "validation loss = 7.514068804289165\n",
      "Batch 83 / 157\n",
      "training loss = 7.241170406341553\n",
      "validation loss = 7.528533835160105\n",
      "Batch 84 / 157\n",
      "training loss = 7.779480457305908\n",
      "validation loss = 7.533947141546952\n",
      "Batch 85 / 157\n",
      "training loss = 7.457242488861084\n",
      "validation loss = 7.520011199148078\n",
      "Batch 86 / 157\n",
      "training loss = 7.425902366638184\n",
      "validation loss = 7.5234064302946395\n",
      "Batch 87 / 157\n",
      "training loss = 7.317873477935791\n",
      "validation loss = 7.505668815813567\n",
      "Batch 88 / 157\n",
      "training loss = 7.452487945556641\n",
      "validation loss = 7.490763337988603\n",
      "Batch 89 / 157\n",
      "training loss = 7.722573757171631\n",
      "validation loss = 7.481093231000398\n",
      "Batch 90 / 157\n",
      "training loss = 7.394556999206543\n",
      "validation loss = 7.4813437712819955\n",
      "Batch 91 / 157\n",
      "training loss = 7.087113380432129\n",
      "validation loss = 7.489932712755706\n",
      "Batch 92 / 157\n",
      "training loss = 7.46213436126709\n",
      "validation loss = 7.494810807077508\n",
      "Batch 93 / 157\n",
      "training loss = 7.469633102416992\n",
      "validation loss = 7.4923834800720215\n",
      "Batch 94 / 157\n",
      "training loss = 6.902987957000732\n",
      "validation loss = 7.477619873849969\n",
      "Batch 95 / 157\n",
      "training loss = 7.27296257019043\n",
      "validation loss = 7.46665620803833\n",
      "Batch 96 / 157\n",
      "training loss = 7.425687789916992\n",
      "validation loss = 7.459471928445916\n",
      "Batch 97 / 157\n",
      "training loss = 7.5958452224731445\n",
      "validation loss = 7.455388370313142\n",
      "Batch 98 / 157\n",
      "training loss = 7.393807411193848\n",
      "validation loss = 7.452423798410516\n",
      "Batch 99 / 157\n",
      "training loss = 7.067920207977295\n",
      "validation loss = 7.436427894391511\n",
      "Batch 100 / 157\n",
      "training loss = 7.6778106689453125\n",
      "validation loss = 7.441805864635267\n",
      "Batch 101 / 157\n",
      "training loss = 7.013017177581787\n",
      "validation loss = 7.43595928894846\n",
      "Batch 102 / 157\n",
      "training loss = 7.82914400100708\n",
      "validation loss = 7.433240815212852\n",
      "Batch 103 / 157\n",
      "training loss = 7.290468215942383\n",
      "validation loss = 7.422390611548173\n",
      "Batch 104 / 157\n",
      "training loss = 7.261127948760986\n",
      "validation loss = 7.427521755820827\n",
      "Batch 105 / 157\n",
      "training loss = 7.585139274597168\n",
      "validation loss = 7.430772931952226\n",
      "Batch 106 / 157\n",
      "training loss = 7.245514392852783\n",
      "validation loss = 7.4290056228637695\n",
      "Batch 107 / 157\n",
      "training loss = 7.513014793395996\n",
      "validation loss = 7.412446022033691\n",
      "Batch 108 / 157\n",
      "training loss = 7.498981952667236\n",
      "validation loss = 7.413854373128791\n",
      "Batch 109 / 157\n",
      "training loss = 7.451597213745117\n",
      "validation loss = 7.40694492741635\n",
      "Batch 110 / 157\n",
      "training loss = 7.217440128326416\n",
      "validation loss = 7.396578437403629\n",
      "Batch 111 / 157\n",
      "training loss = 7.387789249420166\n",
      "validation loss = 7.387538207204718\n",
      "Batch 112 / 157\n",
      "training loss = 7.615292072296143\n",
      "validation loss = 7.396098663932399\n",
      "Batch 113 / 157\n",
      "training loss = 7.269003391265869\n",
      "validation loss = 7.388294169777318\n",
      "Batch 114 / 157\n",
      "training loss = 7.6297478675842285\n",
      "validation loss = 7.3881963679665015\n",
      "Batch 115 / 157\n",
      "training loss = 7.0006303787231445\n",
      "validation loss = 7.374169324573717\n",
      "Batch 116 / 157\n",
      "training loss = 7.483587741851807\n",
      "validation loss = 7.37489622517636\n",
      "Batch 117 / 157\n",
      "training loss = 7.289326190948486\n",
      "validation loss = 7.365698563425164\n",
      "Batch 118 / 157\n",
      "training loss = 7.303738594055176\n",
      "validation loss = 7.371921413823178\n",
      "Batch 119 / 157\n",
      "training loss = 7.7953338623046875\n",
      "validation loss = 7.378248817042301\n",
      "Batch 120 / 157\n",
      "training loss = 7.569697380065918\n",
      "validation loss = 7.399461168991892\n",
      "Batch 121 / 157\n",
      "training loss = 6.994737148284912\n",
      "validation loss = 7.380814025276585\n",
      "Batch 122 / 157\n",
      "training loss = 7.675617694854736\n",
      "validation loss = 7.38148832321167\n",
      "Batch 123 / 157\n",
      "training loss = 7.752448558807373\n",
      "validation loss = 7.378995343258507\n",
      "Batch 124 / 157\n",
      "training loss = 7.672865390777588\n",
      "validation loss = 7.408414263474314\n",
      "Batch 125 / 157\n",
      "training loss = 7.072612285614014\n",
      "validation loss = 7.392953671907124\n",
      "Batch 126 / 157\n",
      "training loss = 7.580808162689209\n",
      "validation loss = 7.384622875012849\n",
      "Batch 127 / 157\n",
      "training loss = 7.236605167388916\n",
      "validation loss = 7.3916244757802865\n",
      "Batch 128 / 157\n",
      "training loss = 7.611880302429199\n",
      "validation loss = 7.373350921430085\n",
      "Batch 129 / 157\n",
      "training loss = 7.650702476501465\n",
      "validation loss = 7.36983743466829\n",
      "Batch 130 / 157\n",
      "training loss = 7.33401346206665\n",
      "validation loss = 7.388110712954872\n",
      "Batch 131 / 157\n",
      "training loss = 7.639835834503174\n",
      "validation loss = 7.370267165334601\n",
      "Batch 132 / 157\n",
      "training loss = 7.406228065490723\n",
      "validation loss = 7.371584691499409\n",
      "Batch 133 / 157\n",
      "training loss = 7.052298545837402\n",
      "validation loss = 7.386211395263672\n",
      "Batch 134 / 157\n",
      "training loss = 7.437772274017334\n",
      "validation loss = 7.393334865570068\n",
      "Batch 135 / 157\n",
      "training loss = 7.808310508728027\n",
      "validation loss = 7.372012815977397\n",
      "Batch 136 / 157\n",
      "training loss = 7.391359806060791\n",
      "validation loss = 7.382829666137695\n",
      "Batch 137 / 157\n",
      "training loss = 7.397450923919678\n",
      "validation loss = 7.364796086361534\n",
      "Batch 138 / 157\n",
      "training loss = 7.0942511558532715\n",
      "validation loss = 7.367626165088854\n",
      "Batch 139 / 157\n",
      "training loss = 7.465475082397461\n",
      "validation loss = 7.363833803879587\n",
      "Batch 140 / 157\n",
      "training loss = 7.2010817527771\n",
      "validation loss = 7.3520488488046745\n",
      "Batch 141 / 157\n",
      "training loss = 7.48689603805542\n",
      "validation loss = 7.347281205026727\n",
      "Batch 142 / 157\n",
      "training loss = 7.564632415771484\n",
      "validation loss = 7.346269632640638\n",
      "Batch 143 / 157\n",
      "training loss = 7.5667829513549805\n",
      "validation loss = 7.335353826221667\n",
      "Batch 144 / 157\n",
      "training loss = 6.996500015258789\n",
      "validation loss = 7.33157057511179\n",
      "Batch 145 / 157\n",
      "training loss = 7.473229885101318\n",
      "validation loss = 7.324113092924419\n",
      "Batch 146 / 157\n",
      "training loss = 7.222940444946289\n",
      "validation loss = 7.332426096263685\n",
      "Batch 147 / 157\n",
      "training loss = 7.340739727020264\n",
      "validation loss = 7.3173744051080005\n",
      "Batch 148 / 157\n",
      "training loss = 7.053740501403809\n",
      "validation loss = 7.3057758431685595\n",
      "Batch 149 / 157\n",
      "training loss = 7.2660698890686035\n",
      "validation loss = 7.318124971891704\n",
      "Batch 150 / 157\n",
      "training loss = 6.9936842918396\n",
      "validation loss = 7.315386420802066\n",
      "Batch 151 / 157\n",
      "training loss = 7.4625983238220215\n",
      "validation loss = 7.310972263938503\n",
      "Batch 152 / 157\n",
      "training loss = 7.477292537689209\n",
      "validation loss = 7.314862326571816\n",
      "Batch 153 / 157\n",
      "training loss = 7.497880935668945\n",
      "validation loss = 7.3341653472498844\n",
      "Batch 154 / 157\n",
      "training loss = 7.404707908630371\n",
      "validation loss = 7.3362655890615365\n",
      "Batch 155 / 157\n",
      "training loss = 7.219330787658691\n",
      "validation loss = 7.324319136770148\n",
      "Batch 156 / 157\n",
      "training loss = 7.0285139083862305\n",
      "validation loss = 7.352861655385871\n",
      "Batch 157 / 157\n",
      "training loss = 8.052273750305176\n",
      "validation loss = 7.3299765837819955\n",
      "Average training loss: 7.64\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.370092391967773\n",
      "validation loss = 10.156343911823473\n",
      "Batch 2 / 157\n",
      "training loss = 10.14321231842041\n",
      "validation loss = 9.661540533366956\n",
      "Batch 3 / 157\n",
      "training loss = 9.638588905334473\n",
      "validation loss = 9.24893876125938\n",
      "Batch 4 / 157\n",
      "training loss = 9.326635360717773\n",
      "validation loss = 8.884260378385845\n",
      "Batch 5 / 157\n",
      "training loss = 8.968324661254883\n",
      "validation loss = 8.555411188225998\n",
      "Batch 6 / 157\n",
      "training loss = 8.63992691040039\n",
      "validation loss = 8.299094400907817\n",
      "Batch 7 / 157\n",
      "training loss = 8.279338836669922\n",
      "validation loss = 8.115570419713071\n",
      "Batch 8 / 157\n",
      "training loss = 7.986969470977783\n",
      "validation loss = 8.00372412330226\n",
      "Batch 9 / 157\n",
      "training loss = 7.946986675262451\n",
      "validation loss = 7.943388336583188\n",
      "Batch 10 / 157\n",
      "training loss = 7.894825458526611\n",
      "validation loss = 7.9278038175482495\n",
      "Batch 11 / 157\n",
      "training loss = 7.951331615447998\n",
      "validation loss = 7.931599240554006\n",
      "Batch 12 / 157\n",
      "training loss = 7.778998851776123\n",
      "validation loss = 7.9415309554652165\n",
      "Batch 13 / 157\n",
      "training loss = 8.326067924499512\n",
      "validation loss = 7.909763511858489\n",
      "Batch 14 / 157\n",
      "training loss = 8.272518157958984\n",
      "validation loss = 7.846647965280633\n",
      "Batch 15 / 157\n",
      "training loss = 8.02784538269043\n",
      "validation loss = 7.85488203952187\n",
      "Batch 16 / 157\n",
      "training loss = 7.737720489501953\n",
      "validation loss = 7.843472355290463\n",
      "Batch 17 / 157\n",
      "training loss = 7.713437557220459\n",
      "validation loss = 7.806402607967979\n",
      "Batch 18 / 157\n",
      "training loss = 7.957607269287109\n",
      "validation loss = 7.800740467874627\n",
      "Batch 19 / 157\n",
      "training loss = 7.737737655639648\n",
      "validation loss = 7.8111467361450195\n",
      "Batch 20 / 157\n",
      "training loss = 8.100555419921875\n",
      "validation loss = 7.797352841025905\n",
      "Batch 21 / 157\n",
      "training loss = 7.950464248657227\n",
      "validation loss = 7.777903883080733\n",
      "Batch 22 / 157\n",
      "training loss = 7.675993919372559\n",
      "validation loss = 7.7645778154072005\n",
      "Batch 23 / 157\n",
      "training loss = 7.724311828613281\n",
      "validation loss = 7.769283144097579\n",
      "Batch 24 / 157\n",
      "training loss = 7.832892894744873\n",
      "validation loss = 7.763467387149208\n",
      "Batch 25 / 157\n",
      "training loss = 7.720297336578369\n",
      "validation loss = 7.747625752499229\n",
      "Batch 26 / 157\n",
      "training loss = 7.877530574798584\n",
      "validation loss = 7.736575327421489\n",
      "Batch 27 / 157\n",
      "training loss = 7.935525894165039\n",
      "validation loss = 7.724007882569966\n",
      "Batch 28 / 157\n",
      "training loss = 7.58671236038208\n",
      "validation loss = 7.720795832182231\n",
      "Batch 29 / 157\n",
      "training loss = 7.767827987670898\n",
      "validation loss = 7.718249923304508\n",
      "Batch 30 / 157\n",
      "training loss = 7.729597091674805\n",
      "validation loss = 7.707665368130333\n",
      "Batch 31 / 157\n",
      "training loss = 7.871279716491699\n",
      "validation loss = 7.7238744183590535\n",
      "Batch 32 / 157\n",
      "training loss = 7.5254106521606445\n",
      "validation loss = 7.687043114712364\n",
      "Batch 33 / 157\n",
      "training loss = 7.776610851287842\n",
      "validation loss = 7.695678936807733\n",
      "Batch 34 / 157\n",
      "training loss = 7.535130023956299\n",
      "validation loss = 7.69424275348061\n",
      "Batch 35 / 157\n",
      "training loss = 7.916692733764648\n",
      "validation loss = 7.674848054584704\n",
      "Batch 36 / 157\n",
      "training loss = 7.493475914001465\n",
      "validation loss = 7.6753615078173185\n",
      "Batch 37 / 157\n",
      "training loss = 7.504508018493652\n",
      "validation loss = 7.664798460508647\n",
      "Batch 38 / 157\n",
      "training loss = 7.619399070739746\n",
      "validation loss = 7.654421003241288\n",
      "Batch 39 / 157\n",
      "training loss = 7.667665004730225\n",
      "validation loss = 7.649061378679778\n",
      "Batch 40 / 157\n",
      "training loss = 7.657672882080078\n",
      "validation loss = 7.64028074866847\n",
      "Batch 41 / 157\n",
      "training loss = 7.6444573402404785\n",
      "validation loss = 7.637265205383301\n",
      "Batch 42 / 157\n",
      "training loss = 7.651121139526367\n",
      "validation loss = 7.631404399871826\n",
      "Batch 43 / 157\n",
      "training loss = 7.9048566818237305\n",
      "validation loss = 7.625462356366609\n",
      "Batch 44 / 157\n",
      "training loss = 7.313227653503418\n",
      "validation loss = 7.616977641457005\n",
      "Batch 45 / 157\n",
      "training loss = 7.478515625\n",
      "validation loss = 7.623243181329024\n",
      "Batch 46 / 157\n",
      "training loss = 7.549051284790039\n",
      "validation loss = 7.595381510885138\n",
      "Batch 47 / 157\n",
      "training loss = 7.48196268081665\n",
      "validation loss = 7.589523917750308\n",
      "Batch 48 / 157\n",
      "training loss = 7.797341823577881\n",
      "validation loss = 7.589964264317563\n",
      "Batch 49 / 157\n",
      "training loss = 7.530333042144775\n",
      "validation loss = 7.566244275946366\n",
      "Batch 50 / 157\n",
      "training loss = 7.867528438568115\n",
      "validation loss = 7.568438379388106\n",
      "Batch 51 / 157\n",
      "training loss = 7.691945552825928\n",
      "validation loss = 7.548811561182926\n",
      "Batch 52 / 157\n",
      "training loss = 7.362275123596191\n",
      "validation loss = 7.55513502422132\n",
      "Batch 53 / 157\n",
      "training loss = 7.640757083892822\n",
      "validation loss = 7.537082245475368\n",
      "Batch 54 / 157\n",
      "training loss = 7.441717147827148\n",
      "validation loss = 7.533179132561934\n",
      "Batch 55 / 157\n",
      "training loss = 7.453888416290283\n",
      "validation loss = 7.528143732171309\n",
      "Batch 56 / 157\n",
      "training loss = 7.808131217956543\n",
      "validation loss = 7.543756911629124\n",
      "Batch 57 / 157\n",
      "training loss = 7.562475204467773\n",
      "validation loss = 7.5392215628373\n",
      "Batch 58 / 157\n",
      "training loss = 7.546829700469971\n",
      "validation loss = 7.510168979042454\n",
      "Batch 59 / 157\n",
      "training loss = 7.306715965270996\n",
      "validation loss = 7.519283043710809\n",
      "Batch 60 / 157\n",
      "training loss = 7.659633159637451\n",
      "validation loss = 7.515539671245374\n",
      "Batch 61 / 157\n",
      "training loss = 7.733239650726318\n",
      "validation loss = 7.507861112293444\n",
      "Batch 62 / 157\n",
      "training loss = 7.481366157531738\n",
      "validation loss = 7.515440288342927\n",
      "Batch 63 / 157\n",
      "training loss = 7.351809024810791\n",
      "validation loss = 7.50044247978612\n",
      "Batch 64 / 157\n",
      "training loss = 7.683080196380615\n",
      "validation loss = 7.4915233160320085\n",
      "Batch 65 / 157\n",
      "training loss = 7.880040168762207\n",
      "validation loss = 7.488787651062012\n",
      "Batch 66 / 157\n",
      "training loss = 7.587362766265869\n",
      "validation loss = 7.484334519034938\n",
      "Batch 67 / 157\n",
      "training loss = 7.439179420471191\n",
      "validation loss = 7.481593633952894\n",
      "Batch 68 / 157\n",
      "training loss = 7.509256839752197\n",
      "validation loss = 7.475248537565532\n",
      "Batch 69 / 157\n",
      "training loss = 7.54536771774292\n",
      "validation loss = 7.475588949103105\n",
      "Batch 70 / 157\n",
      "training loss = 7.547913074493408\n",
      "validation loss = 7.473749035283139\n",
      "Batch 71 / 157\n",
      "training loss = 7.431468486785889\n",
      "validation loss = 7.4645583002190845\n",
      "Batch 72 / 157\n",
      "training loss = 7.739548206329346\n",
      "validation loss = 7.4691392246045565\n",
      "Batch 73 / 157\n",
      "training loss = 7.21198844909668\n",
      "validation loss = 7.4568337892231185\n",
      "Batch 74 / 157\n",
      "training loss = 7.5664567947387695\n",
      "validation loss = 7.447715558503804\n",
      "Batch 75 / 157\n",
      "training loss = 7.73484468460083\n",
      "validation loss = 7.454974199596204\n",
      "Batch 76 / 157\n",
      "training loss = 7.388883590698242\n",
      "validation loss = 7.463596093027215\n",
      "Batch 77 / 157\n",
      "training loss = 7.396312236785889\n",
      "validation loss = 7.457477946030466\n",
      "Batch 78 / 157\n",
      "training loss = 7.582756519317627\n",
      "validation loss = 7.45096869217722\n",
      "Batch 79 / 157\n",
      "training loss = 7.531411170959473\n",
      "validation loss = 7.441546992251747\n",
      "Batch 80 / 157\n",
      "training loss = 7.606630325317383\n",
      "validation loss = 7.441095327076159\n",
      "Batch 81 / 157\n",
      "training loss = 7.436534404754639\n",
      "validation loss = 7.4430343728316455\n",
      "Batch 82 / 157\n",
      "training loss = 7.184737682342529\n",
      "validation loss = 7.446684009150455\n",
      "Batch 83 / 157\n",
      "training loss = 7.380253314971924\n",
      "validation loss = 7.4404324230394865\n",
      "Batch 84 / 157\n",
      "training loss = 7.554518222808838\n",
      "validation loss = 7.444178204787405\n",
      "Batch 85 / 157\n",
      "training loss = 7.396625995635986\n",
      "validation loss = 7.444561883022911\n",
      "Batch 86 / 157\n",
      "training loss = 7.545883655548096\n",
      "validation loss = 7.440404565710771\n",
      "Batch 87 / 157\n",
      "training loss = 7.54542875289917\n",
      "validation loss = 7.432966759330348\n",
      "Batch 88 / 157\n",
      "training loss = 7.360711574554443\n",
      "validation loss = 7.426802710482948\n",
      "Batch 89 / 157\n",
      "training loss = 7.686538219451904\n",
      "validation loss = 7.41871946736386\n",
      "Batch 90 / 157\n",
      "training loss = 7.370357036590576\n",
      "validation loss = 7.415139700237074\n",
      "Batch 91 / 157\n",
      "training loss = 7.467485427856445\n",
      "validation loss = 7.415648460388184\n",
      "Batch 92 / 157\n",
      "training loss = 7.593025207519531\n",
      "validation loss = 7.418058621255975\n",
      "Batch 93 / 157\n",
      "training loss = 7.504544258117676\n",
      "validation loss = 7.41206545578806\n",
      "Batch 94 / 157\n",
      "training loss = 7.295509338378906\n",
      "validation loss = 7.402724065278706\n",
      "Batch 95 / 157\n",
      "training loss = 7.527400970458984\n",
      "validation loss = 7.395160474275288\n",
      "Batch 96 / 157\n",
      "training loss = 7.3619704246521\n",
      "validation loss = 7.39608631635967\n",
      "Batch 97 / 157\n",
      "training loss = 7.5455193519592285\n",
      "validation loss = 7.392747201417622\n",
      "Batch 98 / 157\n",
      "training loss = 7.290924549102783\n",
      "validation loss = 7.386603330311022\n",
      "Batch 99 / 157\n",
      "training loss = 7.491095066070557\n",
      "validation loss = 7.3948843855606885\n",
      "Batch 100 / 157\n",
      "training loss = 7.43973445892334\n",
      "validation loss = 7.392651683405826\n",
      "Batch 101 / 157\n",
      "training loss = 7.487337112426758\n",
      "validation loss = 7.390657726087068\n",
      "Batch 102 / 157\n",
      "training loss = 7.279151439666748\n",
      "validation loss = 7.375436632256759\n",
      "Batch 103 / 157\n",
      "training loss = 7.413880348205566\n",
      "validation loss = 7.3802791896619295\n",
      "Batch 104 / 157\n",
      "training loss = 7.733396053314209\n",
      "validation loss = 7.3775226693404345\n",
      "Batch 105 / 157\n",
      "training loss = 7.374695301055908\n",
      "validation loss = 7.379828377773888\n",
      "Batch 106 / 157\n",
      "training loss = 7.412367820739746\n",
      "validation loss = 7.373286422930266\n",
      "Batch 107 / 157\n",
      "training loss = 7.544312477111816\n",
      "validation loss = 7.37430497219688\n",
      "Batch 108 / 157\n",
      "training loss = 7.418043613433838\n",
      "validation loss = 7.367456385963841\n",
      "Batch 109 / 157\n",
      "training loss = 7.314167499542236\n",
      "validation loss = 7.370533466339111\n",
      "Batch 110 / 157\n",
      "training loss = 7.493950843811035\n",
      "validation loss = 7.370352895636308\n",
      "Batch 111 / 157\n",
      "training loss = 7.443523406982422\n",
      "validation loss = 7.369625568389893\n",
      "Batch 112 / 157\n",
      "training loss = 7.471405506134033\n",
      "validation loss = 7.370135181828549\n",
      "Batch 113 / 157\n",
      "training loss = 7.065789222717285\n",
      "validation loss = 7.357846561231111\n",
      "Batch 114 / 157\n",
      "training loss = 7.553253650665283\n",
      "validation loss = 7.351970446737189\n",
      "Batch 115 / 157\n",
      "training loss = 7.214516639709473\n",
      "validation loss = 7.35514645827444\n",
      "Batch 116 / 157\n",
      "training loss = 7.172699928283691\n",
      "validation loss = 7.356627439197741\n",
      "Batch 117 / 157\n",
      "training loss = 7.477278232574463\n",
      "validation loss = 7.3578000570598405\n",
      "Batch 118 / 157\n",
      "training loss = 7.567898750305176\n",
      "validation loss = 7.359835423921284\n",
      "Batch 119 / 157\n",
      "training loss = 7.55709981918335\n",
      "validation loss = 7.340648349962737\n",
      "Batch 120 / 157\n",
      "training loss = 7.549056053161621\n",
      "validation loss = 7.332924014643619\n",
      "Batch 121 / 157\n",
      "training loss = 7.575375080108643\n",
      "validation loss = 7.3400572224667195\n",
      "Batch 122 / 157\n",
      "training loss = 7.413272857666016\n",
      "validation loss = 7.335479535554585\n",
      "Batch 123 / 157\n",
      "training loss = 7.361780166625977\n",
      "validation loss = 7.33953352978355\n",
      "Batch 124 / 157\n",
      "training loss = 7.538455963134766\n",
      "validation loss = 7.332206173946983\n",
      "Batch 125 / 157\n",
      "training loss = 7.402891159057617\n",
      "validation loss = 7.327774800752339\n",
      "Batch 126 / 157\n",
      "training loss = 7.51922082901001\n",
      "validation loss = 7.327795254556756\n",
      "Batch 127 / 157\n",
      "training loss = 7.276575088500977\n",
      "validation loss = 7.320550843289024\n",
      "Batch 128 / 157\n",
      "training loss = 7.590242862701416\n",
      "validation loss = 7.30580096495779\n",
      "Batch 129 / 157\n",
      "training loss = 7.6289777755737305\n",
      "validation loss = 7.309217779259932\n",
      "Batch 130 / 157\n",
      "training loss = 7.681643486022949\n",
      "validation loss = 7.314881098897834\n",
      "Batch 131 / 157\n",
      "training loss = 7.401366710662842\n",
      "validation loss = 7.304553006824694\n",
      "Batch 132 / 157\n",
      "training loss = 7.101403713226318\n",
      "validation loss = 7.3037520458823755\n",
      "Batch 133 / 157\n",
      "training loss = 7.353651523590088\n",
      "validation loss = 7.305169231013248\n",
      "Batch 134 / 157\n",
      "training loss = 7.226295471191406\n",
      "validation loss = 7.309452834882234\n",
      "Batch 135 / 157\n",
      "training loss = 7.119682788848877\n",
      "validation loss = 7.306722364927593\n",
      "Batch 136 / 157\n",
      "training loss = 7.448353290557861\n",
      "validation loss = 7.294089819255628\n",
      "Batch 137 / 157\n",
      "training loss = 7.217051029205322\n",
      "validation loss = 7.291242724970767\n",
      "Batch 138 / 157\n",
      "training loss = 7.317841529846191\n",
      "validation loss = 7.301738764110365\n",
      "Batch 139 / 157\n",
      "training loss = 7.218870162963867\n",
      "validation loss = 7.291682971151252\n",
      "Batch 140 / 157\n",
      "training loss = 7.408803939819336\n",
      "validation loss = 7.294264015398528\n",
      "Batch 141 / 157\n",
      "training loss = 7.49062967300415\n",
      "validation loss = 7.28390676096866\n",
      "Batch 142 / 157\n",
      "training loss = 7.022216320037842\n",
      "validation loss = 7.2914230447066455\n",
      "Batch 143 / 157\n",
      "training loss = 7.103265762329102\n",
      "validation loss = 7.287889731557746\n",
      "Batch 144 / 157\n",
      "training loss = 7.545196056365967\n",
      "validation loss = 7.283087404150712\n",
      "Batch 145 / 157\n",
      "training loss = 7.369060516357422\n",
      "validation loss = 7.275363093928287\n",
      "Batch 146 / 157\n",
      "training loss = 7.249683856964111\n",
      "validation loss = 7.274238385652241\n",
      "Batch 147 / 157\n",
      "training loss = 7.304832458496094\n",
      "validation loss = 7.276400415520919\n",
      "Batch 148 / 157\n",
      "training loss = 7.248499870300293\n",
      "validation loss = 7.266376997295179\n",
      "Batch 149 / 157\n",
      "training loss = 7.069025993347168\n",
      "validation loss = 7.266657427737587\n",
      "Batch 150 / 157\n",
      "training loss = 7.44651985168457\n",
      "validation loss = 7.264133478465833\n",
      "Batch 151 / 157\n",
      "training loss = 7.417983531951904\n",
      "validation loss = 7.262049599697716\n",
      "Batch 152 / 157\n",
      "training loss = 7.105547904968262\n",
      "validation loss = 7.2624158357319075\n",
      "Batch 153 / 157\n",
      "training loss = 7.635790824890137\n",
      "validation loss = 7.257996308176141\n",
      "Batch 154 / 157\n",
      "training loss = 7.1975860595703125\n",
      "validation loss = 7.255699233004921\n",
      "Batch 155 / 157\n",
      "training loss = 7.491472244262695\n",
      "validation loss = 7.260037723340486\n",
      "Batch 156 / 157\n",
      "training loss = 7.146787166595459\n",
      "validation loss = 7.252146670692845\n",
      "Batch 157 / 157\n",
      "training loss = 6.382641315460205\n",
      "validation loss = 7.255404422157689\n",
      "Average training loss: 7.61\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.382002830505371\n",
      "validation loss = 10.12857417056435\n",
      "Batch 2 / 157\n",
      "training loss = 10.114104270935059\n",
      "validation loss = 9.584630363865903\n",
      "Batch 3 / 157\n",
      "training loss = 9.576957702636719\n",
      "validation loss = 9.168747299595884\n",
      "Batch 4 / 157\n",
      "training loss = 9.101730346679688\n",
      "validation loss = 8.810380232961554\n",
      "Batch 5 / 157\n",
      "training loss = 8.847975730895996\n",
      "validation loss = 8.515034123470908\n",
      "Batch 6 / 157\n",
      "training loss = 8.476544380187988\n",
      "validation loss = 8.272293793527703\n",
      "Batch 7 / 157\n",
      "training loss = 8.197381019592285\n",
      "validation loss = 8.089600663436087\n",
      "Batch 8 / 157\n",
      "training loss = 8.118782997131348\n",
      "validation loss = 7.962657727693257\n",
      "Batch 9 / 157\n",
      "training loss = 7.792135238647461\n",
      "validation loss = 7.8767625156201815\n",
      "Batch 10 / 157\n",
      "training loss = 7.875847816467285\n",
      "validation loss = 7.820656625848067\n",
      "Batch 11 / 157\n",
      "training loss = 7.758372783660889\n",
      "validation loss = 7.783640008223684\n",
      "Batch 12 / 157\n",
      "training loss = 7.7740797996521\n",
      "validation loss = 7.770430213526676\n",
      "Batch 13 / 157\n",
      "training loss = 7.730823993682861\n",
      "validation loss = 7.757943529831736\n",
      "Batch 14 / 157\n",
      "training loss = 7.6462626457214355\n",
      "validation loss = 7.737819044213546\n",
      "Batch 15 / 157\n",
      "training loss = 7.723634719848633\n",
      "validation loss = 7.738321279224596\n",
      "Batch 16 / 157\n",
      "training loss = 7.488309383392334\n",
      "validation loss = 7.713102892825478\n",
      "Batch 17 / 157\n",
      "training loss = 7.61591911315918\n",
      "validation loss = 7.721061882219817\n",
      "Batch 18 / 157\n",
      "training loss = 7.737864017486572\n",
      "validation loss = 7.718957825710899\n",
      "Batch 19 / 157\n",
      "training loss = 7.804712295532227\n",
      "validation loss = 7.706193522403114\n",
      "Batch 20 / 157\n",
      "training loss = 7.956364631652832\n",
      "validation loss = 7.716066184796785\n",
      "Batch 21 / 157\n",
      "training loss = 7.632293224334717\n",
      "validation loss = 7.692219583611739\n",
      "Batch 22 / 157\n",
      "training loss = 7.615991592407227\n",
      "validation loss = 7.6839143853438525\n",
      "Batch 23 / 157\n",
      "training loss = 7.766831398010254\n",
      "validation loss = 7.6743387172096655\n",
      "Batch 24 / 157\n",
      "training loss = 7.463504314422607\n",
      "validation loss = 7.66711573851736\n",
      "Batch 25 / 157\n",
      "training loss = 7.547093868255615\n",
      "validation loss = 7.657626829649272\n",
      "Batch 26 / 157\n",
      "training loss = 7.533550262451172\n",
      "validation loss = 7.650631628538433\n",
      "Batch 27 / 157\n",
      "training loss = 7.6067938804626465\n",
      "validation loss = 7.641539222315738\n",
      "Batch 28 / 157\n",
      "training loss = 7.47029447555542\n",
      "validation loss = 7.6210548250298755\n",
      "Batch 29 / 157\n",
      "training loss = 7.584415912628174\n",
      "validation loss = 7.619246457752428\n",
      "Batch 30 / 157\n",
      "training loss = 7.596291542053223\n",
      "validation loss = 7.611802025845177\n",
      "Batch 31 / 157\n",
      "training loss = 7.601649284362793\n",
      "validation loss = 7.597710308275725\n",
      "Batch 32 / 157\n",
      "training loss = 7.344396114349365\n",
      "validation loss = 7.578773498535156\n",
      "Batch 33 / 157\n",
      "training loss = 7.5363335609436035\n",
      "validation loss = 7.563387594724956\n",
      "Batch 34 / 157\n",
      "training loss = 7.129952430725098\n",
      "validation loss = 7.557778935683401\n",
      "Batch 35 / 157\n",
      "training loss = 7.48311710357666\n",
      "validation loss = 7.547425671627647\n",
      "Batch 36 / 157\n",
      "training loss = 7.567971706390381\n",
      "validation loss = 7.552182297957571\n",
      "Batch 37 / 157\n",
      "training loss = 7.327134609222412\n",
      "validation loss = 7.533660185964484\n",
      "Batch 38 / 157\n",
      "training loss = 7.409602165222168\n",
      "validation loss = 7.5375160919992545\n",
      "Batch 39 / 157\n",
      "training loss = 7.6564483642578125\n",
      "validation loss = 7.513107977415386\n",
      "Batch 40 / 157\n",
      "training loss = 7.375118255615234\n",
      "validation loss = 7.508047505428917\n",
      "Batch 41 / 157\n",
      "training loss = 7.729948997497559\n",
      "validation loss = 7.513232532300447\n",
      "Batch 42 / 157\n",
      "training loss = 7.566249370574951\n",
      "validation loss = 7.501553535461426\n",
      "Batch 43 / 157\n",
      "training loss = 7.560734272003174\n",
      "validation loss = 7.4911580085754395\n",
      "Batch 44 / 157\n",
      "training loss = 7.3213582038879395\n",
      "validation loss = 7.488613153758802\n",
      "Batch 45 / 157\n",
      "training loss = 7.527595520019531\n",
      "validation loss = 7.485073566436768\n",
      "Batch 46 / 157\n",
      "training loss = 7.485058307647705\n",
      "validation loss = 7.470612751810174\n",
      "Batch 47 / 157\n",
      "training loss = 7.451884746551514\n",
      "validation loss = 7.467033486617239\n",
      "Batch 48 / 157\n",
      "training loss = 7.578450679779053\n",
      "validation loss = 7.4546685218811035\n",
      "Batch 49 / 157\n",
      "training loss = 7.6834940910339355\n",
      "validation loss = 7.443436145782471\n",
      "Batch 50 / 157\n",
      "training loss = 7.732565879821777\n",
      "validation loss = 7.447473249937358\n",
      "Batch 51 / 157\n",
      "training loss = 7.510955333709717\n",
      "validation loss = 7.455244967811986\n",
      "Batch 52 / 157\n",
      "training loss = 7.368924140930176\n",
      "validation loss = 7.446772826345343\n",
      "Batch 53 / 157\n",
      "training loss = 7.265300273895264\n",
      "validation loss = 7.436014275801809\n",
      "Batch 54 / 157\n",
      "training loss = 7.6442179679870605\n",
      "validation loss = 7.449163487083034\n",
      "Batch 55 / 157\n",
      "training loss = 7.5778117179870605\n",
      "validation loss = 7.444076713762786\n",
      "Batch 56 / 157\n",
      "training loss = 7.264727592468262\n",
      "validation loss = 7.443836739188747\n",
      "Batch 57 / 157\n",
      "training loss = 7.434439182281494\n",
      "validation loss = 7.433524859578986\n",
      "Batch 58 / 157\n",
      "training loss = 7.391144752502441\n",
      "validation loss = 7.425273167459588\n",
      "Batch 59 / 157\n",
      "training loss = 7.195373058319092\n",
      "validation loss = 7.420745749222605\n",
      "Batch 60 / 157\n",
      "training loss = 7.704314708709717\n",
      "validation loss = 7.421655253360146\n",
      "Batch 61 / 157\n",
      "training loss = 7.298813343048096\n",
      "validation loss = 7.414509547384162\n",
      "Batch 62 / 157\n",
      "training loss = 7.492959976196289\n",
      "validation loss = 7.399578847383198\n",
      "Batch 63 / 157\n",
      "training loss = 7.0496978759765625\n",
      "validation loss = 7.413706804576673\n",
      "Batch 64 / 157\n",
      "training loss = 7.351162433624268\n",
      "validation loss = 7.418534278869629\n",
      "Batch 65 / 157\n",
      "training loss = 7.288323879241943\n",
      "validation loss = 7.394782141635292\n",
      "Batch 66 / 157\n",
      "training loss = 7.392305850982666\n",
      "validation loss = 7.396281970174689\n",
      "Batch 67 / 157\n",
      "training loss = 7.406695365905762\n",
      "validation loss = 7.406088126333136\n",
      "Batch 68 / 157\n",
      "training loss = 7.329965114593506\n",
      "validation loss = 7.392521331184788\n",
      "Batch 69 / 157\n",
      "training loss = 7.274541854858398\n",
      "validation loss = 7.383071146513286\n",
      "Batch 70 / 157\n",
      "training loss = 7.339302062988281\n",
      "validation loss = 7.381588735078511\n",
      "Batch 71 / 157\n",
      "training loss = 7.492621898651123\n",
      "validation loss = 7.390844671349776\n",
      "Batch 72 / 157\n",
      "training loss = 7.324176788330078\n",
      "validation loss = 7.377039407428942\n",
      "Batch 73 / 157\n",
      "training loss = 7.487354278564453\n",
      "validation loss = 7.373103844492059\n",
      "Batch 74 / 157\n",
      "training loss = 7.349843978881836\n",
      "validation loss = 7.368902808741519\n",
      "Batch 75 / 157\n",
      "training loss = 7.404984474182129\n",
      "validation loss = 7.357956183584113\n",
      "Batch 76 / 157\n",
      "training loss = 7.994774341583252\n",
      "validation loss = 7.355429097225792\n",
      "Batch 77 / 157\n",
      "training loss = 7.333029270172119\n",
      "validation loss = 7.358030871341103\n",
      "Batch 78 / 157\n",
      "training loss = 7.429098129272461\n",
      "validation loss = 7.355391678057219\n",
      "Batch 79 / 157\n",
      "training loss = 7.298363208770752\n",
      "validation loss = 7.360673603258635\n",
      "Batch 80 / 157\n",
      "training loss = 7.442606449127197\n",
      "validation loss = 7.350576450950221\n",
      "Batch 81 / 157\n",
      "training loss = 7.220801830291748\n",
      "validation loss = 7.346942449870863\n",
      "Batch 82 / 157\n",
      "training loss = 7.120240688323975\n",
      "validation loss = 7.339616926092851\n",
      "Batch 83 / 157\n",
      "training loss = 7.405135154724121\n",
      "validation loss = 7.33009137605366\n",
      "Batch 84 / 157\n",
      "training loss = 7.405423641204834\n",
      "validation loss = 7.331686371251156\n",
      "Batch 85 / 157\n",
      "training loss = 7.3707356452941895\n",
      "validation loss = 7.327607556393272\n",
      "Batch 86 / 157\n",
      "training loss = 7.05650520324707\n",
      "validation loss = 7.323340039504202\n",
      "Batch 87 / 157\n",
      "training loss = 7.368401050567627\n",
      "validation loss = 7.318753342879446\n",
      "Batch 88 / 157\n",
      "training loss = 7.3783674240112305\n",
      "validation loss = 7.316843760640998\n",
      "Batch 89 / 157\n",
      "training loss = 7.184727191925049\n",
      "validation loss = 7.316065110658345\n",
      "Batch 90 / 157\n",
      "training loss = 7.420351505279541\n",
      "validation loss = 7.310544541007594\n",
      "Batch 91 / 157\n",
      "training loss = 7.340646266937256\n",
      "validation loss = 7.299490451812744\n",
      "Batch 92 / 157\n",
      "training loss = 7.202561855316162\n",
      "validation loss = 7.300244557230096\n",
      "Batch 93 / 157\n",
      "training loss = 7.360893249511719\n",
      "validation loss = 7.293795133891859\n",
      "Batch 94 / 157\n",
      "training loss = 7.365878582000732\n",
      "validation loss = 7.290776127263119\n",
      "Batch 95 / 157\n",
      "training loss = 7.265202522277832\n",
      "validation loss = 7.287964143251118\n",
      "Batch 96 / 157\n",
      "training loss = 7.3412346839904785\n",
      "validation loss = 7.294650378980134\n",
      "Batch 97 / 157\n",
      "training loss = 7.175267219543457\n",
      "validation loss = 7.283471885480378\n",
      "Batch 98 / 157\n",
      "training loss = 7.2992634773254395\n",
      "validation loss = 7.279696414345189\n",
      "Batch 99 / 157\n",
      "training loss = 7.206460952758789\n",
      "validation loss = 7.280111337962904\n",
      "Batch 100 / 157\n",
      "training loss = 7.243346214294434\n",
      "validation loss = 7.280499960246839\n",
      "Batch 101 / 157\n",
      "training loss = 7.3304362297058105\n",
      "validation loss = 7.273836938958419\n",
      "Batch 102 / 157\n",
      "training loss = 7.362114429473877\n",
      "validation loss = 7.277541737807424\n",
      "Batch 103 / 157\n",
      "training loss = 7.390462398529053\n",
      "validation loss = 7.274678606736033\n",
      "Batch 104 / 157\n",
      "training loss = 7.343778610229492\n",
      "validation loss = 7.2749552726745605\n",
      "Batch 105 / 157\n",
      "training loss = 7.0667338371276855\n",
      "validation loss = 7.258081084803531\n",
      "Batch 106 / 157\n",
      "training loss = 7.545920372009277\n",
      "validation loss = 7.253617964292827\n",
      "Batch 107 / 157\n",
      "training loss = 7.275777816772461\n",
      "validation loss = 7.252884162099738\n",
      "Batch 108 / 157\n",
      "training loss = 7.380804061889648\n",
      "validation loss = 7.254296829825954\n",
      "Batch 109 / 157\n",
      "training loss = 7.237934589385986\n",
      "validation loss = 7.252102651094136\n",
      "Batch 110 / 157\n",
      "training loss = 7.300374507904053\n",
      "validation loss = 7.244244801370721\n",
      "Batch 111 / 157\n",
      "training loss = 7.434924125671387\n",
      "validation loss = 7.246790609861675\n",
      "Batch 112 / 157\n",
      "training loss = 6.977288246154785\n",
      "validation loss = 7.246354303861919\n",
      "Batch 113 / 157\n",
      "training loss = 7.028576374053955\n",
      "validation loss = 7.244936165056731\n",
      "Batch 114 / 157\n",
      "training loss = 7.346573829650879\n",
      "validation loss = 7.235052861665425\n",
      "Batch 115 / 157\n",
      "training loss = 7.383338451385498\n",
      "validation loss = 7.247663196764495\n",
      "Batch 116 / 157\n",
      "training loss = 7.236419200897217\n",
      "validation loss = 7.239910778246428\n",
      "Batch 117 / 157\n",
      "training loss = 7.272800922393799\n",
      "validation loss = 7.222584172299034\n",
      "Batch 118 / 157\n",
      "training loss = 7.236495494842529\n",
      "validation loss = 7.244066087823165\n",
      "Batch 119 / 157\n",
      "training loss = 7.181807518005371\n",
      "validation loss = 7.239774001272101\n",
      "Batch 120 / 157\n",
      "training loss = 7.501635551452637\n",
      "validation loss = 7.217928786026804\n",
      "Batch 121 / 157\n",
      "training loss = 7.31342077255249\n",
      "validation loss = 7.225814568369012\n",
      "Batch 122 / 157\n",
      "training loss = 7.20004940032959\n",
      "validation loss = 7.227227913705926\n",
      "Batch 123 / 157\n",
      "training loss = 7.44976806640625\n",
      "validation loss = 7.225383683254845\n",
      "Batch 124 / 157\n",
      "training loss = 7.138704299926758\n",
      "validation loss = 7.2148185529206925\n",
      "Batch 125 / 157\n",
      "training loss = 7.202207565307617\n",
      "validation loss = 7.203991237439607\n",
      "Batch 126 / 157\n",
      "training loss = 7.163244724273682\n",
      "validation loss = 7.204095338520251\n",
      "Batch 127 / 157\n",
      "training loss = 7.01519250869751\n",
      "validation loss = 7.20538488187288\n",
      "Batch 128 / 157\n",
      "training loss = 6.969399452209473\n",
      "validation loss = 7.202670774961772\n",
      "Batch 129 / 157\n",
      "training loss = 7.399694442749023\n",
      "validation loss = 7.198186447745876\n",
      "Batch 130 / 157\n",
      "training loss = 6.781510353088379\n",
      "validation loss = 7.19952056282445\n",
      "Batch 131 / 157\n",
      "training loss = 7.190748691558838\n",
      "validation loss = 7.204301081205669\n",
      "Batch 132 / 157\n",
      "training loss = 7.179191589355469\n",
      "validation loss = 7.194159281881232\n",
      "Batch 133 / 157\n",
      "training loss = 6.907698631286621\n",
      "validation loss = 7.188029213955528\n",
      "Batch 134 / 157\n",
      "training loss = 7.307097434997559\n",
      "validation loss = 7.179745975293611\n",
      "Batch 135 / 157\n",
      "training loss = 7.290110111236572\n",
      "validation loss = 7.181473455931011\n",
      "Batch 136 / 157\n",
      "training loss = 7.171482563018799\n",
      "validation loss = 7.177844750253778\n",
      "Batch 137 / 157\n",
      "training loss = 7.1643242835998535\n",
      "validation loss = 7.175737481368215\n",
      "Batch 138 / 157\n",
      "training loss = 7.036900520324707\n",
      "validation loss = 7.165913004624216\n",
      "Batch 139 / 157\n",
      "training loss = 7.424228191375732\n",
      "validation loss = 7.169002834119294\n",
      "Batch 140 / 157\n",
      "training loss = 7.232243537902832\n",
      "validation loss = 7.171606314809699\n",
      "Batch 141 / 157\n",
      "training loss = 7.277657508850098\n",
      "validation loss = 7.177483332784552\n",
      "Batch 142 / 157\n",
      "training loss = 7.078092098236084\n",
      "validation loss = 7.170900294655247\n",
      "Batch 143 / 157\n",
      "training loss = 6.874814033508301\n",
      "validation loss = 7.177118953905608\n",
      "Batch 144 / 157\n",
      "training loss = 7.424033164978027\n",
      "validation loss = 7.1830911134418685\n",
      "Batch 145 / 157\n",
      "training loss = 7.087043762207031\n",
      "validation loss = 7.173558611618845\n",
      "Batch 146 / 157\n",
      "training loss = 7.050734519958496\n",
      "validation loss = 7.1660592179549365\n",
      "Batch 147 / 157\n",
      "training loss = 7.264256477355957\n",
      "validation loss = 7.167298618115876\n",
      "Batch 148 / 157\n",
      "training loss = 7.157439231872559\n",
      "validation loss = 7.174246612348054\n",
      "Batch 149 / 157\n",
      "training loss = 7.13355016708374\n",
      "validation loss = 7.159399007496081\n",
      "Batch 150 / 157\n",
      "training loss = 7.070367813110352\n",
      "validation loss = 7.156164570858604\n",
      "Batch 151 / 157\n",
      "training loss = 7.089088439941406\n",
      "validation loss = 7.152344854254472\n",
      "Batch 152 / 157\n",
      "training loss = 7.0057291984558105\n",
      "validation loss = 7.146242442883943\n",
      "Batch 153 / 157\n",
      "training loss = 7.053986072540283\n",
      "validation loss = 7.146812162901226\n",
      "Batch 154 / 157\n",
      "training loss = 7.074312210083008\n",
      "validation loss = 7.143251921001234\n",
      "Batch 155 / 157\n",
      "training loss = 7.07712459564209\n",
      "validation loss = 7.144677488427413\n",
      "Batch 156 / 157\n",
      "training loss = 7.421819686889648\n",
      "validation loss = 7.152646641982229\n",
      "Batch 157 / 157\n",
      "training loss = 6.405811786651611\n",
      "validation loss = 7.145654778731497\n",
      "Average training loss: 7.45\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.39523983001709\n",
      "validation loss = 10.140473717137388\n",
      "Batch 2 / 157\n",
      "training loss = 10.16340160369873\n",
      "validation loss = 9.776652637280916\n",
      "Batch 3 / 157\n",
      "training loss = 9.860326766967773\n",
      "validation loss = 9.409589365908975\n",
      "Batch 4 / 157\n",
      "training loss = 9.397226333618164\n",
      "validation loss = 9.07374778546785\n",
      "Batch 5 / 157\n",
      "training loss = 9.129168510437012\n",
      "validation loss = 8.758617350929661\n",
      "Batch 6 / 157\n",
      "training loss = 8.748578071594238\n",
      "validation loss = 8.48167143369976\n",
      "Batch 7 / 157\n",
      "training loss = 8.257369995117188\n",
      "validation loss = 8.270174553519801\n",
      "Batch 8 / 157\n",
      "training loss = 8.301859855651855\n",
      "validation loss = 8.144996542679635\n",
      "Batch 9 / 157\n",
      "training loss = 7.885990619659424\n",
      "validation loss = 8.096856092151842\n",
      "Batch 10 / 157\n",
      "training loss = 8.453990936279297\n",
      "validation loss = 8.115678611554598\n",
      "Batch 11 / 157\n",
      "training loss = 8.56392765045166\n",
      "validation loss = 8.140425933034797\n",
      "Batch 12 / 157\n",
      "training loss = 8.23029899597168\n",
      "validation loss = 8.14065637086567\n",
      "Batch 13 / 157\n",
      "training loss = 7.929697036743164\n",
      "validation loss = 8.071694223504318\n",
      "Batch 14 / 157\n",
      "training loss = 8.149958610534668\n",
      "validation loss = 7.988600705799303\n",
      "Batch 15 / 157\n",
      "training loss = 7.984930515289307\n",
      "validation loss = 8.004925075330233\n",
      "Batch 16 / 157\n",
      "training loss = 7.8641581535339355\n",
      "validation loss = 7.971146759233977\n",
      "Batch 17 / 157\n",
      "training loss = 8.141716003417969\n",
      "validation loss = 7.937370852420204\n",
      "Batch 18 / 157\n",
      "training loss = 8.012941360473633\n",
      "validation loss = 7.926967947106612\n",
      "Batch 19 / 157\n",
      "training loss = 7.711789131164551\n",
      "validation loss = 7.931607296592311\n",
      "Batch 20 / 157\n",
      "training loss = 7.6752028465271\n",
      "validation loss = 7.920814539256849\n",
      "Batch 21 / 157\n",
      "training loss = 8.027626037597656\n",
      "validation loss = 7.875924863313374\n",
      "Batch 22 / 157\n",
      "training loss = 8.143701553344727\n",
      "validation loss = 7.9122293120936344\n",
      "Batch 23 / 157\n",
      "training loss = 7.832698822021484\n",
      "validation loss = 7.855898581053081\n",
      "Batch 24 / 157\n",
      "training loss = 7.63947057723999\n",
      "validation loss = 7.862998686338726\n",
      "Batch 25 / 157\n",
      "training loss = 8.05235481262207\n",
      "validation loss = 7.869826141156648\n",
      "Batch 26 / 157\n",
      "training loss = 8.029644012451172\n",
      "validation loss = 7.855361762799714\n",
      "Batch 27 / 157\n",
      "training loss = 7.857015609741211\n",
      "validation loss = 7.860605089288009\n",
      "Batch 28 / 157\n",
      "training loss = 7.872375965118408\n",
      "validation loss = 7.864308231755307\n",
      "Batch 29 / 157\n",
      "training loss = 7.537227153778076\n",
      "validation loss = 7.84782432254992\n",
      "Batch 30 / 157\n",
      "training loss = 7.79371976852417\n",
      "validation loss = 7.8510129326268245\n",
      "Batch 31 / 157\n",
      "training loss = 7.608500003814697\n",
      "validation loss = 7.848231917933414\n",
      "Batch 32 / 157\n",
      "training loss = 7.948046684265137\n",
      "validation loss = 7.810827430925872\n",
      "Batch 33 / 157\n",
      "training loss = 7.653155326843262\n",
      "validation loss = 7.789499132256759\n",
      "Batch 34 / 157\n",
      "training loss = 7.6516547203063965\n",
      "validation loss = 7.763384618257222\n",
      "Batch 35 / 157\n",
      "training loss = 7.447327136993408\n",
      "validation loss = 7.744224598533229\n",
      "Batch 36 / 157\n",
      "training loss = 7.584265232086182\n",
      "validation loss = 7.719204802262156\n",
      "Batch 37 / 157\n",
      "training loss = 7.880085468292236\n",
      "validation loss = 7.761558106071071\n",
      "Batch 38 / 157\n",
      "training loss = 7.602717399597168\n",
      "validation loss = 7.703454594863088\n",
      "Batch 39 / 157\n",
      "training loss = 7.851248264312744\n",
      "validation loss = 7.730119529523347\n",
      "Batch 40 / 157\n",
      "training loss = 7.939322471618652\n",
      "validation loss = 7.681046410610802\n",
      "Batch 41 / 157\n",
      "training loss = 7.753575325012207\n",
      "validation loss = 7.701986413252981\n",
      "Batch 42 / 157\n",
      "training loss = 7.099157333374023\n",
      "validation loss = 7.669604602612947\n",
      "Batch 43 / 157\n",
      "training loss = 7.202692031860352\n",
      "validation loss = 7.701977027089972\n",
      "Batch 44 / 157\n",
      "training loss = 7.565279483795166\n",
      "validation loss = 7.694031238555908\n",
      "Batch 45 / 157\n",
      "training loss = 7.830713748931885\n",
      "validation loss = 7.649299646678724\n",
      "Batch 46 / 157\n",
      "training loss = 7.48541259765625\n",
      "validation loss = 7.643882952238384\n",
      "Batch 47 / 157\n",
      "training loss = 7.756658554077148\n",
      "validation loss = 7.676356616773103\n",
      "Batch 48 / 157\n",
      "training loss = 7.771567344665527\n",
      "validation loss = 7.6573654475965\n",
      "Batch 49 / 157\n",
      "training loss = 7.6881513595581055\n",
      "validation loss = 7.634612811239142\n",
      "Batch 50 / 157\n",
      "training loss = 7.7762064933776855\n",
      "validation loss = 7.632915321149324\n",
      "Batch 51 / 157\n",
      "training loss = 7.62957763671875\n",
      "validation loss = 7.624872759768837\n",
      "Batch 52 / 157\n",
      "training loss = 7.40360164642334\n",
      "validation loss = 7.627966152994256\n",
      "Batch 53 / 157\n",
      "training loss = 7.665459632873535\n",
      "validation loss = 7.6333044202704174\n",
      "Batch 54 / 157\n",
      "training loss = 6.989445209503174\n",
      "validation loss = 7.611849107240376\n",
      "Batch 55 / 157\n",
      "training loss = 7.605301856994629\n",
      "validation loss = 7.604941970423648\n",
      "Batch 56 / 157\n",
      "training loss = 7.070160388946533\n",
      "validation loss = 7.584381103515625\n",
      "Batch 57 / 157\n",
      "training loss = 7.6291069984436035\n",
      "validation loss = 7.5786356925964355\n",
      "Batch 58 / 157\n",
      "training loss = 7.628239631652832\n",
      "validation loss = 7.589404005753367\n",
      "Batch 59 / 157\n",
      "training loss = 7.680973052978516\n",
      "validation loss = 7.591258224688079\n",
      "Batch 60 / 157\n",
      "training loss = 7.6834564208984375\n",
      "validation loss = 7.559504433682091\n",
      "Batch 61 / 157\n",
      "training loss = 7.823525428771973\n",
      "validation loss = 7.547245151118228\n",
      "Batch 62 / 157\n",
      "training loss = 7.6223063468933105\n",
      "validation loss = 7.544021882508931\n",
      "Batch 63 / 157\n",
      "training loss = 7.82327127456665\n",
      "validation loss = 7.571859610708136\n",
      "Batch 64 / 157\n",
      "training loss = 7.776839733123779\n",
      "validation loss = 7.534451208616558\n",
      "Batch 65 / 157\n",
      "training loss = 7.466670989990234\n",
      "validation loss = 7.521598590047736\n",
      "Batch 66 / 157\n",
      "training loss = 7.591999530792236\n",
      "validation loss = 7.534760324578536\n",
      "Batch 67 / 157\n",
      "training loss = 7.665103435516357\n",
      "validation loss = 7.491851053739849\n",
      "Batch 68 / 157\n",
      "training loss = 7.433210372924805\n",
      "validation loss = 7.496315102828176\n",
      "Batch 69 / 157\n",
      "training loss = 7.7620697021484375\n",
      "validation loss = 7.484501813587389\n",
      "Batch 70 / 157\n",
      "training loss = 7.184067249298096\n",
      "validation loss = 7.489347106532047\n",
      "Batch 71 / 157\n",
      "training loss = 7.381638050079346\n",
      "validation loss = 7.490596620660079\n",
      "Batch 72 / 157\n",
      "training loss = 7.3711256980896\n",
      "validation loss = 7.5005935367785\n",
      "Batch 73 / 157\n",
      "training loss = 7.02014684677124\n",
      "validation loss = 7.474114292546322\n",
      "Batch 74 / 157\n",
      "training loss = 7.387471675872803\n",
      "validation loss = 7.459631041476601\n",
      "Batch 75 / 157\n",
      "training loss = 7.923338413238525\n",
      "validation loss = 7.486876964569092\n",
      "Batch 76 / 157\n",
      "training loss = 7.3277740478515625\n",
      "validation loss = 7.467777026327033\n",
      "Batch 77 / 157\n",
      "training loss = 7.205712795257568\n",
      "validation loss = 7.491077598772551\n",
      "Batch 78 / 157\n",
      "training loss = 7.529088973999023\n",
      "validation loss = 7.466474006050511\n",
      "Batch 79 / 157\n",
      "training loss = 7.2923712730407715\n",
      "validation loss = 7.485193001596551\n",
      "Batch 80 / 157\n",
      "training loss = 7.764652252197266\n",
      "validation loss = 7.447996239913137\n",
      "Batch 81 / 157\n",
      "training loss = 7.975733757019043\n",
      "validation loss = 7.445761078282406\n",
      "Batch 82 / 157\n",
      "training loss = 7.390568733215332\n",
      "validation loss = 7.437772248920641\n",
      "Batch 83 / 157\n",
      "training loss = 7.363211154937744\n",
      "validation loss = 7.428798449666877\n",
      "Batch 84 / 157\n",
      "training loss = 7.477766990661621\n",
      "validation loss = 7.445249808462043\n",
      "Batch 85 / 157\n",
      "training loss = 7.446883678436279\n",
      "validation loss = 7.433557485279284\n",
      "Batch 86 / 157\n",
      "training loss = 7.27753210067749\n",
      "validation loss = 7.409458913301167\n",
      "Batch 87 / 157\n",
      "training loss = 7.496286869049072\n",
      "validation loss = 7.4278883934021\n",
      "Batch 88 / 157\n",
      "training loss = 7.564680099487305\n",
      "validation loss = 7.408123668871428\n",
      "Batch 89 / 157\n",
      "training loss = 6.894375324249268\n",
      "validation loss = 7.406449468512284\n",
      "Batch 90 / 157\n",
      "training loss = 7.8028388023376465\n",
      "validation loss = 7.398758386310778\n",
      "Batch 91 / 157\n",
      "training loss = 7.031816482543945\n",
      "validation loss = 7.37651232669228\n",
      "Batch 92 / 157\n",
      "training loss = 6.9324049949646\n",
      "validation loss = 7.407746540872674\n",
      "Batch 93 / 157\n",
      "training loss = 7.348577976226807\n",
      "validation loss = 7.3928357174522\n",
      "Batch 94 / 157\n",
      "training loss = 7.244720935821533\n",
      "validation loss = 7.39835262298584\n",
      "Batch 95 / 157\n",
      "training loss = 7.007350921630859\n",
      "validation loss = 7.3858565531278915\n",
      "Batch 96 / 157\n",
      "training loss = 7.698950290679932\n",
      "validation loss = 7.3718396237022\n",
      "Batch 97 / 157\n",
      "training loss = 7.38748836517334\n",
      "validation loss = 7.365998017160516\n",
      "Batch 98 / 157\n",
      "training loss = 7.681416988372803\n",
      "validation loss = 7.347834712580631\n",
      "Batch 99 / 157\n",
      "training loss = 7.675328731536865\n",
      "validation loss = 7.360481538270649\n",
      "Batch 100 / 157\n",
      "training loss = 7.076589584350586\n",
      "validation loss = 7.336973767531545\n",
      "Batch 101 / 157\n",
      "training loss = 7.230579853057861\n",
      "validation loss = 7.337554103449771\n",
      "Batch 102 / 157\n",
      "training loss = 7.30502462387085\n",
      "validation loss = 7.322684890345523\n",
      "Batch 103 / 157\n",
      "training loss = 7.629017353057861\n",
      "validation loss = 7.3404715437638135\n",
      "Batch 104 / 157\n",
      "training loss = 7.30636739730835\n",
      "validation loss = 7.360235816554019\n",
      "Batch 105 / 157\n",
      "training loss = 7.345276832580566\n",
      "validation loss = 7.321298925500167\n",
      "Batch 106 / 157\n",
      "training loss = 7.581667900085449\n",
      "validation loss = 7.3161922755994295\n",
      "Batch 107 / 157\n",
      "training loss = 7.165803909301758\n",
      "validation loss = 7.312054282740543\n",
      "Batch 108 / 157\n",
      "training loss = 7.475317001342773\n",
      "validation loss = 7.344480614913137\n",
      "Batch 109 / 157\n",
      "training loss = 7.610179424285889\n",
      "validation loss = 7.327002173975894\n",
      "Batch 110 / 157\n",
      "training loss = 6.92354679107666\n",
      "validation loss = 7.320786300458406\n",
      "Batch 111 / 157\n",
      "training loss = 7.811827659606934\n",
      "validation loss = 7.313748635743794\n",
      "Batch 112 / 157\n",
      "training loss = 7.3435773849487305\n",
      "validation loss = 7.310484534815738\n",
      "Batch 113 / 157\n",
      "training loss = 7.311773777008057\n",
      "validation loss = 7.314731121063232\n",
      "Batch 114 / 157\n",
      "training loss = 7.295920372009277\n",
      "validation loss = 7.3232655023273665\n",
      "Batch 115 / 157\n",
      "training loss = 7.458376407623291\n",
      "validation loss = 7.323807816756399\n",
      "Batch 116 / 157\n",
      "training loss = 7.205548286437988\n",
      "validation loss = 7.31168112001921\n",
      "Batch 117 / 157\n",
      "training loss = 7.263120174407959\n",
      "validation loss = 7.306168932663767\n",
      "Batch 118 / 157\n",
      "training loss = 7.114088535308838\n",
      "validation loss = 7.296481985794871\n",
      "Batch 119 / 157\n",
      "training loss = 7.228964328765869\n",
      "validation loss = 7.304488307551334\n",
      "Batch 120 / 157\n",
      "training loss = 7.441875457763672\n",
      "validation loss = 7.282940136758905\n",
      "Batch 121 / 157\n",
      "training loss = 7.4020490646362305\n",
      "validation loss = 7.303993777224892\n",
      "Batch 122 / 157\n",
      "training loss = 7.37329626083374\n",
      "validation loss = 7.3220257006193465\n",
      "Batch 123 / 157\n",
      "training loss = 7.741607189178467\n",
      "validation loss = 7.284724260631361\n",
      "Batch 124 / 157\n",
      "training loss = 7.619170665740967\n",
      "validation loss = 7.3094383540906405\n",
      "Batch 125 / 157\n",
      "training loss = 7.397848606109619\n",
      "validation loss = 7.3180818808706185\n",
      "Batch 126 / 157\n",
      "training loss = 7.043118476867676\n",
      "validation loss = 7.304630329734401\n",
      "Batch 127 / 157\n",
      "training loss = 6.980477333068848\n",
      "validation loss = 7.308538436889648\n",
      "Batch 128 / 157\n",
      "training loss = 7.313193321228027\n",
      "validation loss = 7.289701662565532\n",
      "Batch 129 / 157\n",
      "training loss = 7.315768718719482\n",
      "validation loss = 7.283130645751953\n",
      "Batch 130 / 157\n",
      "training loss = 7.0584187507629395\n",
      "validation loss = 7.281927334634881\n",
      "Batch 131 / 157\n",
      "training loss = 7.045007228851318\n",
      "validation loss = 7.306457042694092\n",
      "Batch 132 / 157\n",
      "training loss = 6.970770359039307\n",
      "validation loss = 7.297930366114566\n",
      "Batch 133 / 157\n",
      "training loss = 6.838379383087158\n",
      "validation loss = 7.273807876988461\n",
      "Batch 134 / 157\n",
      "training loss = 6.646203994750977\n",
      "validation loss = 7.281055249665913\n",
      "Batch 135 / 157\n",
      "training loss = 7.079442024230957\n",
      "validation loss = 7.273540070182399\n",
      "Batch 136 / 157\n",
      "training loss = 7.033591270446777\n",
      "validation loss = 7.266011213001452\n",
      "Batch 137 / 157\n",
      "training loss = 7.411380290985107\n",
      "validation loss = 7.286727855080052\n",
      "Batch 138 / 157\n",
      "training loss = 7.489450454711914\n",
      "validation loss = 7.255979286996942\n",
      "Batch 139 / 157\n",
      "training loss = 7.046700477600098\n",
      "validation loss = 7.265452635915656\n",
      "Batch 140 / 157\n",
      "training loss = 7.291584014892578\n",
      "validation loss = 7.264912103351794\n",
      "Batch 141 / 157\n",
      "training loss = 7.092462539672852\n",
      "validation loss = 7.257377122577868\n",
      "Batch 142 / 157\n",
      "training loss = 7.612313747406006\n",
      "validation loss = 7.308078665482371\n",
      "Batch 143 / 157\n",
      "training loss = 7.345370769500732\n",
      "validation loss = 7.272225656007466\n",
      "Batch 144 / 157\n",
      "training loss = 7.5438103675842285\n",
      "validation loss = 7.244453078822086\n",
      "Batch 145 / 157\n",
      "training loss = 7.1137895584106445\n",
      "validation loss = 7.255092244399221\n",
      "Batch 146 / 157\n",
      "training loss = 6.989291191101074\n",
      "validation loss = 7.235951498935097\n",
      "Batch 147 / 157\n",
      "training loss = 7.270245552062988\n",
      "validation loss = 7.238316159499319\n",
      "Batch 148 / 157\n",
      "training loss = 7.337862491607666\n",
      "validation loss = 7.229532241821289\n",
      "Batch 149 / 157\n",
      "training loss = 7.435365676879883\n",
      "validation loss = 7.227263450622559\n",
      "Batch 150 / 157\n",
      "training loss = 7.371924877166748\n",
      "validation loss = 7.21621026490864\n",
      "Batch 151 / 157\n",
      "training loss = 7.423417568206787\n",
      "validation loss = 7.224830627441406\n",
      "Batch 152 / 157\n",
      "training loss = 6.923685550689697\n",
      "validation loss = 7.213869094848633\n",
      "Batch 153 / 157\n",
      "training loss = 7.3140974044799805\n",
      "validation loss = 7.21547121750681\n",
      "Batch 154 / 157\n",
      "training loss = 7.253516674041748\n",
      "validation loss = 7.214824651416979\n",
      "Batch 155 / 157\n",
      "training loss = 7.37866735458374\n",
      "validation loss = 7.20111420280055\n",
      "Batch 156 / 157\n",
      "training loss = 7.098418235778809\n",
      "validation loss = 7.200876838282535\n",
      "Batch 157 / 157\n",
      "training loss = 7.999958038330078\n",
      "validation loss = 7.20497317063181\n",
      "Average training loss: 7.59\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.390951156616211\n",
      "validation loss = 10.17666630995901\n",
      "Batch 2 / 157\n",
      "training loss = 10.207941055297852\n",
      "validation loss = 9.701913381877699\n",
      "Batch 3 / 157\n",
      "training loss = 9.72516918182373\n",
      "validation loss = 9.303362946761283\n",
      "Batch 4 / 157\n",
      "training loss = 9.201833724975586\n",
      "validation loss = 8.936948475084806\n",
      "Batch 5 / 157\n",
      "training loss = 8.929739952087402\n",
      "validation loss = 8.616082693401136\n",
      "Batch 6 / 157\n",
      "training loss = 8.640810012817383\n",
      "validation loss = 8.348636024876646\n",
      "Batch 7 / 157\n",
      "training loss = 8.444539070129395\n",
      "validation loss = 8.15311978992663\n",
      "Batch 8 / 157\n",
      "training loss = 8.295527458190918\n",
      "validation loss = 8.025826454162598\n",
      "Batch 9 / 157\n",
      "training loss = 7.893364906311035\n",
      "validation loss = 7.954776111402009\n",
      "Batch 10 / 157\n",
      "training loss = 7.965515613555908\n",
      "validation loss = 7.927972015581633\n",
      "Batch 11 / 157\n",
      "training loss = 8.167318344116211\n",
      "validation loss = 7.929263591766357\n",
      "Batch 12 / 157\n",
      "training loss = 7.915693283081055\n",
      "validation loss = 7.9294945064343905\n",
      "Batch 13 / 157\n",
      "training loss = 8.022854804992676\n",
      "validation loss = 7.905625092355828\n",
      "Batch 14 / 157\n",
      "training loss = 8.017882347106934\n",
      "validation loss = 7.8542049558539135\n",
      "Batch 15 / 157\n",
      "training loss = 7.964110851287842\n",
      "validation loss = 7.844130440762169\n",
      "Batch 16 / 157\n",
      "training loss = 7.997619152069092\n",
      "validation loss = 7.883687897732384\n",
      "Batch 17 / 157\n",
      "training loss = 7.890939235687256\n",
      "validation loss = 7.815811809740569\n",
      "Batch 18 / 157\n",
      "training loss = 7.712666034698486\n",
      "validation loss = 7.813106411381772\n",
      "Batch 19 / 157\n",
      "training loss = 7.636923313140869\n",
      "validation loss = 7.844959434710051\n",
      "Batch 20 / 157\n",
      "training loss = 7.800280570983887\n",
      "validation loss = 7.840879841854698\n",
      "Batch 21 / 157\n",
      "training loss = 7.929960250854492\n",
      "validation loss = 7.812452266090794\n",
      "Batch 22 / 157\n",
      "training loss = 7.717257499694824\n",
      "validation loss = 7.777506301277562\n",
      "Batch 23 / 157\n",
      "training loss = 7.518247127532959\n",
      "validation loss = 7.764929420069644\n",
      "Batch 24 / 157\n",
      "training loss = 8.221482276916504\n",
      "validation loss = 7.792942900406687\n",
      "Batch 25 / 157\n",
      "training loss = 7.930809020996094\n",
      "validation loss = 7.812111804359837\n",
      "Batch 26 / 157\n",
      "training loss = 7.863452434539795\n",
      "validation loss = 7.793840910259046\n",
      "Batch 27 / 157\n",
      "training loss = 7.987856864929199\n",
      "validation loss = 7.763106019873368\n",
      "Batch 28 / 157\n",
      "training loss = 8.126691818237305\n",
      "validation loss = 7.746500441902562\n",
      "Batch 29 / 157\n",
      "training loss = 7.7429022789001465\n",
      "validation loss = 7.752046660373085\n",
      "Batch 30 / 157\n",
      "training loss = 7.574520587921143\n",
      "validation loss = 7.735867952045641\n",
      "Batch 31 / 157\n",
      "training loss = 7.6995530128479\n",
      "validation loss = 7.7224913898267245\n",
      "Batch 32 / 157\n",
      "training loss = 7.744429111480713\n",
      "validation loss = 7.696242583425422\n",
      "Batch 33 / 157\n",
      "training loss = 7.930638790130615\n",
      "validation loss = 7.677817319568835\n",
      "Batch 34 / 157\n",
      "training loss = 7.5819525718688965\n",
      "validation loss = 7.675960917221873\n",
      "Batch 35 / 157\n",
      "training loss = 7.764565944671631\n",
      "validation loss = 7.657227666754472\n",
      "Batch 36 / 157\n",
      "training loss = 7.785651683807373\n",
      "validation loss = 7.626701882011012\n",
      "Batch 37 / 157\n",
      "training loss = 7.725866794586182\n",
      "validation loss = 7.616832833541067\n",
      "Batch 38 / 157\n",
      "training loss = 7.934627056121826\n",
      "validation loss = 7.61159254375257\n",
      "Batch 39 / 157\n",
      "training loss = 7.627060890197754\n",
      "validation loss = 7.606324195861816\n",
      "Batch 40 / 157\n",
      "training loss = 7.74633264541626\n",
      "validation loss = 7.587135440424869\n",
      "Batch 41 / 157\n",
      "training loss = 7.747535705566406\n",
      "validation loss = 7.583271177191484\n",
      "Batch 42 / 157\n",
      "training loss = 7.723794937133789\n",
      "validation loss = 7.573129453157124\n",
      "Batch 43 / 157\n",
      "training loss = 7.5654706954956055\n",
      "validation loss = 7.564874849821392\n",
      "Batch 44 / 157\n",
      "training loss = 7.395278453826904\n",
      "validation loss = 7.5604117293106885\n",
      "Batch 45 / 157\n",
      "training loss = 7.502496242523193\n",
      "validation loss = 7.553278170133892\n",
      "Batch 46 / 157\n",
      "training loss = 7.589871406555176\n",
      "validation loss = 7.551170600088019\n",
      "Batch 47 / 157\n",
      "training loss = 7.442959785461426\n",
      "validation loss = 7.5507276183680485\n",
      "Batch 48 / 157\n",
      "training loss = 7.355711460113525\n",
      "validation loss = 7.544905361376311\n",
      "Batch 49 / 157\n",
      "training loss = 7.537868976593018\n",
      "validation loss = 7.538268164584511\n",
      "Batch 50 / 157\n",
      "training loss = 7.444326877593994\n",
      "validation loss = 7.531787621347528\n",
      "Batch 51 / 157\n",
      "training loss = 7.406233787536621\n",
      "validation loss = 7.521152797498201\n",
      "Batch 52 / 157\n",
      "training loss = 7.69779634475708\n",
      "validation loss = 7.515842889484606\n",
      "Batch 53 / 157\n",
      "training loss = 7.480607986450195\n",
      "validation loss = 7.515471332951596\n",
      "Batch 54 / 157\n",
      "training loss = 7.894519329071045\n",
      "validation loss = 7.525153084805138\n",
      "Batch 55 / 157\n",
      "training loss = 7.664194583892822\n",
      "validation loss = 7.508473521784732\n",
      "Batch 56 / 157\n",
      "training loss = 7.644575119018555\n",
      "validation loss = 7.499101312536943\n",
      "Batch 57 / 157\n",
      "training loss = 7.609577655792236\n",
      "validation loss = 7.4940566765634635\n",
      "Batch 58 / 157\n",
      "training loss = 7.782285690307617\n",
      "validation loss = 7.483980429799933\n",
      "Batch 59 / 157\n",
      "training loss = 7.445697784423828\n",
      "validation loss = 7.481790517505846\n",
      "Batch 60 / 157\n",
      "training loss = 7.567986965179443\n",
      "validation loss = 7.4862927888569075\n",
      "Batch 61 / 157\n",
      "training loss = 7.681992053985596\n",
      "validation loss = 7.469170545276842\n",
      "Batch 62 / 157\n",
      "training loss = 7.596966743469238\n",
      "validation loss = 7.4594880154258325\n",
      "Batch 63 / 157\n",
      "training loss = 7.447085857391357\n",
      "validation loss = 7.4621155136509945\n",
      "Batch 64 / 157\n",
      "training loss = 7.395549297332764\n",
      "validation loss = 7.455555338608591\n",
      "Batch 65 / 157\n",
      "training loss = 7.827754974365234\n",
      "validation loss = 7.467695436979595\n",
      "Batch 66 / 157\n",
      "training loss = 7.3262739181518555\n",
      "validation loss = 7.450190017097874\n",
      "Batch 67 / 157\n",
      "training loss = 7.491394996643066\n",
      "validation loss = 7.445614990435149\n",
      "Batch 68 / 157\n",
      "training loss = 7.429795265197754\n",
      "validation loss = 7.467097558473286\n",
      "Batch 69 / 157\n",
      "training loss = 7.462404727935791\n",
      "validation loss = 7.4606445965014005\n",
      "Batch 70 / 157\n",
      "training loss = 7.3697829246521\n",
      "validation loss = 7.438397382435046\n",
      "Batch 71 / 157\n",
      "training loss = 7.768181800842285\n",
      "validation loss = 7.459452202445583\n",
      "Batch 72 / 157\n",
      "training loss = 7.777565002441406\n",
      "validation loss = 7.468344010804829\n",
      "Batch 73 / 157\n",
      "training loss = 7.353644371032715\n",
      "validation loss = 7.431156735671194\n",
      "Batch 74 / 157\n",
      "training loss = 7.660069942474365\n",
      "validation loss = 7.4344541650069385\n",
      "Batch 75 / 157\n",
      "training loss = 7.203990459442139\n",
      "validation loss = 7.439557803304572\n",
      "Batch 76 / 157\n",
      "training loss = 7.512332439422607\n",
      "validation loss = 7.43283412331029\n",
      "Batch 77 / 157\n",
      "training loss = 7.6616530418396\n",
      "validation loss = 7.432002745176616\n",
      "Batch 78 / 157\n",
      "training loss = 7.601916790008545\n",
      "validation loss = 7.43386361473485\n",
      "Batch 79 / 157\n",
      "training loss = 7.587032794952393\n",
      "validation loss = 7.434094981143349\n",
      "Batch 80 / 157\n",
      "training loss = 7.591486930847168\n",
      "validation loss = 7.419896953984311\n",
      "Batch 81 / 157\n",
      "training loss = 7.241721153259277\n",
      "validation loss = 7.411473976938348\n",
      "Batch 82 / 157\n",
      "training loss = 7.38001012802124\n",
      "validation loss = 7.409538118462813\n",
      "Batch 83 / 157\n",
      "training loss = 7.204464435577393\n",
      "validation loss = 7.40680812534533\n",
      "Batch 84 / 157\n",
      "training loss = 7.553782939910889\n",
      "validation loss = 7.399518966674805\n",
      "Batch 85 / 157\n",
      "training loss = 7.50020170211792\n",
      "validation loss = 7.394466801693565\n",
      "Batch 86 / 157\n",
      "training loss = 7.591861724853516\n",
      "validation loss = 7.40827909268831\n",
      "Batch 87 / 157\n",
      "training loss = 7.276278018951416\n",
      "validation loss = 7.398385424362986\n",
      "Batch 88 / 157\n",
      "training loss = 7.364762306213379\n",
      "validation loss = 7.385535666817113\n",
      "Batch 89 / 157\n",
      "training loss = 7.401555061340332\n",
      "validation loss = 7.387923341048391\n",
      "Batch 90 / 157\n",
      "training loss = 7.2025346755981445\n",
      "validation loss = 7.388415236222117\n",
      "Batch 91 / 157\n",
      "training loss = 7.390448093414307\n",
      "validation loss = 7.382180138638145\n",
      "Batch 92 / 157\n",
      "training loss = 7.366565227508545\n",
      "validation loss = 7.377314944016306\n",
      "Batch 93 / 157\n",
      "training loss = 7.472946643829346\n",
      "validation loss = 7.373230683176141\n",
      "Batch 94 / 157\n",
      "training loss = 7.438958644866943\n",
      "validation loss = 7.385662204340885\n",
      "Batch 95 / 157\n",
      "training loss = 7.404938697814941\n",
      "validation loss = 7.38119797957571\n",
      "Batch 96 / 157\n",
      "training loss = 7.581259727478027\n",
      "validation loss = 7.376379514995374\n",
      "Batch 97 / 157\n",
      "training loss = 7.236893177032471\n",
      "validation loss = 7.366479396820068\n",
      "Batch 98 / 157\n",
      "training loss = 7.37360954284668\n",
      "validation loss = 7.372078970858925\n",
      "Batch 99 / 157\n",
      "training loss = 7.557553768157959\n",
      "validation loss = 7.371657798164769\n",
      "Batch 100 / 157\n",
      "training loss = 7.447662353515625\n",
      "validation loss = 7.359682183516653\n",
      "Batch 101 / 157\n",
      "training loss = 7.3528642654418945\n",
      "validation loss = 7.351283173812063\n",
      "Batch 102 / 157\n",
      "training loss = 7.160628795623779\n",
      "validation loss = 7.359812008707147\n",
      "Batch 103 / 157\n",
      "training loss = 7.315038681030273\n",
      "validation loss = 7.364564719953035\n",
      "Batch 104 / 157\n",
      "training loss = 7.325869560241699\n",
      "validation loss = 7.359782369513261\n",
      "Batch 105 / 157\n",
      "training loss = 7.43906307220459\n",
      "validation loss = 7.353294146688361\n",
      "Batch 106 / 157\n",
      "training loss = 7.4669294357299805\n",
      "validation loss = 7.34295496187712\n",
      "Batch 107 / 157\n",
      "training loss = 7.226016044616699\n",
      "validation loss = 7.340312531119899\n",
      "Batch 108 / 157\n",
      "training loss = 7.505496501922607\n",
      "validation loss = 7.34314341294138\n",
      "Batch 109 / 157\n",
      "training loss = 7.145075798034668\n",
      "validation loss = 7.347846608412893\n",
      "Batch 110 / 157\n",
      "training loss = 7.167144775390625\n",
      "validation loss = 7.341035516638505\n",
      "Batch 111 / 157\n",
      "training loss = 7.306633949279785\n",
      "validation loss = 7.334956420095343\n",
      "Batch 112 / 157\n",
      "training loss = 7.471837997436523\n",
      "validation loss = 7.333380172127171\n",
      "Batch 113 / 157\n",
      "training loss = 7.539651870727539\n",
      "validation loss = 7.330259072153192\n",
      "Batch 114 / 157\n",
      "training loss = 7.485793113708496\n",
      "validation loss = 7.333354473114014\n",
      "Batch 115 / 157\n",
      "training loss = 7.530801296234131\n",
      "validation loss = 7.332511098761308\n",
      "Batch 116 / 157\n",
      "training loss = 7.070812702178955\n",
      "validation loss = 7.319418781682065\n",
      "Batch 117 / 157\n",
      "training loss = 7.3016815185546875\n",
      "validation loss = 7.315888128782573\n",
      "Batch 118 / 157\n",
      "training loss = 7.430027484893799\n",
      "validation loss = 7.317104113729377\n",
      "Batch 119 / 157\n",
      "training loss = 7.301883220672607\n",
      "validation loss = 7.328244460256476\n",
      "Batch 120 / 157\n",
      "training loss = 7.561464309692383\n",
      "validation loss = 7.3273947113438656\n",
      "Batch 121 / 157\n",
      "training loss = 7.402038097381592\n",
      "validation loss = 7.314115348615144\n",
      "Batch 122 / 157\n",
      "training loss = 7.118678569793701\n",
      "validation loss = 7.306249794207122\n",
      "Batch 123 / 157\n",
      "training loss = 7.4686737060546875\n",
      "validation loss = 7.299001894499126\n",
      "Batch 124 / 157\n",
      "training loss = 7.033452033996582\n",
      "validation loss = 7.309301928470009\n",
      "Batch 125 / 157\n",
      "training loss = 7.313551425933838\n",
      "validation loss = 7.304958167829011\n",
      "Batch 126 / 157\n",
      "training loss = 7.284739017486572\n",
      "validation loss = 7.298370587198358\n",
      "Batch 127 / 157\n",
      "training loss = 7.349119663238525\n",
      "validation loss = 7.285709004653127\n",
      "Batch 128 / 157\n",
      "training loss = 7.24824333190918\n",
      "validation loss = 7.286205643101742\n",
      "Batch 129 / 157\n",
      "training loss = 7.256202697753906\n",
      "validation loss = 7.288042219061601\n",
      "Batch 130 / 157\n",
      "training loss = 7.529499530792236\n",
      "validation loss = 7.287990896325362\n",
      "Batch 131 / 157\n",
      "training loss = 7.371101379394531\n",
      "validation loss = 7.277419792978387\n",
      "Batch 132 / 157\n",
      "training loss = 7.388652801513672\n",
      "validation loss = 7.266834158646433\n",
      "Batch 133 / 157\n",
      "training loss = 7.406979084014893\n",
      "validation loss = 7.267990840108771\n",
      "Batch 134 / 157\n",
      "training loss = 7.308481693267822\n",
      "validation loss = 7.26989766171104\n",
      "Batch 135 / 157\n",
      "training loss = 7.3633503913879395\n",
      "validation loss = 7.2645392417907715\n",
      "Batch 136 / 157\n",
      "training loss = 7.596085548400879\n",
      "validation loss = 7.260123855189273\n",
      "Batch 137 / 157\n",
      "training loss = 7.298962593078613\n",
      "validation loss = 7.267210483551025\n",
      "Batch 138 / 157\n",
      "training loss = 7.182711124420166\n",
      "validation loss = 7.267510589800383\n",
      "Batch 139 / 157\n",
      "training loss = 7.071901798248291\n",
      "validation loss = 7.264814075670745\n",
      "Batch 140 / 157\n",
      "training loss = 7.065431594848633\n",
      "validation loss = 7.260384107890882\n",
      "Batch 141 / 157\n",
      "training loss = 7.2264909744262695\n",
      "validation loss = 7.25545795340287\n",
      "Batch 142 / 157\n",
      "training loss = 7.252804279327393\n",
      "validation loss = 7.251051777287533\n",
      "Batch 143 / 157\n",
      "training loss = 7.285383701324463\n",
      "validation loss = 7.257220970956903\n",
      "Batch 144 / 157\n",
      "training loss = 7.397090911865234\n",
      "validation loss = 7.2533266669825505\n",
      "Batch 145 / 157\n",
      "training loss = 7.315316677093506\n",
      "validation loss = 7.2443016202826245\n",
      "Batch 146 / 157\n",
      "training loss = 7.354464530944824\n",
      "validation loss = 7.23647581903558\n",
      "Batch 147 / 157\n",
      "training loss = 7.213785171508789\n",
      "validation loss = 7.246228720012464\n",
      "Batch 148 / 157\n",
      "training loss = 7.1059441566467285\n",
      "validation loss = 7.240472718289024\n",
      "Batch 149 / 157\n",
      "training loss = 7.174978733062744\n",
      "validation loss = 7.234816626498573\n",
      "Batch 150 / 157\n",
      "training loss = 7.242641448974609\n",
      "validation loss = 7.232024544163754\n",
      "Batch 151 / 157\n",
      "training loss = 7.228639602661133\n",
      "validation loss = 7.2318421915957805\n",
      "Batch 152 / 157\n",
      "training loss = 7.459958553314209\n",
      "validation loss = 7.225062997717607\n",
      "Batch 153 / 157\n",
      "training loss = 7.154726505279541\n",
      "validation loss = 7.227966484270598\n",
      "Batch 154 / 157\n",
      "training loss = 7.168920516967773\n",
      "validation loss = 7.222767854991712\n",
      "Batch 155 / 157\n",
      "training loss = 7.285443305969238\n",
      "validation loss = 7.215841192948191\n",
      "Batch 156 / 157\n",
      "training loss = 7.090834140777588\n",
      "validation loss = 7.216829174443295\n",
      "Batch 157 / 157\n",
      "training loss = 7.279418468475342\n",
      "validation loss = 7.228093247664602\n",
      "Average training loss: 7.59\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.394155502319336\n",
      "validation loss = 10.168125002007736\n",
      "Batch 2 / 157\n",
      "training loss = 10.137948989868164\n",
      "validation loss = 9.63948224720202\n",
      "Batch 3 / 157\n",
      "training loss = 9.675677299499512\n",
      "validation loss = 9.229513218528346\n",
      "Batch 4 / 157\n",
      "training loss = 9.240846633911133\n",
      "validation loss = 8.875466698094419\n",
      "Batch 5 / 157\n",
      "training loss = 8.901097297668457\n",
      "validation loss = 8.567000188325581\n",
      "Batch 6 / 157\n",
      "training loss = 8.603167533874512\n",
      "validation loss = 8.318195744564658\n",
      "Batch 7 / 157\n",
      "training loss = 8.33229923248291\n",
      "validation loss = 8.130740918611226\n",
      "Batch 8 / 157\n",
      "training loss = 7.8693976402282715\n",
      "validation loss = 7.986460459859748\n",
      "Batch 9 / 157\n",
      "training loss = 8.265998840332031\n",
      "validation loss = 7.904299459959331\n",
      "Batch 10 / 157\n",
      "training loss = 7.876107215881348\n",
      "validation loss = 7.835018383829217\n",
      "Batch 11 / 157\n",
      "training loss = 8.051505088806152\n",
      "validation loss = 7.7993349527057845\n",
      "Batch 12 / 157\n",
      "training loss = 7.99161434173584\n",
      "validation loss = 7.779627724697716\n",
      "Batch 13 / 157\n",
      "training loss = 7.537550926208496\n",
      "validation loss = 7.773387181131463\n",
      "Batch 14 / 157\n",
      "training loss = 7.680888652801514\n",
      "validation loss = 7.745744002492804\n",
      "Batch 15 / 157\n",
      "training loss = 7.62039852142334\n",
      "validation loss = 7.740757038718776\n",
      "Batch 16 / 157\n",
      "training loss = 7.619358539581299\n",
      "validation loss = 7.740675650144878\n",
      "Batch 17 / 157\n",
      "training loss = 7.902667999267578\n",
      "validation loss = 7.7246551513671875\n",
      "Batch 18 / 157\n",
      "training loss = 7.900547027587891\n",
      "validation loss = 7.70120540418123\n",
      "Batch 19 / 157\n",
      "training loss = 7.780830383300781\n",
      "validation loss = 7.691817107953523\n",
      "Batch 20 / 157\n",
      "training loss = 7.736140251159668\n",
      "validation loss = 7.690232803947048\n",
      "Batch 21 / 157\n",
      "training loss = 7.786215305328369\n",
      "validation loss = 7.675175290358694\n",
      "Batch 22 / 157\n",
      "training loss = 7.744472980499268\n",
      "validation loss = 7.661550722624126\n",
      "Batch 23 / 157\n",
      "training loss = 7.7884202003479\n",
      "validation loss = 7.644904989945261\n",
      "Batch 24 / 157\n",
      "training loss = 7.703120708465576\n",
      "validation loss = 7.623787528590152\n",
      "Batch 25 / 157\n",
      "training loss = 7.493610858917236\n",
      "validation loss = 7.599988912281237\n",
      "Batch 26 / 157\n",
      "training loss = 7.8312201499938965\n",
      "validation loss = 7.600311354586952\n",
      "Batch 27 / 157\n",
      "training loss = 7.654541015625\n",
      "validation loss = 7.586525063765676\n",
      "Batch 28 / 157\n",
      "training loss = 7.323004245758057\n",
      "validation loss = 7.587351648431075\n",
      "Batch 29 / 157\n",
      "training loss = 7.476626873016357\n",
      "validation loss = 7.565128401706093\n",
      "Batch 30 / 157\n",
      "training loss = 7.493410587310791\n",
      "validation loss = 7.566317307321649\n",
      "Batch 31 / 157\n",
      "training loss = 7.398531436920166\n",
      "validation loss = 7.549184071390252\n",
      "Batch 32 / 157\n",
      "training loss = 7.539478778839111\n",
      "validation loss = 7.54740770239579\n",
      "Batch 33 / 157\n",
      "training loss = 7.64088249206543\n",
      "validation loss = 7.5267543290790755\n",
      "Batch 34 / 157\n",
      "training loss = 7.504815101623535\n",
      "validation loss = 7.522405950646651\n",
      "Batch 35 / 157\n",
      "training loss = 7.435930252075195\n",
      "validation loss = 7.512374049738834\n",
      "Batch 36 / 157\n",
      "training loss = 7.45049524307251\n",
      "validation loss = 7.496638549001593\n",
      "Batch 37 / 157\n",
      "training loss = 7.6567158699035645\n",
      "validation loss = 7.499722079226845\n",
      "Batch 38 / 157\n",
      "training loss = 7.213282108306885\n",
      "validation loss = 7.4965839636953255\n",
      "Batch 39 / 157\n",
      "training loss = 7.308528900146484\n",
      "validation loss = 7.488090163783023\n",
      "Batch 40 / 157\n",
      "training loss = 7.542959213256836\n",
      "validation loss = 7.486272937373111\n",
      "Batch 41 / 157\n",
      "training loss = 7.294823169708252\n",
      "validation loss = 7.482329067430999\n",
      "Batch 42 / 157\n",
      "training loss = 7.223701000213623\n",
      "validation loss = 7.477031406603362\n",
      "Batch 43 / 157\n",
      "training loss = 7.468688488006592\n",
      "validation loss = 7.468877817455091\n",
      "Batch 44 / 157\n",
      "training loss = 7.535323619842529\n",
      "validation loss = 7.453185282255474\n",
      "Batch 45 / 157\n",
      "training loss = 7.133372783660889\n",
      "validation loss = 7.46026463257639\n",
      "Batch 46 / 157\n",
      "training loss = 7.574935436248779\n",
      "validation loss = 7.455579054983039\n",
      "Batch 47 / 157\n",
      "training loss = 7.318942070007324\n",
      "validation loss = 7.44171498951159\n",
      "Batch 48 / 157\n",
      "training loss = 7.32368278503418\n",
      "validation loss = 7.439898064261989\n",
      "Batch 49 / 157\n",
      "training loss = 7.122123718261719\n",
      "validation loss = 7.4359682986610816\n",
      "Batch 50 / 157\n",
      "training loss = 7.4723124504089355\n",
      "validation loss = 7.4195628668132585\n",
      "Batch 51 / 157\n",
      "training loss = 7.335203647613525\n",
      "validation loss = 7.42321935452913\n",
      "Batch 52 / 157\n",
      "training loss = 7.322154521942139\n",
      "validation loss = 7.425277157833702\n",
      "Batch 53 / 157\n",
      "training loss = 7.545895099639893\n",
      "validation loss = 7.400687569066098\n",
      "Batch 54 / 157\n",
      "training loss = 7.425518989562988\n",
      "validation loss = 7.400084194384124\n",
      "Batch 55 / 157\n",
      "training loss = 7.204182147979736\n",
      "validation loss = 7.418203529558684\n",
      "Batch 56 / 157\n",
      "training loss = 7.254611968994141\n",
      "validation loss = 7.420410658183851\n",
      "Batch 57 / 157\n",
      "training loss = 7.327627658843994\n",
      "validation loss = 7.397792113454718\n",
      "Batch 58 / 157\n",
      "training loss = 7.509236812591553\n",
      "validation loss = 7.393901674371016\n",
      "Batch 59 / 157\n",
      "training loss = 7.260776042938232\n",
      "validation loss = 7.400522206958971\n",
      "Batch 60 / 157\n",
      "training loss = 7.456217288970947\n",
      "validation loss = 7.395711371773167\n",
      "Batch 61 / 157\n",
      "training loss = 7.211636543273926\n",
      "validation loss = 7.38293605101736\n",
      "Batch 62 / 157\n",
      "training loss = 7.355186462402344\n",
      "validation loss = 7.384301411478143\n",
      "Batch 63 / 157\n",
      "training loss = 7.586705684661865\n",
      "validation loss = 7.387960760216964\n",
      "Batch 64 / 157\n",
      "training loss = 7.275871753692627\n",
      "validation loss = 7.382490057694285\n",
      "Batch 65 / 157\n",
      "training loss = 7.152580738067627\n",
      "validation loss = 7.386917766771819\n",
      "Batch 66 / 157\n",
      "training loss = 7.341560363769531\n",
      "validation loss = 7.380427862468519\n",
      "Batch 67 / 157\n",
      "training loss = 7.668310165405273\n",
      "validation loss = 7.362195090243691\n",
      "Batch 68 / 157\n",
      "training loss = 7.526426792144775\n",
      "validation loss = 7.3623937054684285\n",
      "Batch 69 / 157\n",
      "training loss = 7.449027061462402\n",
      "validation loss = 7.358748235200581\n",
      "Batch 70 / 157\n",
      "training loss = 7.621992588043213\n",
      "validation loss = 7.349208279659874\n",
      "Batch 71 / 157\n",
      "training loss = 7.401934623718262\n",
      "validation loss = 7.354564365587737\n",
      "Batch 72 / 157\n",
      "training loss = 7.400730609893799\n",
      "validation loss = 7.348155623988101\n",
      "Batch 73 / 157\n",
      "training loss = 7.494166374206543\n",
      "validation loss = 7.335494970020495\n",
      "Batch 74 / 157\n",
      "training loss = 7.408071517944336\n",
      "validation loss = 7.346079575388055\n",
      "Batch 75 / 157\n",
      "training loss = 7.037724494934082\n",
      "validation loss = 7.347725316097862\n",
      "Batch 76 / 157\n",
      "training loss = 7.688724040985107\n",
      "validation loss = 7.348977239508378\n",
      "Batch 77 / 157\n",
      "training loss = 7.440913200378418\n",
      "validation loss = 7.333380448190789\n",
      "Batch 78 / 157\n",
      "training loss = 7.054841041564941\n",
      "validation loss = 7.331884434348659\n",
      "Batch 79 / 157\n",
      "training loss = 7.556364059448242\n",
      "validation loss = 7.324497498964009\n",
      "Batch 80 / 157\n",
      "training loss = 7.3856353759765625\n",
      "validation loss = 7.332628275218763\n",
      "Batch 81 / 157\n",
      "training loss = 7.72046422958374\n",
      "validation loss = 7.326985836029053\n",
      "Batch 82 / 157\n",
      "training loss = 7.366174697875977\n",
      "validation loss = 7.31221668343795\n",
      "Batch 83 / 157\n",
      "training loss = 7.347703456878662\n",
      "validation loss = 7.320950809277986\n",
      "Batch 84 / 157\n",
      "training loss = 7.5470991134643555\n",
      "validation loss = 7.3166403017546005\n",
      "Batch 85 / 157\n",
      "training loss = 7.196540832519531\n",
      "validation loss = 7.308637719405325\n",
      "Batch 86 / 157\n",
      "training loss = 7.250576972961426\n",
      "validation loss = 7.303205038371839\n",
      "Batch 87 / 157\n",
      "training loss = 7.583889961242676\n",
      "validation loss = 7.302256257910478\n",
      "Batch 88 / 157\n",
      "training loss = 7.3511199951171875\n",
      "validation loss = 7.2866647870917065\n",
      "Batch 89 / 157\n",
      "training loss = 7.300426959991455\n",
      "validation loss = 7.296107392562063\n",
      "Batch 90 / 157\n",
      "training loss = 7.291593551635742\n",
      "validation loss = 7.29797907879478\n",
      "Batch 91 / 157\n",
      "training loss = 7.111789703369141\n",
      "validation loss = 7.279072736438952\n",
      "Batch 92 / 157\n",
      "training loss = 7.550817966461182\n",
      "validation loss = 7.279951672804983\n",
      "Batch 93 / 157\n",
      "training loss = 7.104865074157715\n",
      "validation loss = 7.2803750791047745\n",
      "Batch 94 / 157\n",
      "training loss = 7.058355331420898\n",
      "validation loss = 7.271874929729261\n",
      "Batch 95 / 157\n",
      "training loss = 7.207291603088379\n",
      "validation loss = 7.27910496059217\n",
      "Batch 96 / 157\n",
      "training loss = 7.12205696105957\n",
      "validation loss = 7.274159883197985\n",
      "Batch 97 / 157\n",
      "training loss = 7.160032272338867\n",
      "validation loss = 7.2597002732126334\n",
      "Batch 98 / 157\n",
      "training loss = 7.329424858093262\n",
      "validation loss = 7.255316859797428\n",
      "Batch 99 / 157\n",
      "training loss = 7.342700958251953\n",
      "validation loss = 7.268011770750347\n",
      "Batch 100 / 157\n",
      "training loss = 7.231503963470459\n",
      "validation loss = 7.256084843685753\n",
      "Batch 101 / 157\n",
      "training loss = 7.304293632507324\n",
      "validation loss = 7.24307750400744\n",
      "Batch 102 / 157\n",
      "training loss = 7.317993640899658\n",
      "validation loss = 7.255211027044999\n",
      "Batch 103 / 157\n",
      "training loss = 7.407632827758789\n",
      "validation loss = 7.254681386445698\n",
      "Batch 104 / 157\n",
      "training loss = 7.198392868041992\n",
      "validation loss = 7.254942768498471\n",
      "Batch 105 / 157\n",
      "training loss = 7.13383674621582\n",
      "validation loss = 7.242879993037174\n",
      "Batch 106 / 157\n",
      "training loss = 7.066888332366943\n",
      "validation loss = 7.2338683730677555\n",
      "Batch 107 / 157\n",
      "training loss = 7.46418571472168\n",
      "validation loss = 7.2405299638447005\n",
      "Batch 108 / 157\n",
      "training loss = 7.292171955108643\n",
      "validation loss = 7.235086039492958\n",
      "Batch 109 / 157\n",
      "training loss = 7.16523551940918\n",
      "validation loss = 7.227006435394287\n",
      "Batch 110 / 157\n",
      "training loss = 7.428691387176514\n",
      "validation loss = 7.224857857352809\n",
      "Batch 111 / 157\n",
      "training loss = 7.16217565536499\n",
      "validation loss = 7.222741302691008\n",
      "Batch 112 / 157\n",
      "training loss = 7.426955699920654\n",
      "validation loss = 7.211865425109863\n",
      "Batch 113 / 157\n",
      "training loss = 7.261211395263672\n",
      "validation loss = 7.209418121137117\n",
      "Batch 114 / 157\n",
      "training loss = 7.127885818481445\n",
      "validation loss = 7.207924893027858\n",
      "Batch 115 / 157\n",
      "training loss = 7.380706787109375\n",
      "validation loss = 7.204569967169511\n",
      "Batch 116 / 157\n",
      "training loss = 7.27890157699585\n",
      "validation loss = 7.1944300250003215\n",
      "Batch 117 / 157\n",
      "training loss = 7.08749532699585\n",
      "validation loss = 7.19241654245477\n",
      "Batch 118 / 157\n",
      "training loss = 7.183112144470215\n",
      "validation loss = 7.1898956298828125\n",
      "Batch 119 / 157\n",
      "training loss = 7.218280792236328\n",
      "validation loss = 7.1883972820482755\n",
      "Batch 120 / 157\n",
      "training loss = 7.147275924682617\n",
      "validation loss = 7.185926964408473\n",
      "Batch 121 / 157\n",
      "training loss = 7.18157434463501\n",
      "validation loss = 7.175013015144749\n",
      "Batch 122 / 157\n",
      "training loss = 7.230924606323242\n",
      "validation loss = 7.176107607389751\n",
      "Batch 123 / 157\n",
      "training loss = 7.085330009460449\n",
      "validation loss = 7.169285573457417\n",
      "Batch 124 / 157\n",
      "training loss = 7.077418327331543\n",
      "validation loss = 7.176536911412289\n",
      "Batch 125 / 157\n",
      "training loss = 6.875149726867676\n",
      "validation loss = 7.163131287223415\n",
      "Batch 126 / 157\n",
      "training loss = 6.997442245483398\n",
      "validation loss = 7.162104531338341\n",
      "Batch 127 / 157\n",
      "training loss = 6.973517894744873\n",
      "validation loss = 7.1621123113130265\n",
      "Batch 128 / 157\n",
      "training loss = 7.283669948577881\n",
      "validation loss = 7.177292748501427\n",
      "Batch 129 / 157\n",
      "training loss = 6.981889247894287\n",
      "validation loss = 7.151177381214342\n",
      "Batch 130 / 157\n",
      "training loss = 6.799189567565918\n",
      "validation loss = 7.165870089279978\n",
      "Batch 131 / 157\n",
      "training loss = 7.080765247344971\n",
      "validation loss = 7.163939049369411\n",
      "Batch 132 / 157\n",
      "training loss = 7.100721836090088\n",
      "validation loss = 7.16228545339484\n",
      "Batch 133 / 157\n",
      "training loss = 7.256501197814941\n",
      "validation loss = 7.154326890644274\n",
      "Batch 134 / 157\n",
      "training loss = 7.2137980461120605\n",
      "validation loss = 7.1571353611193205\n",
      "Batch 135 / 157\n",
      "training loss = 7.127624034881592\n",
      "validation loss = 7.158328357495759\n",
      "Batch 136 / 157\n",
      "training loss = 7.278412342071533\n",
      "validation loss = 7.156714489585475\n",
      "Batch 137 / 157\n",
      "training loss = 6.893789291381836\n",
      "validation loss = 7.14263747867785\n",
      "Batch 138 / 157\n",
      "training loss = 7.288751602172852\n",
      "validation loss = 7.142773377267938\n",
      "Batch 139 / 157\n",
      "training loss = 7.083291053771973\n",
      "validation loss = 7.142622972789564\n",
      "Batch 140 / 157\n",
      "training loss = 7.4027814865112305\n",
      "validation loss = 7.1404439273633455\n",
      "Batch 141 / 157\n",
      "training loss = 7.0824079513549805\n",
      "validation loss = 7.13218355178833\n",
      "Batch 142 / 157\n",
      "training loss = 7.000704765319824\n",
      "validation loss = 7.125234854848761\n",
      "Batch 143 / 157\n",
      "training loss = 7.141265392303467\n",
      "validation loss = 7.128752457468133\n",
      "Batch 144 / 157\n",
      "training loss = 7.051547050476074\n",
      "validation loss = 7.1250691413879395\n",
      "Batch 145 / 157\n",
      "training loss = 6.955612659454346\n",
      "validation loss = 7.120798788572612\n",
      "Batch 146 / 157\n",
      "training loss = 7.119115829467773\n",
      "validation loss = 7.113894839035837\n",
      "Batch 147 / 157\n",
      "training loss = 6.566652297973633\n",
      "validation loss = 7.122482174321225\n",
      "Batch 148 / 157\n",
      "training loss = 7.264773845672607\n",
      "validation loss = 7.11867954856471\n",
      "Batch 149 / 157\n",
      "training loss = 7.0020904541015625\n",
      "validation loss = 7.114758391129343\n",
      "Batch 150 / 157\n",
      "training loss = 6.935277938842773\n",
      "validation loss = 7.112198628877339\n",
      "Batch 151 / 157\n",
      "training loss = 7.140106201171875\n",
      "validation loss = 7.11132782383969\n",
      "Batch 152 / 157\n",
      "training loss = 6.660314083099365\n",
      "validation loss = 7.109070476732756\n",
      "Batch 153 / 157\n",
      "training loss = 6.978407859802246\n",
      "validation loss = 7.099399365876851\n",
      "Batch 154 / 157\n",
      "training loss = 7.160898685455322\n",
      "validation loss = 7.094585669668097\n",
      "Batch 155 / 157\n",
      "training loss = 6.953302383422852\n",
      "validation loss = 7.0987929043016935\n",
      "Batch 156 / 157\n",
      "training loss = 7.0736894607543945\n",
      "validation loss = 7.086353000841643\n",
      "Batch 157 / 157\n",
      "training loss = 7.091804027557373\n",
      "validation loss = 7.099490642547607\n",
      "Average training loss: 7.43\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.387406349182129\n",
      "validation loss = 10.175460664849533\n",
      "Batch 2 / 157\n",
      "training loss = 10.154470443725586\n",
      "validation loss = 9.750188576547723\n",
      "Batch 3 / 157\n",
      "training loss = 9.735849380493164\n",
      "validation loss = 9.406860702916196\n",
      "Batch 4 / 157\n",
      "training loss = 9.469049453735352\n",
      "validation loss = 9.066570081208882\n",
      "Batch 5 / 157\n",
      "training loss = 9.108949661254883\n",
      "validation loss = 8.755227038734837\n",
      "Batch 6 / 157\n",
      "training loss = 8.732958793640137\n",
      "validation loss = 8.486373399433337\n",
      "Batch 7 / 157\n",
      "training loss = 8.559350967407227\n",
      "validation loss = 8.27450062099256\n",
      "Batch 8 / 157\n",
      "training loss = 8.435998916625977\n",
      "validation loss = 8.14126908151727\n",
      "Batch 9 / 157\n",
      "training loss = 7.966432571411133\n",
      "validation loss = 8.071221251236764\n",
      "Batch 10 / 157\n",
      "training loss = 8.000540733337402\n",
      "validation loss = 8.0585987944352\n",
      "Batch 11 / 157\n",
      "training loss = 8.06941032409668\n",
      "validation loss = 8.093580522035298\n",
      "Batch 12 / 157\n",
      "training loss = 8.203439712524414\n",
      "validation loss = 8.098380716223465\n",
      "Batch 13 / 157\n",
      "training loss = 8.336664199829102\n",
      "validation loss = 8.042817843587775\n",
      "Batch 14 / 157\n",
      "training loss = 8.20215129852295\n",
      "validation loss = 8.094584389736777\n",
      "Batch 15 / 157\n",
      "training loss = 7.962096691131592\n",
      "validation loss = 7.950455163654528\n",
      "Batch 16 / 157\n",
      "training loss = 7.716852188110352\n",
      "validation loss = 7.9385864609166195\n",
      "Batch 17 / 157\n",
      "training loss = 7.629300594329834\n",
      "validation loss = 7.935199511678595\n",
      "Batch 18 / 157\n",
      "training loss = 7.642965316772461\n",
      "validation loss = 7.964653868424265\n",
      "Batch 19 / 157\n",
      "training loss = 8.060026168823242\n",
      "validation loss = 7.936103620027241\n",
      "Batch 20 / 157\n",
      "training loss = 8.091970443725586\n",
      "validation loss = 7.906401333055999\n",
      "Batch 21 / 157\n",
      "training loss = 7.862660884857178\n",
      "validation loss = 7.898540296052632\n",
      "Batch 22 / 157\n",
      "training loss = 7.775545597076416\n",
      "validation loss = 7.905534894842851\n",
      "Batch 23 / 157\n",
      "training loss = 7.710895538330078\n",
      "validation loss = 7.895010571730764\n",
      "Batch 24 / 157\n",
      "training loss = 8.110640525817871\n",
      "validation loss = 7.889238232060483\n",
      "Batch 25 / 157\n",
      "training loss = 7.986606597900391\n",
      "validation loss = 7.886180752202084\n",
      "Batch 26 / 157\n",
      "training loss = 8.124640464782715\n",
      "validation loss = 7.872968247062282\n",
      "Batch 27 / 157\n",
      "training loss = 8.085766792297363\n",
      "validation loss = 7.875358957993357\n",
      "Batch 28 / 157\n",
      "training loss = 7.716024875640869\n",
      "validation loss = 7.856735806716116\n",
      "Batch 29 / 157\n",
      "training loss = 7.779466152191162\n",
      "validation loss = 7.844872976604261\n",
      "Batch 30 / 157\n",
      "training loss = 7.620332717895508\n",
      "validation loss = 7.85054161674098\n",
      "Batch 31 / 157\n",
      "training loss = 7.672952651977539\n",
      "validation loss = 7.819400310516357\n",
      "Batch 32 / 157\n",
      "training loss = 8.15519905090332\n",
      "validation loss = 7.823175781651547\n",
      "Batch 33 / 157\n",
      "training loss = 7.93157434463501\n",
      "validation loss = 7.792446814085308\n",
      "Batch 34 / 157\n",
      "training loss = 7.9718170166015625\n",
      "validation loss = 7.758324397237677\n",
      "Batch 35 / 157\n",
      "training loss = 7.65399694442749\n",
      "validation loss = 7.765187439165618\n",
      "Batch 36 / 157\n",
      "training loss = 7.939029693603516\n",
      "validation loss = 7.736068324038857\n",
      "Batch 37 / 157\n",
      "training loss = 7.60506010055542\n",
      "validation loss = 7.736911096070942\n",
      "Batch 38 / 157\n",
      "training loss = 7.554961681365967\n",
      "validation loss = 7.7147318689446704\n",
      "Batch 39 / 157\n",
      "training loss = 7.550418853759766\n",
      "validation loss = 7.743640222047505\n",
      "Batch 40 / 157\n",
      "training loss = 7.59356164932251\n",
      "validation loss = 7.7130075755872225\n",
      "Batch 41 / 157\n",
      "training loss = 7.813312530517578\n",
      "validation loss = 7.849861170116224\n",
      "Batch 42 / 157\n",
      "training loss = 7.7211151123046875\n",
      "validation loss = 7.830129548123009\n",
      "Batch 43 / 157\n",
      "training loss = 7.710928916931152\n",
      "validation loss = 7.889975397210372\n",
      "Batch 44 / 157\n",
      "training loss = 7.8443803787231445\n",
      "validation loss = 7.819457882329037\n",
      "Batch 45 / 157\n",
      "training loss = 8.183653831481934\n",
      "validation loss = 7.7759662427400285\n",
      "Batch 46 / 157\n",
      "training loss = 7.888940334320068\n",
      "validation loss = 7.857071801235802\n",
      "Batch 47 / 157\n",
      "training loss = 8.420825958251953\n",
      "validation loss = 7.96675948092812\n",
      "Batch 48 / 157\n",
      "training loss = 7.89410400390625\n",
      "validation loss = 7.879491529966655\n",
      "Batch 49 / 157\n",
      "training loss = 7.851099967956543\n",
      "validation loss = 7.788167928394518\n",
      "Batch 50 / 157\n",
      "training loss = 7.5417256355285645\n",
      "validation loss = 7.8021675160056665\n",
      "Batch 51 / 157\n",
      "training loss = 7.554041862487793\n",
      "validation loss = 7.861606673190468\n",
      "Batch 52 / 157\n",
      "training loss = 7.525890350341797\n",
      "validation loss = 7.885237417723003\n",
      "Batch 53 / 157\n",
      "training loss = 7.944879055023193\n",
      "validation loss = 7.84048574849179\n",
      "Batch 54 / 157\n",
      "training loss = 7.7550554275512695\n",
      "validation loss = 7.778705345956903\n",
      "Batch 55 / 157\n",
      "training loss = 7.676114559173584\n",
      "validation loss = 7.730724937037418\n",
      "Batch 56 / 157\n",
      "training loss = 7.611386299133301\n",
      "validation loss = 7.733109373795359\n",
      "Batch 57 / 157\n",
      "training loss = 7.649956703186035\n",
      "validation loss = 7.750265899457429\n",
      "Batch 58 / 157\n",
      "training loss = 7.839277267456055\n",
      "validation loss = 7.750404483393619\n",
      "Batch 59 / 157\n",
      "training loss = 7.975237846374512\n",
      "validation loss = 7.730412156958329\n",
      "Batch 60 / 157\n",
      "training loss = 7.4413957595825195\n",
      "validation loss = 7.705140440087569\n",
      "Batch 61 / 157\n",
      "training loss = 7.445402145385742\n",
      "validation loss = 7.680357933044434\n",
      "Batch 62 / 157\n",
      "training loss = 7.945217132568359\n",
      "validation loss = 7.665822355370772\n",
      "Batch 63 / 157\n",
      "training loss = 7.491474628448486\n",
      "validation loss = 7.662116452267296\n",
      "Batch 64 / 157\n",
      "training loss = 7.714740753173828\n",
      "validation loss = 7.647271331987883\n",
      "Batch 65 / 157\n",
      "training loss = 7.627645969390869\n",
      "validation loss = 7.6319842087595084\n",
      "Batch 66 / 157\n",
      "training loss = 7.671163082122803\n",
      "validation loss = 7.637190040789153\n",
      "Batch 67 / 157\n",
      "training loss = 7.973795413970947\n",
      "validation loss = 7.646019609350907\n",
      "Batch 68 / 157\n",
      "training loss = 7.396332740783691\n",
      "validation loss = 7.6090570500022485\n",
      "Batch 69 / 157\n",
      "training loss = 7.090249538421631\n",
      "validation loss = 7.611153652793483\n",
      "Batch 70 / 157\n",
      "training loss = 7.530341625213623\n",
      "validation loss = 7.632189650284617\n",
      "Batch 71 / 157\n",
      "training loss = 7.610220432281494\n",
      "validation loss = 7.640450653276946\n",
      "Batch 72 / 157\n",
      "training loss = 7.187536716461182\n",
      "validation loss = 7.616788261815121\n",
      "Batch 73 / 157\n",
      "training loss = 7.514979362487793\n",
      "validation loss = 7.578942800823011\n",
      "Batch 74 / 157\n",
      "training loss = 7.630894660949707\n",
      "validation loss = 7.567014116989939\n",
      "Batch 75 / 157\n",
      "training loss = 7.82025671005249\n",
      "validation loss = 7.591799986989875\n",
      "Batch 76 / 157\n",
      "training loss = 7.414386749267578\n",
      "validation loss = 7.572817601655659\n",
      "Batch 77 / 157\n",
      "training loss = 7.686553478240967\n",
      "validation loss = 7.561384502210115\n",
      "Batch 78 / 157\n",
      "training loss = 7.672722816467285\n",
      "validation loss = 7.540162864484285\n",
      "Batch 79 / 157\n",
      "training loss = 7.830349922180176\n",
      "validation loss = 7.543969505711606\n",
      "Batch 80 / 157\n",
      "training loss = 7.319095134735107\n",
      "validation loss = 7.549926155491879\n",
      "Batch 81 / 157\n",
      "training loss = 7.964945316314697\n",
      "validation loss = 7.513936168269107\n",
      "Batch 82 / 157\n",
      "training loss = 7.50909948348999\n",
      "validation loss = 7.525633611177144\n",
      "Batch 83 / 157\n",
      "training loss = 7.375583171844482\n",
      "validation loss = 7.523090663709138\n",
      "Batch 84 / 157\n",
      "training loss = 7.360383987426758\n",
      "validation loss = 7.523482222306101\n",
      "Batch 85 / 157\n",
      "training loss = 7.558382987976074\n",
      "validation loss = 7.5311150550842285\n",
      "Batch 86 / 157\n",
      "training loss = 7.299063205718994\n",
      "validation loss = 7.530310605701647\n",
      "Batch 87 / 157\n",
      "training loss = 7.4103569984436035\n",
      "validation loss = 7.506761048969469\n",
      "Batch 88 / 157\n",
      "training loss = 7.497191429138184\n",
      "validation loss = 7.503262143386038\n",
      "Batch 89 / 157\n",
      "training loss = 7.266414165496826\n",
      "validation loss = 7.488286118758352\n",
      "Batch 90 / 157\n",
      "training loss = 7.919424057006836\n",
      "validation loss = 7.489818547901354\n",
      "Batch 91 / 157\n",
      "training loss = 7.5682759284973145\n",
      "validation loss = 7.487310158578973\n",
      "Batch 92 / 157\n",
      "training loss = 7.362863540649414\n",
      "validation loss = 7.493077905554521\n",
      "Batch 93 / 157\n",
      "training loss = 7.492450714111328\n",
      "validation loss = 7.485292259015535\n",
      "Batch 94 / 157\n",
      "training loss = 7.463984489440918\n",
      "validation loss = 7.518996489675422\n",
      "Batch 95 / 157\n",
      "training loss = 7.954071521759033\n",
      "validation loss = 7.592138968015972\n",
      "Batch 96 / 157\n",
      "training loss = 7.367513656616211\n",
      "validation loss = 7.588677581987883\n",
      "Batch 97 / 157\n",
      "training loss = 7.7560248374938965\n",
      "validation loss = 7.546195130599172\n",
      "Batch 98 / 157\n",
      "training loss = 7.829741477966309\n",
      "validation loss = 7.528074741363525\n",
      "Batch 99 / 157\n",
      "training loss = 7.692765235900879\n",
      "validation loss = 7.516385856427644\n",
      "Batch 100 / 157\n",
      "training loss = 7.806379318237305\n",
      "validation loss = 7.520533561706543\n",
      "Batch 101 / 157\n",
      "training loss = 7.617144584655762\n",
      "validation loss = 7.549859348096345\n",
      "Batch 102 / 157\n",
      "training loss = 7.6475138664245605\n",
      "validation loss = 7.5371065390737435\n",
      "Batch 103 / 157\n",
      "training loss = 7.316765785217285\n",
      "validation loss = 7.504940810956453\n",
      "Batch 104 / 157\n",
      "training loss = 7.756976127624512\n",
      "validation loss = 7.495952932458175\n",
      "Batch 105 / 157\n",
      "training loss = 7.9717583656311035\n",
      "validation loss = 7.506029580768786\n",
      "Batch 106 / 157\n",
      "training loss = 7.66020393371582\n",
      "validation loss = 7.496172001487331\n",
      "Batch 107 / 157\n",
      "training loss = 7.756689071655273\n",
      "validation loss = 7.50309607857152\n",
      "Batch 108 / 157\n",
      "training loss = 7.412064552307129\n",
      "validation loss = 7.507001726250899\n",
      "Batch 109 / 157\n",
      "training loss = 7.428506851196289\n",
      "validation loss = 7.492441604011937\n",
      "Batch 110 / 157\n",
      "training loss = 7.352967262268066\n",
      "validation loss = 7.495953986519261\n",
      "Batch 111 / 157\n",
      "training loss = 7.5116682052612305\n",
      "validation loss = 7.493407249450684\n",
      "Batch 112 / 157\n",
      "training loss = 7.46813440322876\n",
      "validation loss = 7.47155217120522\n",
      "Batch 113 / 157\n",
      "training loss = 7.386453151702881\n",
      "validation loss = 7.46948400296663\n",
      "Batch 114 / 157\n",
      "training loss = 7.379992961883545\n",
      "validation loss = 7.477380426306474\n",
      "Batch 115 / 157\n",
      "training loss = 7.39926290512085\n",
      "validation loss = 7.474574340017218\n",
      "Batch 116 / 157\n",
      "training loss = 7.29252815246582\n",
      "validation loss = 7.464739824596204\n",
      "Batch 117 / 157\n",
      "training loss = 7.695858001708984\n",
      "validation loss = 7.469938177811472\n",
      "Batch 118 / 157\n",
      "training loss = 7.366266250610352\n",
      "validation loss = 7.465760356501529\n",
      "Batch 119 / 157\n",
      "training loss = 7.686200141906738\n",
      "validation loss = 7.466953829715126\n",
      "Batch 120 / 157\n",
      "training loss = 7.085018634796143\n",
      "validation loss = 7.4551926914014315\n",
      "Batch 121 / 157\n",
      "training loss = 7.152725696563721\n",
      "validation loss = 7.457471696954024\n",
      "Batch 122 / 157\n",
      "training loss = 7.235006332397461\n",
      "validation loss = 7.457319109063399\n",
      "Batch 123 / 157\n",
      "training loss = 7.410709381103516\n",
      "validation loss = 7.4483240529110555\n",
      "Batch 124 / 157\n",
      "training loss = 7.921943187713623\n",
      "validation loss = 7.45820753197921\n",
      "Batch 125 / 157\n",
      "training loss = 7.51186466217041\n",
      "validation loss = 7.467007737410696\n",
      "Batch 126 / 157\n",
      "training loss = 7.452129364013672\n",
      "validation loss = 7.4554722183629085\n",
      "Batch 127 / 157\n",
      "training loss = 7.1168718338012695\n",
      "validation loss = 7.457526382647063\n",
      "Batch 128 / 157\n",
      "training loss = 7.389051914215088\n",
      "validation loss = 7.474004092969392\n",
      "Batch 129 / 157\n",
      "training loss = 7.681857109069824\n",
      "validation loss = 7.451733614269056\n",
      "Batch 130 / 157\n",
      "training loss = 7.287435054779053\n",
      "validation loss = 7.445983409881592\n",
      "Batch 131 / 157\n",
      "training loss = 7.507643222808838\n",
      "validation loss = 7.463183653982062\n",
      "Batch 132 / 157\n",
      "training loss = 7.4459004402160645\n",
      "validation loss = 7.4464539226732755\n",
      "Batch 133 / 157\n",
      "training loss = 7.5207648277282715\n",
      "validation loss = 7.430797250647294\n",
      "Batch 134 / 157\n",
      "training loss = 7.0972676277160645\n",
      "validation loss = 7.443904550451982\n",
      "Batch 135 / 157\n",
      "training loss = 7.179001331329346\n",
      "validation loss = 7.445956757194118\n",
      "Batch 136 / 157\n",
      "training loss = 7.116352081298828\n",
      "validation loss = 7.429879188537598\n",
      "Batch 137 / 157\n",
      "training loss = 7.358484268188477\n",
      "validation loss = 7.428187094236675\n",
      "Batch 138 / 157\n",
      "training loss = 7.6425557136535645\n",
      "validation loss = 7.439736692528975\n",
      "Batch 139 / 157\n",
      "training loss = 7.6582818031311035\n",
      "validation loss = 7.426714696382222\n",
      "Batch 140 / 157\n",
      "training loss = 7.668806076049805\n",
      "validation loss = 7.415781497955322\n",
      "Batch 141 / 157\n",
      "training loss = 7.535830020904541\n",
      "validation loss = 7.413062773252788\n",
      "Batch 142 / 157\n",
      "training loss = 7.310414791107178\n",
      "validation loss = 7.421577001872816\n",
      "Batch 143 / 157\n",
      "training loss = 7.313055992126465\n",
      "validation loss = 7.400620535800331\n",
      "Batch 144 / 157\n",
      "training loss = 7.324682712554932\n",
      "validation loss = 7.4092412747834855\n",
      "Batch 145 / 157\n",
      "training loss = 7.528769493103027\n",
      "validation loss = 7.409138428537469\n",
      "Batch 146 / 157\n",
      "training loss = 7.3183064460754395\n",
      "validation loss = 7.4056243896484375\n",
      "Batch 147 / 157\n",
      "training loss = 7.45933723449707\n",
      "validation loss = 7.3983840691415885\n",
      "Batch 148 / 157\n",
      "training loss = 7.58955717086792\n",
      "validation loss = 7.380685931757877\n",
      "Batch 149 / 157\n",
      "training loss = 7.589576244354248\n",
      "validation loss = 7.394993731850072\n",
      "Batch 150 / 157\n",
      "training loss = 7.590541362762451\n",
      "validation loss = 7.421343803405762\n",
      "Batch 151 / 157\n",
      "training loss = 7.282223224639893\n",
      "validation loss = 7.393864054428904\n",
      "Batch 152 / 157\n",
      "training loss = 7.751677513122559\n",
      "validation loss = 7.403028513255872\n",
      "Batch 153 / 157\n",
      "training loss = 7.406840801239014\n",
      "validation loss = 7.409842917793675\n",
      "Batch 154 / 157\n",
      "training loss = 7.233016014099121\n",
      "validation loss = 7.411884533731561\n",
      "Batch 155 / 157\n",
      "training loss = 7.2474365234375\n",
      "validation loss = 7.39899027974982\n",
      "Batch 156 / 157\n",
      "training loss = 7.637186527252197\n",
      "validation loss = 7.403983141246595\n",
      "Batch 157 / 157\n",
      "training loss = 7.462643146514893\n",
      "validation loss = 7.393631859829552\n",
      "Average training loss: 7.71\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.388833045959473\n",
      "validation loss = 10.160220698306436\n",
      "Batch 2 / 157\n",
      "training loss = 10.168707847595215\n",
      "validation loss = 9.664335401434647\n",
      "Batch 3 / 157\n",
      "training loss = 9.673015594482422\n",
      "validation loss = 9.255841656735068\n",
      "Batch 4 / 157\n",
      "training loss = 9.258344650268555\n",
      "validation loss = 8.906980112979287\n",
      "Batch 5 / 157\n",
      "training loss = 8.863619804382324\n",
      "validation loss = 8.603205429880243\n",
      "Batch 6 / 157\n",
      "training loss = 8.4241361618042\n",
      "validation loss = 8.36175803134316\n",
      "Batch 7 / 157\n",
      "training loss = 8.27126407623291\n",
      "validation loss = 8.177144000404759\n",
      "Batch 8 / 157\n",
      "training loss = 8.109517097473145\n",
      "validation loss = 8.055846088810972\n",
      "Batch 9 / 157\n",
      "training loss = 8.126930236816406\n",
      "validation loss = 7.980556111586721\n",
      "Batch 10 / 157\n",
      "training loss = 8.137616157531738\n",
      "validation loss = 7.944192233838533\n",
      "Batch 11 / 157\n",
      "training loss = 7.806251049041748\n",
      "validation loss = 7.921796974382903\n",
      "Batch 12 / 157\n",
      "training loss = 7.966514587402344\n",
      "validation loss = 7.900497812973826\n",
      "Batch 13 / 157\n",
      "training loss = 8.126439094543457\n",
      "validation loss = 7.843710121355559\n",
      "Batch 14 / 157\n",
      "training loss = 7.898728370666504\n",
      "validation loss = 8.06906865772448\n",
      "Batch 15 / 157\n",
      "training loss = 8.138461112976074\n",
      "validation loss = 7.838328236027768\n",
      "Batch 16 / 157\n",
      "training loss = 8.023775100708008\n",
      "validation loss = 7.8892284192537\n",
      "Batch 17 / 157\n",
      "training loss = 8.121947288513184\n",
      "validation loss = 7.8915909716957495\n",
      "Batch 18 / 157\n",
      "training loss = 7.695500373840332\n",
      "validation loss = 7.860657767245644\n",
      "Batch 19 / 157\n",
      "training loss = 7.989867210388184\n",
      "validation loss = 7.816375857905338\n",
      "Batch 20 / 157\n",
      "training loss = 7.618006706237793\n",
      "validation loss = 7.788418016935649\n",
      "Batch 21 / 157\n",
      "training loss = 7.720704555511475\n",
      "validation loss = 7.800530458751478\n",
      "Batch 22 / 157\n",
      "training loss = 7.823512554168701\n",
      "validation loss = 7.820598652488307\n",
      "Batch 23 / 157\n",
      "training loss = 7.7227582931518555\n",
      "validation loss = 7.812786428551925\n",
      "Batch 24 / 157\n",
      "training loss = 7.789724349975586\n",
      "validation loss = 7.789783101332815\n",
      "Batch 25 / 157\n",
      "training loss = 7.729579448699951\n",
      "validation loss = 7.784265794252095\n",
      "Batch 26 / 157\n",
      "training loss = 7.901403427124023\n",
      "validation loss = 7.781797910991468\n",
      "Batch 27 / 157\n",
      "training loss = 7.730093002319336\n",
      "validation loss = 7.780019333488063\n",
      "Batch 28 / 157\n",
      "training loss = 7.624786853790283\n",
      "validation loss = 7.77502283297087\n",
      "Batch 29 / 157\n",
      "training loss = 7.755970001220703\n",
      "validation loss = 7.759937211086876\n",
      "Batch 30 / 157\n",
      "training loss = 7.7397050857543945\n",
      "validation loss = 7.745350059710051\n",
      "Batch 31 / 157\n",
      "training loss = 7.877503395080566\n",
      "validation loss = 7.7425742400319955\n",
      "Batch 32 / 157\n",
      "training loss = 7.553391456604004\n",
      "validation loss = 7.744861828653436\n",
      "Batch 33 / 157\n",
      "training loss = 7.803208351135254\n",
      "validation loss = 7.747678455553557\n",
      "Batch 34 / 157\n",
      "training loss = 7.7530646324157715\n",
      "validation loss = 7.743550677048533\n",
      "Batch 35 / 157\n",
      "training loss = 7.693421363830566\n",
      "validation loss = 7.729887661180999\n",
      "Batch 36 / 157\n",
      "training loss = 7.801986217498779\n",
      "validation loss = 7.7216511023672005\n",
      "Batch 37 / 157\n",
      "training loss = 7.471183776855469\n",
      "validation loss = 7.717293312675075\n",
      "Batch 38 / 157\n",
      "training loss = 7.631579399108887\n",
      "validation loss = 7.715058878848427\n",
      "Batch 39 / 157\n",
      "training loss = 7.767477989196777\n",
      "validation loss = 7.706695757414165\n",
      "Batch 40 / 157\n",
      "training loss = 7.795931339263916\n",
      "validation loss = 7.696885460301449\n",
      "Batch 41 / 157\n",
      "training loss = 7.747478008270264\n",
      "validation loss = 7.703589088038394\n",
      "Batch 42 / 157\n",
      "training loss = 7.604714870452881\n",
      "validation loss = 7.699493408203125\n",
      "Batch 43 / 157\n",
      "training loss = 7.585142612457275\n",
      "validation loss = 7.685861687911184\n",
      "Batch 44 / 157\n",
      "training loss = 7.744992256164551\n",
      "validation loss = 7.689137684671502\n",
      "Batch 45 / 157\n",
      "training loss = 8.085844039916992\n",
      "validation loss = 7.683361655787418\n",
      "Batch 46 / 157\n",
      "training loss = 7.583681106567383\n",
      "validation loss = 7.68059135738172\n",
      "Batch 47 / 157\n",
      "training loss = 7.503026962280273\n",
      "validation loss = 7.68139302103143\n",
      "Batch 48 / 157\n",
      "training loss = 7.982778072357178\n",
      "validation loss = 7.680857482709382\n",
      "Batch 49 / 157\n",
      "training loss = 7.681137561798096\n",
      "validation loss = 7.684421263243022\n",
      "Batch 50 / 157\n",
      "training loss = 7.499781131744385\n",
      "validation loss = 7.678528459448564\n",
      "Batch 51 / 157\n",
      "training loss = 7.6631693840026855\n",
      "validation loss = 7.6767378857261255\n",
      "Batch 52 / 157\n",
      "training loss = 7.475959777832031\n",
      "validation loss = 7.682095402165463\n",
      "Batch 53 / 157\n",
      "training loss = 7.501668453216553\n",
      "validation loss = 7.688105056160374\n",
      "Batch 54 / 157\n",
      "training loss = 7.445321083068848\n",
      "validation loss = 7.678562515660336\n",
      "Batch 55 / 157\n",
      "training loss = 7.788288116455078\n",
      "validation loss = 7.666477429239373\n",
      "Batch 56 / 157\n",
      "training loss = 7.757555961608887\n",
      "validation loss = 7.676580755334151\n",
      "Batch 57 / 157\n",
      "training loss = 7.527217388153076\n",
      "validation loss = 7.665962871752288\n",
      "Batch 58 / 157\n",
      "training loss = 7.594722747802734\n",
      "validation loss = 7.6575586168389576\n",
      "Batch 59 / 157\n",
      "training loss = 7.513822078704834\n",
      "validation loss = 7.66359816099468\n",
      "Batch 60 / 157\n",
      "training loss = 7.6653947830200195\n",
      "validation loss = 7.667169596019544\n",
      "Batch 61 / 157\n",
      "training loss = 7.857660293579102\n",
      "validation loss = 7.6530937144630835\n",
      "Batch 62 / 157\n",
      "training loss = 7.723278999328613\n",
      "validation loss = 7.6442645976417944\n",
      "Batch 63 / 157\n",
      "training loss = 7.875853538513184\n",
      "validation loss = 7.672651416377017\n",
      "Batch 64 / 157\n",
      "training loss = 7.806037425994873\n",
      "validation loss = 7.673219103562205\n",
      "Batch 65 / 157\n",
      "training loss = 7.631288528442383\n",
      "validation loss = 7.63694627661454\n",
      "Batch 66 / 157\n",
      "training loss = 7.675717830657959\n",
      "validation loss = 7.646273362009149\n",
      "Batch 67 / 157\n",
      "training loss = 7.625201225280762\n",
      "validation loss = 7.655214585756001\n",
      "Batch 68 / 157\n",
      "training loss = 7.736171245574951\n",
      "validation loss = 7.645110607147217\n",
      "Batch 69 / 157\n",
      "training loss = 7.847296237945557\n",
      "validation loss = 7.634331954152961\n",
      "Batch 70 / 157\n",
      "training loss = 7.573750972747803\n",
      "validation loss = 7.638193632427015\n",
      "Batch 71 / 157\n",
      "training loss = 7.565563678741455\n",
      "validation loss = 7.64631640283685\n",
      "Batch 72 / 157\n",
      "training loss = 7.812227249145508\n",
      "validation loss = 7.638063631559673\n",
      "Batch 73 / 157\n",
      "training loss = 7.7613959312438965\n",
      "validation loss = 7.623525217959755\n",
      "Batch 74 / 157\n",
      "training loss = 7.575018882751465\n",
      "validation loss = 7.621401485643889\n",
      "Batch 75 / 157\n",
      "training loss = 7.818300247192383\n",
      "validation loss = 7.621308326721191\n",
      "Batch 76 / 157\n",
      "training loss = 7.533511161804199\n",
      "validation loss = 7.61740601690192\n",
      "Batch 77 / 157\n",
      "training loss = 7.777242183685303\n",
      "validation loss = 7.6081620015596085\n",
      "Batch 78 / 157\n",
      "training loss = 7.572182655334473\n",
      "validation loss = 7.607682479055304\n",
      "Batch 79 / 157\n",
      "training loss = 7.488523006439209\n",
      "validation loss = 7.607118907727693\n",
      "Batch 80 / 157\n",
      "training loss = 7.825915813446045\n",
      "validation loss = 7.600071480399684\n",
      "Batch 81 / 157\n",
      "training loss = 7.6490325927734375\n",
      "validation loss = 7.593899601384213\n",
      "Batch 82 / 157\n",
      "training loss = 7.415929317474365\n",
      "validation loss = 7.588354336588006\n",
      "Batch 83 / 157\n",
      "training loss = 7.914113998413086\n",
      "validation loss = 7.582984422382555\n",
      "Batch 84 / 157\n",
      "training loss = 7.536269187927246\n",
      "validation loss = 7.574875731217234\n",
      "Batch 85 / 157\n",
      "training loss = 7.526074409484863\n",
      "validation loss = 7.570985342326917\n",
      "Batch 86 / 157\n",
      "training loss = 7.469690799713135\n",
      "validation loss = 7.558206257067229\n",
      "Batch 87 / 157\n",
      "training loss = 7.757298469543457\n",
      "validation loss = 7.554007555309095\n",
      "Batch 88 / 157\n",
      "training loss = 7.887329578399658\n",
      "validation loss = 7.555752754211426\n",
      "Batch 89 / 157\n",
      "training loss = 7.607570171356201\n",
      "validation loss = 7.548641029157136\n",
      "Batch 90 / 157\n",
      "training loss = 7.378684997558594\n",
      "validation loss = 7.543877124786377\n",
      "Batch 91 / 157\n",
      "training loss = 7.886658668518066\n",
      "validation loss = 7.53783833353143\n",
      "Batch 92 / 157\n",
      "training loss = 7.672966480255127\n",
      "validation loss = 7.540963925813374\n",
      "Batch 93 / 157\n",
      "training loss = 8.274600982666016\n",
      "validation loss = 7.611166853653757\n",
      "Batch 94 / 157\n",
      "training loss = 7.580434322357178\n",
      "validation loss = 7.550433711001747\n",
      "Batch 95 / 157\n",
      "training loss = 7.63128137588501\n",
      "validation loss = 7.584134804575067\n",
      "Batch 96 / 157\n",
      "training loss = 7.754590034484863\n",
      "validation loss = 7.564778804779053\n",
      "Batch 97 / 157\n",
      "training loss = 7.653919219970703\n",
      "validation loss = 7.535994253660503\n",
      "Batch 98 / 157\n",
      "training loss = 7.390402317047119\n",
      "validation loss = 7.535557947660747\n",
      "Batch 99 / 157\n",
      "training loss = 7.628859043121338\n",
      "validation loss = 7.555264146704423\n",
      "Batch 100 / 157\n",
      "training loss = 7.748439311981201\n",
      "validation loss = 7.560217305233604\n",
      "Batch 101 / 157\n",
      "training loss = 7.489171981811523\n",
      "validation loss = 7.535754479860005\n",
      "Batch 102 / 157\n",
      "training loss = 7.579080104827881\n",
      "validation loss = 7.520551430551629\n",
      "Batch 103 / 157\n",
      "training loss = 7.507684707641602\n",
      "validation loss = 7.522433933458831\n",
      "Batch 104 / 157\n",
      "training loss = 7.615137100219727\n",
      "validation loss = 7.529275818874962\n",
      "Batch 105 / 157\n",
      "training loss = 7.562986850738525\n",
      "validation loss = 7.528185543261077\n",
      "Batch 106 / 157\n",
      "training loss = 7.627192974090576\n",
      "validation loss = 7.517960523304186\n",
      "Batch 107 / 157\n",
      "training loss = 7.620884895324707\n",
      "validation loss = 7.509753929941278\n",
      "Batch 108 / 157\n",
      "training loss = 7.2347540855407715\n",
      "validation loss = 7.508105152531674\n",
      "Batch 109 / 157\n",
      "training loss = 7.624182224273682\n",
      "validation loss = 7.509066431145919\n",
      "Batch 110 / 157\n",
      "training loss = 7.604897499084473\n",
      "validation loss = 7.508202075958252\n",
      "Batch 111 / 157\n",
      "training loss = 7.799070835113525\n",
      "validation loss = 7.499488253342478\n",
      "Batch 112 / 157\n",
      "training loss = 7.432788848876953\n",
      "validation loss = 7.489344396089253\n",
      "Batch 113 / 157\n",
      "training loss = 7.460152626037598\n",
      "validation loss = 7.489291793421695\n",
      "Batch 114 / 157\n",
      "training loss = 7.705029010772705\n",
      "validation loss = 7.482678262810958\n",
      "Batch 115 / 157\n",
      "training loss = 7.361282825469971\n",
      "validation loss = 7.483835822657535\n",
      "Batch 116 / 157\n",
      "training loss = 7.620799541473389\n",
      "validation loss = 7.474964970036557\n",
      "Batch 117 / 157\n",
      "training loss = 7.487411975860596\n",
      "validation loss = 7.47616730238262\n",
      "Batch 118 / 157\n",
      "training loss = 7.583982467651367\n",
      "validation loss = 7.462889269778603\n",
      "Batch 119 / 157\n",
      "training loss = 7.313028812408447\n",
      "validation loss = 7.462046673423366\n",
      "Batch 120 / 157\n",
      "training loss = 7.60299015045166\n",
      "validation loss = 7.459665474138762\n",
      "Batch 121 / 157\n",
      "training loss = 7.187795639038086\n",
      "validation loss = 7.457303047180176\n",
      "Batch 122 / 157\n",
      "training loss = 7.274768352508545\n",
      "validation loss = 7.456658589212518\n",
      "Batch 123 / 157\n",
      "training loss = 7.5521464347839355\n",
      "validation loss = 7.452772291083085\n",
      "Batch 124 / 157\n",
      "training loss = 7.5486226081848145\n",
      "validation loss = 7.453626783270585\n",
      "Batch 125 / 157\n",
      "training loss = 7.555047512054443\n",
      "validation loss = 7.448121397118819\n",
      "Batch 126 / 157\n",
      "training loss = 7.4896721839904785\n",
      "validation loss = 7.446372559196071\n",
      "Batch 127 / 157\n",
      "training loss = 7.584422588348389\n",
      "validation loss = 7.44193865123548\n",
      "Batch 128 / 157\n",
      "training loss = 7.2228803634643555\n",
      "validation loss = 7.443123767250462\n",
      "Batch 129 / 157\n",
      "training loss = 7.239126205444336\n",
      "validation loss = 7.439805758626838\n",
      "Batch 130 / 157\n",
      "training loss = 7.5668044090271\n",
      "validation loss = 7.435044338828639\n",
      "Batch 131 / 157\n",
      "training loss = 7.350483417510986\n",
      "validation loss = 7.435571068211606\n",
      "Batch 132 / 157\n",
      "training loss = 7.4407243728637695\n",
      "validation loss = 7.432497400986521\n",
      "Batch 133 / 157\n",
      "training loss = 7.442314624786377\n",
      "validation loss = 7.433135584781044\n",
      "Batch 134 / 157\n",
      "training loss = 7.498579025268555\n",
      "validation loss = 7.433140704506322\n",
      "Batch 135 / 157\n",
      "training loss = 7.562673568725586\n",
      "validation loss = 7.4316934535377905\n",
      "Batch 136 / 157\n",
      "training loss = 7.300006866455078\n",
      "validation loss = 7.431096051868639\n",
      "Batch 137 / 157\n",
      "training loss = 7.426762104034424\n",
      "validation loss = 7.4287042115864\n",
      "Batch 138 / 157\n",
      "training loss = 7.3690876960754395\n",
      "validation loss = 7.427445461875514\n",
      "Batch 139 / 157\n",
      "training loss = 7.175350666046143\n",
      "validation loss = 7.427342816403038\n",
      "Batch 140 / 157\n",
      "training loss = 7.67041540145874\n",
      "validation loss = 7.423062500200774\n",
      "Batch 141 / 157\n",
      "training loss = 8.129678726196289\n",
      "validation loss = 7.436871679205644\n",
      "Batch 142 / 157\n",
      "training loss = 7.701479434967041\n",
      "validation loss = 7.475753131665681\n",
      "Batch 143 / 157\n",
      "training loss = 7.47086238861084\n",
      "validation loss = 7.4448427652057845\n",
      "Batch 144 / 157\n",
      "training loss = 7.6267409324646\n",
      "validation loss = 7.413211822509766\n",
      "Batch 145 / 157\n",
      "training loss = 7.399500846862793\n",
      "validation loss = 7.420225544979698\n",
      "Batch 146 / 157\n",
      "training loss = 7.525635242462158\n",
      "validation loss = 7.422557353973389\n",
      "Batch 147 / 157\n",
      "training loss = 7.6234517097473145\n",
      "validation loss = 7.423818713740299\n",
      "Batch 148 / 157\n",
      "training loss = 7.628741264343262\n",
      "validation loss = 7.410356847863448\n",
      "Batch 149 / 157\n",
      "training loss = 7.3333330154418945\n",
      "validation loss = 7.405518657282779\n",
      "Batch 150 / 157\n",
      "training loss = 7.602433681488037\n",
      "validation loss = 7.410284318422017\n",
      "Batch 151 / 157\n",
      "training loss = 7.547829627990723\n",
      "validation loss = 7.417710404647024\n",
      "Batch 152 / 157\n",
      "training loss = 7.33559513092041\n",
      "validation loss = 7.409475728085167\n",
      "Batch 153 / 157\n",
      "training loss = 7.5399885177612305\n",
      "validation loss = 7.399306974912944\n",
      "Batch 154 / 157\n",
      "training loss = 7.700666427612305\n",
      "validation loss = 7.392129872974596\n",
      "Batch 155 / 157\n",
      "training loss = 7.402671813964844\n",
      "validation loss = 7.391941271330181\n",
      "Batch 156 / 157\n",
      "training loss = 7.551133155822754\n",
      "validation loss = 7.395585787923713\n",
      "Batch 157 / 157\n",
      "training loss = 7.431146144866943\n",
      "validation loss = 7.398131395641126\n",
      "Average training loss: 7.72\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.39437198638916\n",
      "validation loss = 10.257364172684518\n",
      "Batch 2 / 157\n",
      "training loss = 10.265698432922363\n",
      "validation loss = 9.688038022894608\n",
      "Batch 3 / 157\n",
      "training loss = 9.656694412231445\n",
      "validation loss = 9.275415119371916\n",
      "Batch 4 / 157\n",
      "training loss = 9.234899520874023\n",
      "validation loss = 8.939279556274414\n",
      "Batch 5 / 157\n",
      "training loss = 9.022969245910645\n",
      "validation loss = 8.640983782316509\n",
      "Batch 6 / 157\n",
      "training loss = 8.587911605834961\n",
      "validation loss = 8.387079891405607\n",
      "Batch 7 / 157\n",
      "training loss = 8.498740196228027\n",
      "validation loss = 8.187887342352616\n",
      "Batch 8 / 157\n",
      "training loss = 8.275029182434082\n",
      "validation loss = 8.036490967399196\n",
      "Batch 9 / 157\n",
      "training loss = 8.049355506896973\n",
      "validation loss = 7.9245148708945825\n",
      "Batch 10 / 157\n",
      "training loss = 7.913747787475586\n",
      "validation loss = 7.842641755154259\n",
      "Batch 11 / 157\n",
      "training loss = 7.798558235168457\n",
      "validation loss = 7.795214351854827\n",
      "Batch 12 / 157\n",
      "training loss = 7.687314987182617\n",
      "validation loss = 7.780085162112587\n",
      "Batch 13 / 157\n",
      "training loss = 7.6972336769104\n",
      "validation loss = 7.771587447116249\n",
      "Batch 14 / 157\n",
      "training loss = 7.818965911865234\n",
      "validation loss = 7.763627353467439\n",
      "Batch 15 / 157\n",
      "training loss = 7.7018890380859375\n",
      "validation loss = 7.742538627825286\n",
      "Batch 16 / 157\n",
      "training loss = 8.029638290405273\n",
      "validation loss = 7.747779243870785\n",
      "Batch 17 / 157\n",
      "training loss = 7.54622220993042\n",
      "validation loss = 7.73492599788465\n",
      "Batch 18 / 157\n",
      "training loss = 8.032044410705566\n",
      "validation loss = 7.723358806810881\n",
      "Batch 19 / 157\n",
      "training loss = 7.7390899658203125\n",
      "validation loss = 7.707961283232036\n",
      "Batch 20 / 157\n",
      "training loss = 7.7901153564453125\n",
      "validation loss = 7.692272236472682\n",
      "Batch 21 / 157\n",
      "training loss = 7.539798259735107\n",
      "validation loss = 7.6880646755820825\n",
      "Batch 22 / 157\n",
      "training loss = 7.656375885009766\n",
      "validation loss = 7.6738333451120475\n",
      "Batch 23 / 157\n",
      "training loss = 7.801881790161133\n",
      "validation loss = 7.667510032653809\n",
      "Batch 24 / 157\n",
      "training loss = 7.68383264541626\n",
      "validation loss = 7.659377524727269\n",
      "Batch 25 / 157\n",
      "training loss = 7.605248928070068\n",
      "validation loss = 7.644871586247494\n",
      "Batch 26 / 157\n",
      "training loss = 7.886658668518066\n",
      "validation loss = 7.630692080447548\n",
      "Batch 27 / 157\n",
      "training loss = 7.21408224105835\n",
      "validation loss = 7.6241323320489185\n",
      "Batch 28 / 157\n",
      "training loss = 7.571307182312012\n",
      "validation loss = 7.616851781543932\n",
      "Batch 29 / 157\n",
      "training loss = 7.5147504806518555\n",
      "validation loss = 7.605908293473093\n",
      "Batch 30 / 157\n",
      "training loss = 7.352101802825928\n",
      "validation loss = 7.577578770486932\n",
      "Batch 31 / 157\n",
      "training loss = 7.3227219581604\n",
      "validation loss = 7.579781105643825\n",
      "Batch 32 / 157\n",
      "training loss = 7.618228912353516\n",
      "validation loss = 7.568072394320839\n",
      "Batch 33 / 157\n",
      "training loss = 7.642698764801025\n",
      "validation loss = 7.559803510967054\n",
      "Batch 34 / 157\n",
      "training loss = 7.648879528045654\n",
      "validation loss = 7.539932200783177\n",
      "Batch 35 / 157\n",
      "training loss = 7.481123447418213\n",
      "validation loss = 7.548256146280389\n",
      "Batch 36 / 157\n",
      "training loss = 7.443838119506836\n",
      "validation loss = 7.531396740361264\n",
      "Batch 37 / 157\n",
      "training loss = 7.659613609313965\n",
      "validation loss = 7.5394770722640185\n",
      "Batch 38 / 157\n",
      "training loss = 7.574372291564941\n",
      "validation loss = 7.514513869034617\n",
      "Batch 39 / 157\n",
      "training loss = 7.430245876312256\n",
      "validation loss = 7.52116574739155\n",
      "Batch 40 / 157\n",
      "training loss = 7.640397548675537\n",
      "validation loss = 7.508274429722836\n",
      "Batch 41 / 157\n",
      "training loss = 7.387810230255127\n",
      "validation loss = 7.508622370268169\n",
      "Batch 42 / 157\n",
      "training loss = 7.459765434265137\n",
      "validation loss = 7.517985494513261\n",
      "Batch 43 / 157\n",
      "training loss = 7.495883941650391\n",
      "validation loss = 7.5144570250260205\n",
      "Batch 44 / 157\n",
      "training loss = 7.500445365905762\n",
      "validation loss = 7.497311692488821\n",
      "Batch 45 / 157\n",
      "training loss = 7.319365501403809\n",
      "validation loss = 7.508677231638055\n",
      "Batch 46 / 157\n",
      "training loss = 7.4891839027404785\n",
      "validation loss = 7.492907197851884\n",
      "Batch 47 / 157\n",
      "training loss = 7.424004554748535\n",
      "validation loss = 7.497354783509907\n",
      "Batch 48 / 157\n",
      "training loss = 7.0771098136901855\n",
      "validation loss = 7.484900349064877\n",
      "Batch 49 / 157\n",
      "training loss = 7.456846714019775\n",
      "validation loss = 7.484182684045089\n",
      "Batch 50 / 157\n",
      "training loss = 7.719076633453369\n",
      "validation loss = 7.489850872441342\n",
      "Batch 51 / 157\n",
      "training loss = 7.26734733581543\n",
      "validation loss = 7.482650631352475\n",
      "Batch 52 / 157\n",
      "training loss = 7.2894792556762695\n",
      "validation loss = 7.476024351621929\n",
      "Batch 53 / 157\n",
      "training loss = 7.506252288818359\n",
      "validation loss = 7.466511575799239\n",
      "Batch 54 / 157\n",
      "training loss = 7.565561771392822\n",
      "validation loss = 7.470971132579603\n",
      "Batch 55 / 157\n",
      "training loss = 7.492533206939697\n",
      "validation loss = 7.470236000261809\n",
      "Batch 56 / 157\n",
      "training loss = 7.422957420349121\n",
      "validation loss = 7.459277705142372\n",
      "Batch 57 / 157\n",
      "training loss = 7.342784881591797\n",
      "validation loss = 7.45803541886179\n",
      "Batch 58 / 157\n",
      "training loss = 7.310113430023193\n",
      "validation loss = 7.44950176540174\n",
      "Batch 59 / 157\n",
      "training loss = 7.283596038818359\n",
      "validation loss = 7.45772113298115\n",
      "Batch 60 / 157\n",
      "training loss = 7.211350917816162\n",
      "validation loss = 7.453034175069709\n",
      "Batch 61 / 157\n",
      "training loss = 7.357868194580078\n",
      "validation loss = 7.438845258010061\n",
      "Batch 62 / 157\n",
      "training loss = 7.726687908172607\n",
      "validation loss = 7.431664215890985\n",
      "Batch 63 / 157\n",
      "training loss = 7.51718807220459\n",
      "validation loss = 7.424564813312731\n",
      "Batch 64 / 157\n",
      "training loss = 7.454395294189453\n",
      "validation loss = 7.421160472066779\n",
      "Batch 65 / 157\n",
      "training loss = 7.532970428466797\n",
      "validation loss = 7.427189626191792\n",
      "Batch 66 / 157\n",
      "training loss = 7.444936275482178\n",
      "validation loss = 7.429802593431975\n",
      "Batch 67 / 157\n",
      "training loss = 7.603321552276611\n",
      "validation loss = 7.425685054377506\n",
      "Batch 68 / 157\n",
      "training loss = 7.282596111297607\n",
      "validation loss = 7.4118400372956925\n",
      "Batch 69 / 157\n",
      "training loss = 7.366255283355713\n",
      "validation loss = 7.420655175259239\n",
      "Batch 70 / 157\n",
      "training loss = 7.575486183166504\n",
      "validation loss = 7.426208897640831\n",
      "Batch 71 / 157\n",
      "training loss = 6.9744954109191895\n",
      "validation loss = 7.424575830760755\n",
      "Batch 72 / 157\n",
      "training loss = 7.273318290710449\n",
      "validation loss = 7.4158217279534595\n",
      "Batch 73 / 157\n",
      "training loss = 7.246188163757324\n",
      "validation loss = 7.40806918395193\n",
      "Batch 74 / 157\n",
      "training loss = 7.261251926422119\n",
      "validation loss = 7.403634698767411\n",
      "Batch 75 / 157\n",
      "training loss = 7.55167293548584\n",
      "validation loss = 7.39877186323467\n",
      "Batch 76 / 157\n",
      "training loss = 7.423144340515137\n",
      "validation loss = 7.410382772746839\n",
      "Batch 77 / 157\n",
      "training loss = 7.581466197967529\n",
      "validation loss = 7.4129025810643245\n",
      "Batch 78 / 157\n",
      "training loss = 7.27431058883667\n",
      "validation loss = 7.386876884259675\n",
      "Batch 79 / 157\n",
      "training loss = 7.5338873863220215\n",
      "validation loss = 7.387860448736894\n",
      "Batch 80 / 157\n",
      "training loss = 7.363814830780029\n",
      "validation loss = 7.389869213104248\n",
      "Batch 81 / 157\n",
      "training loss = 7.36100435256958\n",
      "validation loss = 7.38633216054816\n",
      "Batch 82 / 157\n",
      "training loss = 7.506749629974365\n",
      "validation loss = 7.399511588247199\n",
      "Batch 83 / 157\n",
      "training loss = 7.28978967666626\n",
      "validation loss = 7.383184458080091\n",
      "Batch 84 / 157\n",
      "training loss = 7.227291584014893\n",
      "validation loss = 7.388760667098196\n",
      "Batch 85 / 157\n",
      "training loss = 7.530093193054199\n",
      "validation loss = 7.37953361712004\n",
      "Batch 86 / 157\n",
      "training loss = 7.305558204650879\n",
      "validation loss = 7.368154801820454\n",
      "Batch 87 / 157\n",
      "training loss = 7.273311614990234\n",
      "validation loss = 7.368463365655196\n",
      "Batch 88 / 157\n",
      "training loss = 7.196047306060791\n",
      "validation loss = 7.362465356525622\n",
      "Batch 89 / 157\n",
      "training loss = 7.302867412567139\n",
      "validation loss = 7.363956677286248\n",
      "Batch 90 / 157\n",
      "training loss = 7.155376434326172\n",
      "validation loss = 7.358577276531019\n",
      "Batch 91 / 157\n",
      "training loss = 7.184837818145752\n",
      "validation loss = 7.371562154669511\n",
      "Batch 92 / 157\n",
      "training loss = 7.409428596496582\n",
      "validation loss = 7.364534152181525\n",
      "Batch 93 / 157\n",
      "training loss = 7.5307159423828125\n",
      "validation loss = 7.351271102302952\n",
      "Batch 94 / 157\n",
      "training loss = 7.313697338104248\n",
      "validation loss = 7.343638595781829\n",
      "Batch 95 / 157\n",
      "training loss = 7.276558876037598\n",
      "validation loss = 7.3435975626895305\n",
      "Batch 96 / 157\n",
      "training loss = 7.44796085357666\n",
      "validation loss = 7.333935988576789\n",
      "Batch 97 / 157\n",
      "training loss = 7.396679401397705\n",
      "validation loss = 7.3489886334067895\n",
      "Batch 98 / 157\n",
      "training loss = 7.312012195587158\n",
      "validation loss = 7.339749687596371\n",
      "Batch 99 / 157\n",
      "training loss = 7.4838056564331055\n",
      "validation loss = 7.346020723644056\n",
      "Batch 100 / 157\n",
      "training loss = 6.9497246742248535\n",
      "validation loss = 7.329371853878624\n",
      "Batch 101 / 157\n",
      "training loss = 7.240968704223633\n",
      "validation loss = 7.341929009086208\n",
      "Batch 102 / 157\n",
      "training loss = 7.411431789398193\n",
      "validation loss = 7.334860902083547\n",
      "Batch 103 / 157\n",
      "training loss = 7.308734893798828\n",
      "validation loss = 7.3337077843515495\n",
      "Batch 104 / 157\n",
      "training loss = 7.198554515838623\n",
      "validation loss = 7.328633534280877\n",
      "Batch 105 / 157\n",
      "training loss = 7.39578104019165\n",
      "validation loss = 7.332477669966848\n",
      "Batch 106 / 157\n",
      "training loss = 7.391093730926514\n",
      "validation loss = 7.341848273026316\n",
      "Batch 107 / 157\n",
      "training loss = 7.177132606506348\n",
      "validation loss = 7.328362490001478\n",
      "Batch 108 / 157\n",
      "training loss = 7.501771926879883\n",
      "validation loss = 7.320559125197561\n",
      "Batch 109 / 157\n",
      "training loss = 7.441577911376953\n",
      "validation loss = 7.3240839054709985\n",
      "Batch 110 / 157\n",
      "training loss = 7.391646385192871\n",
      "validation loss = 7.317485382682399\n",
      "Batch 111 / 157\n",
      "training loss = 7.23307466506958\n",
      "validation loss = 7.316192200309352\n",
      "Batch 112 / 157\n",
      "training loss = 7.392200469970703\n",
      "validation loss = 7.315123206690738\n",
      "Batch 113 / 157\n",
      "training loss = 7.147294998168945\n",
      "validation loss = 7.310179308841103\n",
      "Batch 114 / 157\n",
      "training loss = 7.444092750549316\n",
      "validation loss = 7.314838735680831\n",
      "Batch 115 / 157\n",
      "training loss = 7.028835773468018\n",
      "validation loss = 7.308284634038022\n",
      "Batch 116 / 157\n",
      "training loss = 7.238813400268555\n",
      "validation loss = 7.304930988110994\n",
      "Batch 117 / 157\n",
      "training loss = 7.510739803314209\n",
      "validation loss = 7.303544119784706\n",
      "Batch 118 / 157\n",
      "training loss = 7.330563068389893\n",
      "validation loss = 7.298356608340614\n",
      "Batch 119 / 157\n",
      "training loss = 7.292421340942383\n",
      "validation loss = 7.2976350031400985\n",
      "Batch 120 / 157\n",
      "training loss = 7.189807891845703\n",
      "validation loss = 7.2944590919896175\n",
      "Batch 121 / 157\n",
      "training loss = 7.2904767990112305\n",
      "validation loss = 7.288628929539731\n",
      "Batch 122 / 157\n",
      "training loss = 7.36273717880249\n",
      "validation loss = 7.287557426251863\n",
      "Batch 123 / 157\n",
      "training loss = 7.3318257331848145\n",
      "validation loss = 7.309337515580027\n",
      "Batch 124 / 157\n",
      "training loss = 7.430257320404053\n",
      "validation loss = 7.27580303894846\n",
      "Batch 125 / 157\n",
      "training loss = 6.960721015930176\n",
      "validation loss = 7.2983192142687345\n",
      "Batch 126 / 157\n",
      "training loss = 7.2570366859436035\n",
      "validation loss = 7.28710412979126\n",
      "Batch 127 / 157\n",
      "training loss = 7.322932720184326\n",
      "validation loss = 7.294486949318333\n",
      "Batch 128 / 157\n",
      "training loss = 7.300288677215576\n",
      "validation loss = 7.299453685158177\n",
      "Batch 129 / 157\n",
      "training loss = 7.305456638336182\n",
      "validation loss = 7.295589672891717\n",
      "Batch 130 / 157\n",
      "training loss = 7.320106029510498\n",
      "validation loss = 7.284421067488821\n",
      "Batch 131 / 157\n",
      "training loss = 7.356371879577637\n",
      "validation loss = 7.289385870883339\n",
      "Batch 132 / 157\n",
      "training loss = 7.4942097663879395\n",
      "validation loss = 7.280805035641319\n",
      "Batch 133 / 157\n",
      "training loss = 7.271683692932129\n",
      "validation loss = 7.27241523642289\n",
      "Batch 134 / 157\n",
      "training loss = 7.3600006103515625\n",
      "validation loss = 7.286763065739682\n",
      "Batch 135 / 157\n",
      "training loss = 7.508354663848877\n",
      "validation loss = 7.285053880591142\n",
      "Batch 136 / 157\n",
      "training loss = 7.425824165344238\n",
      "validation loss = 7.281814725775468\n",
      "Batch 137 / 157\n",
      "training loss = 7.09756326675415\n",
      "validation loss = 7.265756883119282\n",
      "Batch 138 / 157\n",
      "training loss = 7.286863803863525\n",
      "validation loss = 7.282395061693694\n",
      "Batch 139 / 157\n",
      "training loss = 7.443665027618408\n",
      "validation loss = 7.2661186017488175\n",
      "Batch 140 / 157\n",
      "training loss = 7.214842319488525\n",
      "validation loss = 7.266434343237626\n",
      "Batch 141 / 157\n",
      "training loss = 7.420109272003174\n",
      "validation loss = 7.260941229368511\n",
      "Batch 142 / 157\n",
      "training loss = 7.157222270965576\n",
      "validation loss = 7.2614257461146305\n",
      "Batch 143 / 157\n",
      "training loss = 6.8458099365234375\n",
      "validation loss = 7.249585879476447\n",
      "Batch 144 / 157\n",
      "training loss = 7.1564412117004395\n",
      "validation loss = 7.257511992203562\n",
      "Batch 145 / 157\n",
      "training loss = 7.323504447937012\n",
      "validation loss = 7.253838815187153\n",
      "Batch 146 / 157\n",
      "training loss = 6.910624027252197\n",
      "validation loss = 7.259423381403873\n",
      "Batch 147 / 157\n",
      "training loss = 7.339625358581543\n",
      "validation loss = 7.251545253552888\n",
      "Batch 148 / 157\n",
      "training loss = 7.406379699707031\n",
      "validation loss = 7.250750616977089\n",
      "Batch 149 / 157\n",
      "training loss = 7.526401519775391\n",
      "validation loss = 7.261490947321842\n",
      "Batch 150 / 157\n",
      "training loss = 7.259875297546387\n",
      "validation loss = 7.248903751373291\n",
      "Batch 151 / 157\n",
      "training loss = 7.277553081512451\n",
      "validation loss = 7.247069032568681\n",
      "Batch 152 / 157\n",
      "training loss = 7.203923225402832\n",
      "validation loss = 7.258380689119038\n",
      "Batch 153 / 157\n",
      "training loss = 7.259983062744141\n",
      "validation loss = 7.263120450471577\n",
      "Batch 154 / 157\n",
      "training loss = 7.229179859161377\n",
      "validation loss = 7.252829024666234\n",
      "Batch 155 / 157\n",
      "training loss = 7.159221172332764\n",
      "validation loss = 7.2445935952036\n",
      "Batch 156 / 157\n",
      "training loss = 7.1534247398376465\n",
      "validation loss = 7.224650659059224\n",
      "Batch 157 / 157\n",
      "training loss = 7.254691123962402\n",
      "validation loss = 7.261205221477308\n",
      "Average training loss: 7.50\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.387983322143555\n",
      "validation loss = 10.183440961335835\n",
      "Batch 2 / 157\n",
      "training loss = 10.197172164916992\n",
      "validation loss = 9.744958827370091\n",
      "Batch 3 / 157\n",
      "training loss = 9.715662002563477\n",
      "validation loss = 9.394089698791504\n",
      "Batch 4 / 157\n",
      "training loss = 9.428462982177734\n",
      "validation loss = 9.070916477002596\n",
      "Batch 5 / 157\n",
      "training loss = 9.013397216796875\n",
      "validation loss = 8.770263571488229\n",
      "Batch 6 / 157\n",
      "training loss = 8.772028923034668\n",
      "validation loss = 8.511377384788112\n",
      "Batch 7 / 157\n",
      "training loss = 8.6574068069458\n",
      "validation loss = 8.310053574411493\n",
      "Batch 8 / 157\n",
      "training loss = 8.352678298950195\n",
      "validation loss = 8.17146803203382\n",
      "Batch 9 / 157\n",
      "training loss = 7.967406272888184\n",
      "validation loss = 8.109895655983372\n",
      "Batch 10 / 157\n",
      "training loss = 8.164584159851074\n",
      "validation loss = 8.102148056030273\n",
      "Batch 11 / 157\n",
      "training loss = 7.945367813110352\n",
      "validation loss = 8.125918212689852\n",
      "Batch 12 / 157\n",
      "training loss = 8.102581977844238\n",
      "validation loss = 8.13949670289692\n",
      "Batch 13 / 157\n",
      "training loss = 8.106647491455078\n",
      "validation loss = 8.076803608944541\n",
      "Batch 14 / 157\n",
      "training loss = 8.053701400756836\n",
      "validation loss = 7.992198065707558\n",
      "Batch 15 / 157\n",
      "training loss = 7.9511871337890625\n",
      "validation loss = 8.001877910212466\n",
      "Batch 16 / 157\n",
      "training loss = 7.969382286071777\n",
      "validation loss = 7.954163501137181\n",
      "Batch 17 / 157\n",
      "training loss = 7.994150161743164\n",
      "validation loss = 7.965329220420436\n",
      "Batch 18 / 157\n",
      "training loss = 7.968846321105957\n",
      "validation loss = 7.943634334363435\n",
      "Batch 19 / 157\n",
      "training loss = 7.837414264678955\n",
      "validation loss = 7.907117617757697\n",
      "Batch 20 / 157\n",
      "training loss = 7.899936199188232\n",
      "validation loss = 7.903966903686523\n",
      "Batch 21 / 157\n",
      "training loss = 7.689123630523682\n",
      "validation loss = 7.892760402277896\n",
      "Batch 22 / 157\n",
      "training loss = 8.063658714294434\n",
      "validation loss = 7.880182517202277\n",
      "Batch 23 / 157\n",
      "training loss = 8.039336204528809\n",
      "validation loss = 7.863147936369243\n",
      "Batch 24 / 157\n",
      "training loss = 8.07185173034668\n",
      "validation loss = 7.855064191316304\n",
      "Batch 25 / 157\n",
      "training loss = 8.088408470153809\n",
      "validation loss = 7.854909470206813\n",
      "Batch 26 / 157\n",
      "training loss = 7.373366355895996\n",
      "validation loss = 7.835368909333882\n",
      "Batch 27 / 157\n",
      "training loss = 8.119365692138672\n",
      "validation loss = 7.8369258328488\n",
      "Batch 28 / 157\n",
      "training loss = 7.874316215515137\n",
      "validation loss = 7.81089617076673\n",
      "Batch 29 / 157\n",
      "training loss = 7.795955657958984\n",
      "validation loss = 7.81924285386738\n",
      "Batch 30 / 157\n",
      "training loss = 7.952859401702881\n",
      "validation loss = 7.8136465925919385\n",
      "Batch 31 / 157\n",
      "training loss = 7.874990463256836\n",
      "validation loss = 7.787380143215782\n",
      "Batch 32 / 157\n",
      "training loss = 7.6292033195495605\n",
      "validation loss = 7.809828080629048\n",
      "Batch 33 / 157\n",
      "training loss = 7.894629955291748\n",
      "validation loss = 7.75937527104428\n",
      "Batch 34 / 157\n",
      "training loss = 7.571996688842773\n",
      "validation loss = 7.765975801568282\n",
      "Batch 35 / 157\n",
      "training loss = 7.748476505279541\n",
      "validation loss = 7.73987340927124\n",
      "Batch 36 / 157\n",
      "training loss = 7.525163173675537\n",
      "validation loss = 7.7794494126972396\n",
      "Batch 37 / 157\n",
      "training loss = 7.724016189575195\n",
      "validation loss = 7.761785206041838\n",
      "Batch 38 / 157\n",
      "training loss = 7.807948112487793\n",
      "validation loss = 7.712497635891563\n",
      "Batch 39 / 157\n",
      "training loss = 7.723267078399658\n",
      "validation loss = 7.794734829350522\n",
      "Batch 40 / 157\n",
      "training loss = 7.960909366607666\n",
      "validation loss = 7.701849335118344\n",
      "Batch 41 / 157\n",
      "training loss = 7.251556396484375\n",
      "validation loss = 7.762209315049021\n",
      "Batch 42 / 157\n",
      "training loss = 7.544164180755615\n",
      "validation loss = 7.757318044963636\n",
      "Batch 43 / 157\n",
      "training loss = 7.711671352386475\n",
      "validation loss = 7.721631777913947\n",
      "Batch 44 / 157\n",
      "training loss = 7.702561378479004\n",
      "validation loss = 7.736325640427439\n",
      "Batch 45 / 157\n",
      "training loss = 7.614238262176514\n",
      "validation loss = 7.73080775612279\n",
      "Batch 46 / 157\n",
      "training loss = 7.779955863952637\n",
      "validation loss = 7.7155857588115495\n",
      "Batch 47 / 157\n",
      "training loss = 7.520700454711914\n",
      "validation loss = 7.7044372056659896\n",
      "Batch 48 / 157\n",
      "training loss = 8.088998794555664\n",
      "validation loss = 7.699937569467645\n",
      "Batch 49 / 157\n",
      "training loss = 7.823541164398193\n",
      "validation loss = 7.684392803593686\n",
      "Batch 50 / 157\n",
      "training loss = 7.472470760345459\n",
      "validation loss = 7.673993637687282\n",
      "Batch 51 / 157\n",
      "training loss = 6.924194812774658\n",
      "validation loss = 7.658038540890343\n",
      "Batch 52 / 157\n",
      "training loss = 7.544489860534668\n",
      "validation loss = 7.645036471517463\n",
      "Batch 53 / 157\n",
      "training loss = 7.206172943115234\n",
      "validation loss = 7.632426814029091\n",
      "Batch 54 / 157\n",
      "training loss = 7.603830814361572\n",
      "validation loss = 7.652605132052773\n",
      "Batch 55 / 157\n",
      "training loss = 8.162066459655762\n",
      "validation loss = 7.611429189380846\n",
      "Batch 56 / 157\n",
      "training loss = 7.752725124359131\n",
      "validation loss = 7.5963970234519556\n",
      "Batch 57 / 157\n",
      "training loss = 7.897606372833252\n",
      "validation loss = 7.586150219565944\n",
      "Batch 58 / 157\n",
      "training loss = 7.820019245147705\n",
      "validation loss = 7.612328353681062\n",
      "Batch 59 / 157\n",
      "training loss = 7.72341251373291\n",
      "validation loss = 7.56823700352719\n",
      "Batch 60 / 157\n",
      "training loss = 7.931260585784912\n",
      "validation loss = 7.558200258957712\n",
      "Batch 61 / 157\n",
      "training loss = 7.299428462982178\n",
      "validation loss = 7.545436081133391\n",
      "Batch 62 / 157\n",
      "training loss = 7.620174407958984\n",
      "validation loss = 7.52749164480912\n",
      "Batch 63 / 157\n",
      "training loss = 7.36409854888916\n",
      "validation loss = 7.526880013315301\n",
      "Batch 64 / 157\n",
      "training loss = 7.714174270629883\n",
      "validation loss = 7.531465179041812\n",
      "Batch 65 / 157\n",
      "training loss = 7.767292499542236\n",
      "validation loss = 7.549975947329872\n",
      "Batch 66 / 157\n",
      "training loss = 7.354012966156006\n",
      "validation loss = 7.5133421044600635\n",
      "Batch 67 / 157\n",
      "training loss = 7.112255573272705\n",
      "validation loss = 7.485380122536107\n",
      "Batch 68 / 157\n",
      "training loss = 7.3519182205200195\n",
      "validation loss = 7.4905922789322705\n",
      "Batch 69 / 157\n",
      "training loss = 7.625919342041016\n",
      "validation loss = 7.493451996853477\n",
      "Batch 70 / 157\n",
      "training loss = 7.486438751220703\n",
      "validation loss = 7.477340296695107\n",
      "Batch 71 / 157\n",
      "training loss = 7.454323768615723\n",
      "validation loss = 7.478038361198024\n",
      "Batch 72 / 157\n",
      "training loss = 7.56600284576416\n",
      "validation loss = 7.4873560353329305\n",
      "Batch 73 / 157\n",
      "training loss = 7.4758477210998535\n",
      "validation loss = 7.521915360500938\n",
      "Batch 74 / 157\n",
      "training loss = 7.522152423858643\n",
      "validation loss = 7.494343381179006\n",
      "Batch 75 / 157\n",
      "training loss = 7.74807596206665\n",
      "validation loss = 7.513239935824745\n",
      "Batch 76 / 157\n",
      "training loss = 7.429868221282959\n",
      "validation loss = 7.494776073255037\n",
      "Batch 77 / 157\n",
      "training loss = 7.840679168701172\n",
      "validation loss = 7.522726485603734\n",
      "Batch 78 / 157\n",
      "training loss = 7.973458766937256\n",
      "validation loss = 7.551219839798777\n",
      "Batch 79 / 157\n",
      "training loss = 7.467710494995117\n",
      "validation loss = 7.476436740473697\n",
      "Batch 80 / 157\n",
      "training loss = 7.150117874145508\n",
      "validation loss = 7.467138315501966\n",
      "Batch 81 / 157\n",
      "training loss = 7.826045513153076\n",
      "validation loss = 7.48776398207012\n",
      "Batch 82 / 157\n",
      "training loss = 7.313787460327148\n",
      "validation loss = 7.475840091705322\n",
      "Batch 83 / 157\n",
      "training loss = 7.4225077629089355\n",
      "validation loss = 7.476673301897551\n",
      "Batch 84 / 157\n",
      "training loss = 7.435975074768066\n",
      "validation loss = 7.462456527509187\n",
      "Batch 85 / 157\n",
      "training loss = 7.0521240234375\n",
      "validation loss = 7.450813193070261\n",
      "Batch 86 / 157\n",
      "training loss = 7.8302812576293945\n",
      "validation loss = 7.449135529367547\n",
      "Batch 87 / 157\n",
      "training loss = 7.567008018493652\n",
      "validation loss = 7.4567159853483505\n",
      "Batch 88 / 157\n",
      "training loss = 7.5654296875\n",
      "validation loss = 7.467866395649157\n",
      "Batch 89 / 157\n",
      "training loss = 7.158990383148193\n",
      "validation loss = 7.427332401275635\n",
      "Batch 90 / 157\n",
      "training loss = 7.346578121185303\n",
      "validation loss = 7.463420918113307\n",
      "Batch 91 / 157\n",
      "training loss = 7.400910377502441\n",
      "validation loss = 7.438340889780145\n",
      "Batch 92 / 157\n",
      "training loss = 7.551614761352539\n",
      "validation loss = 7.423346695147063\n",
      "Batch 93 / 157\n",
      "training loss = 7.632205963134766\n",
      "validation loss = 7.46954890301353\n",
      "Batch 94 / 157\n",
      "training loss = 7.7770843505859375\n",
      "validation loss = 7.419307683643542\n",
      "Batch 95 / 157\n",
      "training loss = 7.1952385902404785\n",
      "validation loss = 7.407095808731882\n",
      "Batch 96 / 157\n",
      "training loss = 7.009695529937744\n",
      "validation loss = 7.436749960246839\n",
      "Batch 97 / 157\n",
      "training loss = 7.504237174987793\n",
      "validation loss = 7.416232309843364\n",
      "Batch 98 / 157\n",
      "training loss = 7.1232805252075195\n",
      "validation loss = 7.377900675723427\n",
      "Batch 99 / 157\n",
      "training loss = 7.5233988761901855\n",
      "validation loss = 7.391338248001902\n",
      "Batch 100 / 157\n",
      "training loss = 7.454543590545654\n",
      "validation loss = 7.416041700463546\n",
      "Batch 101 / 157\n",
      "training loss = 7.190145969390869\n",
      "validation loss = 7.381298215765702\n",
      "Batch 102 / 157\n",
      "training loss = 7.3697638511657715\n",
      "validation loss = 7.364618728035374\n",
      "Batch 103 / 157\n",
      "training loss = 7.534467697143555\n",
      "validation loss = 7.386181906649941\n",
      "Batch 104 / 157\n",
      "training loss = 7.168397426605225\n",
      "validation loss = 7.3901753425598145\n",
      "Batch 105 / 157\n",
      "training loss = 6.670319080352783\n",
      "validation loss = 7.390306999808864\n",
      "Batch 106 / 157\n",
      "training loss = 7.616634368896484\n",
      "validation loss = 7.3592893700850635\n",
      "Batch 107 / 157\n",
      "training loss = 7.00676155090332\n",
      "validation loss = 7.362753667329487\n",
      "Batch 108 / 157\n",
      "training loss = 7.367826461791992\n",
      "validation loss = 7.365929151836195\n",
      "Batch 109 / 157\n",
      "training loss = 7.026897430419922\n",
      "validation loss = 7.3449327820225765\n",
      "Batch 110 / 157\n",
      "training loss = 6.71293306350708\n",
      "validation loss = 7.3927432863335865\n",
      "Batch 111 / 157\n",
      "training loss = 7.294637203216553\n",
      "validation loss = 7.357410255231355\n",
      "Batch 112 / 157\n",
      "training loss = 7.229996204376221\n",
      "validation loss = 7.344981544896176\n",
      "Batch 113 / 157\n",
      "training loss = 7.298933029174805\n",
      "validation loss = 7.37270460630718\n",
      "Batch 114 / 157\n",
      "training loss = 7.122640132904053\n",
      "validation loss = 7.354537562320107\n",
      "Batch 115 / 157\n",
      "training loss = 6.900150299072266\n",
      "validation loss = 7.357289188786557\n",
      "Batch 116 / 157\n",
      "training loss = 7.547645092010498\n",
      "validation loss = 7.360441308272512\n",
      "Batch 117 / 157\n",
      "training loss = 7.008468151092529\n",
      "validation loss = 7.3500260302894995\n",
      "Batch 118 / 157\n",
      "training loss = 7.567655563354492\n",
      "validation loss = 7.350169608467503\n",
      "Batch 119 / 157\n",
      "training loss = 7.085322380065918\n",
      "validation loss = 7.350332686775609\n",
      "Batch 120 / 157\n",
      "training loss = 7.5504279136657715\n",
      "validation loss = 7.351271177593031\n",
      "Batch 121 / 157\n",
      "training loss = 7.365108966827393\n",
      "validation loss = 7.329093983298854\n",
      "Batch 122 / 157\n",
      "training loss = 7.241890907287598\n",
      "validation loss = 7.331351581372712\n",
      "Batch 123 / 157\n",
      "training loss = 7.488454818725586\n",
      "validation loss = 7.34497961245085\n",
      "Batch 124 / 157\n",
      "training loss = 7.47355842590332\n",
      "validation loss = 7.31294732344778\n",
      "Batch 125 / 157\n",
      "training loss = 7.668722629547119\n",
      "validation loss = 7.3418549738432235\n",
      "Batch 126 / 157\n",
      "training loss = 7.501450538635254\n",
      "validation loss = 7.342648079520778\n",
      "Batch 127 / 157\n",
      "training loss = 7.338089466094971\n",
      "validation loss = 7.343056954835591\n",
      "Batch 128 / 157\n",
      "training loss = 7.425943374633789\n",
      "validation loss = 7.3304699345638875\n",
      "Batch 129 / 157\n",
      "training loss = 7.546509742736816\n",
      "validation loss = 7.324698096827457\n",
      "Batch 130 / 157\n",
      "training loss = 7.123281955718994\n",
      "validation loss = 7.338825803054006\n",
      "Batch 131 / 157\n",
      "training loss = 7.70672082901001\n",
      "validation loss = 7.320551997736881\n",
      "Batch 132 / 157\n",
      "training loss = 7.276627063751221\n",
      "validation loss = 7.323375777194374\n",
      "Batch 133 / 157\n",
      "training loss = 6.964505672454834\n",
      "validation loss = 7.317447260806435\n",
      "Batch 134 / 157\n",
      "training loss = 7.1988301277160645\n",
      "validation loss = 7.33873324645193\n",
      "Batch 135 / 157\n",
      "training loss = 7.196570873260498\n",
      "validation loss = 7.341813162753456\n",
      "Batch 136 / 157\n",
      "training loss = 7.509296417236328\n",
      "validation loss = 7.313697840038099\n",
      "Batch 137 / 157\n",
      "training loss = 7.528660297393799\n",
      "validation loss = 7.324876559408088\n",
      "Batch 138 / 157\n",
      "training loss = 7.585992813110352\n",
      "validation loss = 7.335890870345266\n",
      "Batch 139 / 157\n",
      "training loss = 7.02738094329834\n",
      "validation loss = 7.3077746943423625\n",
      "Batch 140 / 157\n",
      "training loss = 7.676374912261963\n",
      "validation loss = 7.311707847996762\n",
      "Batch 141 / 157\n",
      "training loss = 7.17102575302124\n",
      "validation loss = 7.328368563401072\n",
      "Batch 142 / 157\n",
      "training loss = 7.471935272216797\n",
      "validation loss = 7.304513881081029\n",
      "Batch 143 / 157\n",
      "training loss = 7.420475482940674\n",
      "validation loss = 7.286907196044922\n",
      "Batch 144 / 157\n",
      "training loss = 7.446656703948975\n",
      "validation loss = 7.304975685320403\n",
      "Batch 145 / 157\n",
      "training loss = 7.283132553100586\n",
      "validation loss = 7.286305076197574\n",
      "Batch 146 / 157\n",
      "training loss = 7.242180347442627\n",
      "validation loss = 7.262869659223054\n",
      "Batch 147 / 157\n",
      "training loss = 7.172802925109863\n",
      "validation loss = 7.2682673554671435\n",
      "Batch 148 / 157\n",
      "training loss = 7.089554309844971\n",
      "validation loss = 7.282413156409013\n",
      "Batch 149 / 157\n",
      "training loss = 7.298207759857178\n",
      "validation loss = 7.281308801550614\n",
      "Batch 150 / 157\n",
      "training loss = 7.1712446212768555\n",
      "validation loss = 7.258212315408807\n",
      "Batch 151 / 157\n",
      "training loss = 7.356003761291504\n",
      "validation loss = 7.26264140480443\n",
      "Batch 152 / 157\n",
      "training loss = 7.196399688720703\n",
      "validation loss = 7.27606012946681\n",
      "Batch 153 / 157\n",
      "training loss = 7.635362148284912\n",
      "validation loss = 7.281313494632118\n",
      "Batch 154 / 157\n",
      "training loss = 7.332258224487305\n",
      "validation loss = 7.259926645379317\n",
      "Batch 155 / 157\n",
      "training loss = 7.34735107421875\n",
      "validation loss = 7.251765853480289\n",
      "Batch 156 / 157\n",
      "training loss = 6.950510501861572\n",
      "validation loss = 7.279767011341296\n",
      "Batch 157 / 157\n",
      "training loss = 7.1319403648376465\n",
      "validation loss = 7.343116183029978\n",
      "Average training loss: 7.61\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.383337020874023\n",
      "validation loss = 10.168338625054611\n",
      "Batch 2 / 157\n",
      "training loss = 10.192239761352539\n",
      "validation loss = 9.681821120412726\n",
      "Batch 3 / 157\n",
      "training loss = 9.716617584228516\n",
      "validation loss = 9.260740079377827\n",
      "Batch 4 / 157\n",
      "training loss = 9.250275611877441\n",
      "validation loss = 8.907632827758789\n",
      "Batch 5 / 157\n",
      "training loss = 9.035654067993164\n",
      "validation loss = 8.603152425665604\n",
      "Batch 6 / 157\n",
      "training loss = 8.671567916870117\n",
      "validation loss = 8.35794358504446\n",
      "Batch 7 / 157\n",
      "training loss = 8.268071174621582\n",
      "validation loss = 8.169997315657767\n",
      "Batch 8 / 157\n",
      "training loss = 8.169238090515137\n",
      "validation loss = 8.040796857131156\n",
      "Batch 9 / 157\n",
      "training loss = 8.230010032653809\n",
      "validation loss = 7.970071064798455\n",
      "Batch 10 / 157\n",
      "training loss = 8.313283920288086\n",
      "validation loss = 7.9334435964885515\n",
      "Batch 11 / 157\n",
      "training loss = 7.964327335357666\n",
      "validation loss = 7.924490200845819\n",
      "Batch 12 / 157\n",
      "training loss = 7.873154640197754\n",
      "validation loss = 7.911422177364952\n",
      "Batch 13 / 157\n",
      "training loss = 7.642224311828613\n",
      "validation loss = 7.87952807075099\n",
      "Batch 14 / 157\n",
      "training loss = 7.84613037109375\n",
      "validation loss = 7.832402555566085\n",
      "Batch 15 / 157\n",
      "training loss = 7.768691062927246\n",
      "validation loss = 7.884519075092516\n",
      "Batch 16 / 157\n",
      "training loss = 7.898676872253418\n",
      "validation loss = 7.81176737735146\n",
      "Batch 17 / 157\n",
      "training loss = 7.845109939575195\n",
      "validation loss = 7.830579908270585\n",
      "Batch 18 / 157\n",
      "training loss = 7.84959077835083\n",
      "validation loss = 7.8245586596037215\n",
      "Batch 19 / 157\n",
      "training loss = 7.807748794555664\n",
      "validation loss = 7.794377050901714\n",
      "Batch 20 / 157\n",
      "training loss = 7.845047950744629\n",
      "validation loss = 7.772581276140715\n",
      "Batch 21 / 157\n",
      "training loss = 7.950611114501953\n",
      "validation loss = 7.794817271985505\n",
      "Batch 22 / 157\n",
      "training loss = 7.868897914886475\n",
      "validation loss = 7.787888677496659\n",
      "Batch 23 / 157\n",
      "training loss = 7.692770957946777\n",
      "validation loss = 7.753310278842323\n",
      "Batch 24 / 157\n",
      "training loss = 7.864659786224365\n",
      "validation loss = 7.743220856315212\n",
      "Batch 25 / 157\n",
      "training loss = 7.708383083343506\n",
      "validation loss = 7.751667223478618\n",
      "Batch 26 / 157\n",
      "training loss = 7.931216716766357\n",
      "validation loss = 7.746411775287829\n",
      "Batch 27 / 157\n",
      "training loss = 7.900284290313721\n",
      "validation loss = 7.73090934753418\n",
      "Batch 28 / 157\n",
      "training loss = 7.84545373916626\n",
      "validation loss = 7.723721805371736\n",
      "Batch 29 / 157\n",
      "training loss = 7.469350337982178\n",
      "validation loss = 7.721392656627454\n",
      "Batch 30 / 157\n",
      "training loss = 7.830363750457764\n",
      "validation loss = 7.714100561643901\n",
      "Batch 31 / 157\n",
      "training loss = 7.808969020843506\n",
      "validation loss = 7.6982786278975635\n",
      "Batch 32 / 157\n",
      "training loss = 7.6040215492248535\n",
      "validation loss = 7.680246579019647\n",
      "Batch 33 / 157\n",
      "training loss = 7.462746620178223\n",
      "validation loss = 7.680647247716\n",
      "Batch 34 / 157\n",
      "training loss = 7.6745171546936035\n",
      "validation loss = 7.664405270626671\n",
      "Batch 35 / 157\n",
      "training loss = 7.746620178222656\n",
      "validation loss = 7.6383030289097835\n",
      "Batch 36 / 157\n",
      "training loss = 7.787893772125244\n",
      "validation loss = 7.666017532348633\n",
      "Batch 37 / 157\n",
      "training loss = 7.688164234161377\n",
      "validation loss = 7.6206464767456055\n",
      "Batch 38 / 157\n",
      "training loss = 7.564417362213135\n",
      "validation loss = 7.640765215221204\n",
      "Batch 39 / 157\n",
      "training loss = 7.350941181182861\n",
      "validation loss = 7.649655492682206\n",
      "Batch 40 / 157\n",
      "training loss = 7.496631622314453\n",
      "validation loss = 7.623389495046515\n",
      "Batch 41 / 157\n",
      "training loss = 8.025178909301758\n",
      "validation loss = 7.606802087081106\n",
      "Batch 42 / 157\n",
      "training loss = 7.840036392211914\n",
      "validation loss = 7.6601462615163705\n",
      "Batch 43 / 157\n",
      "training loss = 7.7315263748168945\n",
      "validation loss = 7.628900201697099\n",
      "Batch 44 / 157\n",
      "training loss = 7.788640022277832\n",
      "validation loss = 7.589362796984221\n",
      "Batch 45 / 157\n",
      "training loss = 7.6482977867126465\n",
      "validation loss = 7.592060089111328\n",
      "Batch 46 / 157\n",
      "training loss = 7.395482540130615\n",
      "validation loss = 7.605499167191355\n",
      "Batch 47 / 157\n",
      "training loss = 7.955203533172607\n",
      "validation loss = 7.592363984961259\n",
      "Batch 48 / 157\n",
      "training loss = 7.315034866333008\n",
      "validation loss = 7.575153777473851\n",
      "Batch 49 / 157\n",
      "training loss = 7.6079840660095215\n",
      "validation loss = 7.561779950794421\n",
      "Batch 50 / 157\n",
      "training loss = 7.4885783195495605\n",
      "validation loss = 7.564441881681743\n",
      "Batch 51 / 157\n",
      "training loss = 7.744957447052002\n",
      "validation loss = 7.5678479796961735\n",
      "Batch 52 / 157\n",
      "training loss = 7.569164276123047\n",
      "validation loss = 7.563789392772474\n",
      "Batch 53 / 157\n",
      "training loss = 7.470778465270996\n",
      "validation loss = 7.559560273822985\n",
      "Batch 54 / 157\n",
      "training loss = 7.656304359436035\n",
      "validation loss = 7.547562122344971\n",
      "Batch 55 / 157\n",
      "training loss = 7.8597893714904785\n",
      "validation loss = 7.544910255231355\n",
      "Batch 56 / 157\n",
      "training loss = 7.6602935791015625\n",
      "validation loss = 7.535599859137284\n",
      "Batch 57 / 157\n",
      "training loss = 7.860701560974121\n",
      "validation loss = 7.524641162470767\n",
      "Batch 58 / 157\n",
      "training loss = 7.646860599517822\n",
      "validation loss = 7.518976964448628\n",
      "Batch 59 / 157\n",
      "training loss = 7.2863359451293945\n",
      "validation loss = 7.511468937522487\n",
      "Batch 60 / 157\n",
      "training loss = 7.381330490112305\n",
      "validation loss = 7.499545348318\n",
      "Batch 61 / 157\n",
      "training loss = 7.49980354309082\n",
      "validation loss = 7.490259546982615\n",
      "Batch 62 / 157\n",
      "training loss = 7.563907623291016\n",
      "validation loss = 7.4946285548963045\n",
      "Batch 63 / 157\n",
      "training loss = 7.546804904937744\n",
      "validation loss = 7.486166928943835\n",
      "Batch 64 / 157\n",
      "training loss = 7.659320831298828\n",
      "validation loss = 7.483381447039153\n",
      "Batch 65 / 157\n",
      "training loss = 7.732189655303955\n",
      "validation loss = 7.485253007788407\n",
      "Batch 66 / 157\n",
      "training loss = 7.309123992919922\n",
      "validation loss = 7.477251805757222\n",
      "Batch 67 / 157\n",
      "training loss = 7.442573070526123\n",
      "validation loss = 7.475375376249614\n",
      "Batch 68 / 157\n",
      "training loss = 7.4439377784729\n",
      "validation loss = 7.469078239641692\n",
      "Batch 69 / 157\n",
      "training loss = 7.557584285736084\n",
      "validation loss = 7.470679333335475\n",
      "Batch 70 / 157\n",
      "training loss = 7.508805274963379\n",
      "validation loss = 7.462162795819734\n",
      "Batch 71 / 157\n",
      "training loss = 7.407884120941162\n",
      "validation loss = 7.459072012650339\n",
      "Batch 72 / 157\n",
      "training loss = 7.521049499511719\n",
      "validation loss = 7.458520487735146\n",
      "Batch 73 / 157\n",
      "training loss = 7.494035243988037\n",
      "validation loss = 7.463424958680806\n",
      "Batch 74 / 157\n",
      "training loss = 7.616613864898682\n",
      "validation loss = 7.462401214398835\n",
      "Batch 75 / 157\n",
      "training loss = 7.562237739562988\n",
      "validation loss = 7.456458643863075\n",
      "Batch 76 / 157\n",
      "training loss = 7.498406410217285\n",
      "validation loss = 7.450034543087608\n",
      "Batch 77 / 157\n",
      "training loss = 7.024691581726074\n",
      "validation loss = 7.452009050469649\n",
      "Batch 78 / 157\n",
      "training loss = 7.640113353729248\n",
      "validation loss = 7.450192125220048\n",
      "Batch 79 / 157\n",
      "training loss = 7.458062171936035\n",
      "validation loss = 7.448103478080348\n",
      "Batch 80 / 157\n",
      "training loss = 7.242971897125244\n",
      "validation loss = 7.4484282292817765\n",
      "Batch 81 / 157\n",
      "training loss = 7.373593807220459\n",
      "validation loss = 7.441464323746531\n",
      "Batch 82 / 157\n",
      "training loss = 7.484164237976074\n",
      "validation loss = 7.4348647970902295\n",
      "Batch 83 / 157\n",
      "training loss = 7.4236578941345215\n",
      "validation loss = 7.43991558175338\n",
      "Batch 84 / 157\n",
      "training loss = 7.423271179199219\n",
      "validation loss = 7.435787125637657\n",
      "Batch 85 / 157\n",
      "training loss = 7.439302444458008\n",
      "validation loss = 7.431070528532329\n",
      "Batch 86 / 157\n",
      "training loss = 7.641672611236572\n",
      "validation loss = 7.430837455548738\n",
      "Batch 87 / 157\n",
      "training loss = 7.574531555175781\n",
      "validation loss = 7.425348256763659\n",
      "Batch 88 / 157\n",
      "training loss = 7.388147354125977\n",
      "validation loss = 7.41459866573936\n",
      "Batch 89 / 157\n",
      "training loss = 7.400105953216553\n",
      "validation loss = 7.410195526323821\n",
      "Batch 90 / 157\n",
      "training loss = 7.546975612640381\n",
      "validation loss = 7.416030406951904\n",
      "Batch 91 / 157\n",
      "training loss = 7.597748279571533\n",
      "validation loss = 7.416812093634355\n",
      "Batch 92 / 157\n",
      "training loss = 7.364067077636719\n",
      "validation loss = 7.412105459915964\n",
      "Batch 93 / 157\n",
      "training loss = 7.464032173156738\n",
      "validation loss = 7.410144504747893\n",
      "Batch 94 / 157\n",
      "training loss = 7.180026531219482\n",
      "validation loss = 7.407975723868923\n",
      "Batch 95 / 157\n",
      "training loss = 7.495360374450684\n",
      "validation loss = 7.393327537335847\n",
      "Batch 96 / 157\n",
      "training loss = 7.33367919921875\n",
      "validation loss = 7.399738311767578\n",
      "Batch 97 / 157\n",
      "training loss = 7.161979675292969\n",
      "validation loss = 7.405310229251259\n",
      "Batch 98 / 157\n",
      "training loss = 7.055967330932617\n",
      "validation loss = 7.401113083488063\n",
      "Batch 99 / 157\n",
      "training loss = 7.603959560394287\n",
      "validation loss = 7.3985498829891805\n",
      "Batch 100 / 157\n",
      "training loss = 7.523812770843506\n",
      "validation loss = 7.408473516765394\n",
      "Batch 101 / 157\n",
      "training loss = 7.553762435913086\n",
      "validation loss = 7.393915427358527\n",
      "Batch 102 / 157\n",
      "training loss = 7.440941333770752\n",
      "validation loss = 7.397453157525313\n",
      "Batch 103 / 157\n",
      "training loss = 7.6241068840026855\n",
      "validation loss = 7.392051295230263\n",
      "Batch 104 / 157\n",
      "training loss = 7.413958549499512\n",
      "validation loss = 7.3926919886940405\n",
      "Batch 105 / 157\n",
      "training loss = 7.218598365783691\n",
      "validation loss = 7.381182168659411\n",
      "Batch 106 / 157\n",
      "training loss = 7.461477756500244\n",
      "validation loss = 7.371542027122096\n",
      "Batch 107 / 157\n",
      "training loss = 7.407878875732422\n",
      "validation loss = 7.362963626259251\n",
      "Batch 108 / 157\n",
      "training loss = 7.335484981536865\n",
      "validation loss = 7.368293862593801\n",
      "Batch 109 / 157\n",
      "training loss = 7.487284183502197\n",
      "validation loss = 7.3621113927740796\n",
      "Batch 110 / 157\n",
      "training loss = 7.546356201171875\n",
      "validation loss = 7.36392503035696\n",
      "Batch 111 / 157\n",
      "training loss = 7.5154032707214355\n",
      "validation loss = 7.361041043934069\n",
      "Batch 112 / 157\n",
      "training loss = 7.438502788543701\n",
      "validation loss = 7.365822340312757\n",
      "Batch 113 / 157\n",
      "training loss = 7.375190258026123\n",
      "validation loss = 7.358564502314517\n",
      "Batch 114 / 157\n",
      "training loss = 7.254815578460693\n",
      "validation loss = 7.354625124680369\n",
      "Batch 115 / 157\n",
      "training loss = 7.41826057434082\n",
      "validation loss = 7.3603032011734815\n",
      "Batch 116 / 157\n",
      "training loss = 7.630612373352051\n",
      "validation loss = 7.357883829819529\n",
      "Batch 117 / 157\n",
      "training loss = 7.360827445983887\n",
      "validation loss = 7.348604252463893\n",
      "Batch 118 / 157\n",
      "training loss = 7.38571310043335\n",
      "validation loss = 7.350920928151984\n",
      "Batch 119 / 157\n",
      "training loss = 7.545035362243652\n",
      "validation loss = 7.353653330551951\n",
      "Batch 120 / 157\n",
      "training loss = 7.327610969543457\n",
      "validation loss = 7.342175483703613\n",
      "Batch 121 / 157\n",
      "training loss = 7.5081586837768555\n",
      "validation loss = 7.350720756932309\n",
      "Batch 122 / 157\n",
      "training loss = 7.233731269836426\n",
      "validation loss = 7.341971046046207\n",
      "Batch 123 / 157\n",
      "training loss = 7.283454418182373\n",
      "validation loss = 7.332715084678249\n",
      "Batch 124 / 157\n",
      "training loss = 7.591732978820801\n",
      "validation loss = 7.338051971636321\n",
      "Batch 125 / 157\n",
      "training loss = 7.224567413330078\n",
      "validation loss = 7.330292902494731\n",
      "Batch 126 / 157\n",
      "training loss = 7.276124477386475\n",
      "validation loss = 7.324637613798442\n",
      "Batch 127 / 157\n",
      "training loss = 7.416613578796387\n",
      "validation loss = 7.3274789107473275\n",
      "Batch 128 / 157\n",
      "training loss = 7.485856533050537\n",
      "validation loss = 7.324849078529759\n",
      "Batch 129 / 157\n",
      "training loss = 7.75486421585083\n",
      "validation loss = 7.3292663222865055\n",
      "Batch 130 / 157\n",
      "training loss = 7.086275577545166\n",
      "validation loss = 7.316569729855186\n",
      "Batch 131 / 157\n",
      "training loss = 7.508732795715332\n",
      "validation loss = 7.320802989758943\n",
      "Batch 132 / 157\n",
      "training loss = 7.207726001739502\n",
      "validation loss = 7.3175065642909\n",
      "Batch 133 / 157\n",
      "training loss = 7.576119899749756\n",
      "validation loss = 7.313681953831723\n",
      "Batch 134 / 157\n",
      "training loss = 7.295041561126709\n",
      "validation loss = 7.328005238583214\n",
      "Batch 135 / 157\n",
      "training loss = 7.347355365753174\n",
      "validation loss = 7.316375079907869\n",
      "Batch 136 / 157\n",
      "training loss = 7.534640789031982\n",
      "validation loss = 7.311364751113088\n",
      "Batch 137 / 157\n",
      "training loss = 7.26651668548584\n",
      "validation loss = 7.3210396013761825\n",
      "Batch 138 / 157\n",
      "training loss = 7.206886291503906\n",
      "validation loss = 7.307579542461195\n",
      "Batch 139 / 157\n",
      "training loss = 7.496582508087158\n",
      "validation loss = 7.304698241384406\n",
      "Batch 140 / 157\n",
      "training loss = 7.420192241668701\n",
      "validation loss = 7.317847377375553\n",
      "Batch 141 / 157\n",
      "training loss = 7.31052303314209\n",
      "validation loss = 7.303101715288665\n",
      "Batch 142 / 157\n",
      "training loss = 7.311063766479492\n",
      "validation loss = 7.296801240820634\n",
      "Batch 143 / 157\n",
      "training loss = 7.2450971603393555\n",
      "validation loss = 7.302129896063554\n",
      "Batch 144 / 157\n",
      "training loss = 7.264376640319824\n",
      "validation loss = 7.306678495909038\n",
      "Batch 145 / 157\n",
      "training loss = 7.379455089569092\n",
      "validation loss = 7.288036823272705\n",
      "Batch 146 / 157\n",
      "training loss = 7.461733341217041\n",
      "validation loss = 7.290515196950812\n",
      "Batch 147 / 157\n",
      "training loss = 7.35129451751709\n",
      "validation loss = 7.299474691089831\n",
      "Batch 148 / 157\n",
      "training loss = 7.30324125289917\n",
      "validation loss = 7.290256475147448\n",
      "Batch 149 / 157\n",
      "training loss = 7.427374839782715\n",
      "validation loss = 7.280622432106419\n",
      "Batch 150 / 157\n",
      "training loss = 7.247152805328369\n",
      "validation loss = 7.290466584657368\n",
      "Batch 151 / 157\n",
      "training loss = 7.432233810424805\n",
      "validation loss = 7.289700683794524\n",
      "Batch 152 / 157\n",
      "training loss = 7.287672519683838\n",
      "validation loss = 7.276725819236354\n",
      "Batch 153 / 157\n",
      "training loss = 7.113871097564697\n",
      "validation loss = 7.274829789211876\n",
      "Batch 154 / 157\n",
      "training loss = 7.2859110832214355\n",
      "validation loss = 7.269210188012374\n",
      "Batch 155 / 157\n",
      "training loss = 7.074833393096924\n",
      "validation loss = 7.266585525713469\n",
      "Batch 156 / 157\n",
      "training loss = 7.316600799560547\n",
      "validation loss = 7.266751264270983\n",
      "Batch 157 / 157\n",
      "training loss = 7.055272579193115\n",
      "validation loss = 7.2689101068597095\n",
      "Average training loss: 7.61\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.397366523742676\n",
      "validation loss = 10.258616999575967\n",
      "Batch 2 / 157\n",
      "training loss = 10.254329681396484\n",
      "validation loss = 9.692954464962607\n",
      "Batch 3 / 157\n",
      "training loss = 9.759689331054688\n",
      "validation loss = 9.284222251490542\n",
      "Batch 4 / 157\n",
      "training loss = 9.28474235534668\n",
      "validation loss = 8.94239300175717\n",
      "Batch 5 / 157\n",
      "training loss = 8.97613525390625\n",
      "validation loss = 8.647822078905607\n",
      "Batch 6 / 157\n",
      "training loss = 8.571529388427734\n",
      "validation loss = 8.392031217876234\n",
      "Batch 7 / 157\n",
      "training loss = 8.369043350219727\n",
      "validation loss = 8.192655864514803\n",
      "Batch 8 / 157\n",
      "training loss = 8.258830070495605\n",
      "validation loss = 8.039379170066432\n",
      "Batch 9 / 157\n",
      "training loss = 8.040526390075684\n",
      "validation loss = 7.927518417960719\n",
      "Batch 10 / 157\n",
      "training loss = 7.854363918304443\n",
      "validation loss = 7.847264114179109\n",
      "Batch 11 / 157\n",
      "training loss = 7.947883129119873\n",
      "validation loss = 7.794335315102025\n",
      "Batch 12 / 157\n",
      "training loss = 7.837628364562988\n",
      "validation loss = 7.760389378196315\n",
      "Batch 13 / 157\n",
      "training loss = 7.816745281219482\n",
      "validation loss = 7.73359331331755\n",
      "Batch 14 / 157\n",
      "training loss = 7.673471450805664\n",
      "validation loss = 7.712647714112935\n",
      "Batch 15 / 157\n",
      "training loss = 7.6630539894104\n",
      "validation loss = 7.698150735152395\n",
      "Batch 16 / 157\n",
      "training loss = 7.618172645568848\n",
      "validation loss = 7.686456128170616\n",
      "Batch 17 / 157\n",
      "training loss = 7.607499122619629\n",
      "validation loss = 7.681856431459126\n",
      "Batch 18 / 157\n",
      "training loss = 7.7578840255737305\n",
      "validation loss = 7.772567899603593\n",
      "Batch 19 / 157\n",
      "training loss = 7.789702415466309\n",
      "validation loss = 7.707776872735274\n",
      "Batch 20 / 157\n",
      "training loss = 7.72245979309082\n",
      "validation loss = 7.70210946233649\n",
      "Batch 21 / 157\n",
      "training loss = 7.758996963500977\n",
      "validation loss = 7.670294560884175\n",
      "Batch 22 / 157\n",
      "training loss = 7.436641216278076\n",
      "validation loss = 7.660907619877865\n",
      "Batch 23 / 157\n",
      "training loss = 7.695242404937744\n",
      "validation loss = 7.650417629041169\n",
      "Batch 24 / 157\n",
      "training loss = 7.7742018699646\n",
      "validation loss = 7.631413284100984\n",
      "Batch 25 / 157\n",
      "training loss = 7.327976226806641\n",
      "validation loss = 7.621168789110686\n",
      "Batch 26 / 157\n",
      "training loss = 7.562432289123535\n",
      "validation loss = 7.624598201952483\n",
      "Batch 27 / 157\n",
      "training loss = 7.690811634063721\n",
      "validation loss = 7.6279745603862565\n",
      "Batch 28 / 157\n",
      "training loss = 7.666543006896973\n",
      "validation loss = 7.62065345362613\n",
      "Batch 29 / 157\n",
      "training loss = 7.633543014526367\n",
      "validation loss = 7.609590731169048\n",
      "Batch 30 / 157\n",
      "training loss = 7.6687140464782715\n",
      "validation loss = 7.607662000154194\n",
      "Batch 31 / 157\n",
      "training loss = 7.743420600891113\n",
      "validation loss = 7.579532522904246\n",
      "Batch 32 / 157\n",
      "training loss = 7.492746829986572\n",
      "validation loss = 7.563600289194207\n",
      "Batch 33 / 157\n",
      "training loss = 7.651799201965332\n",
      "validation loss = 7.560645279131438\n",
      "Batch 34 / 157\n",
      "training loss = 7.8180341720581055\n",
      "validation loss = 7.573975211695621\n",
      "Batch 35 / 157\n",
      "training loss = 7.649955749511719\n",
      "validation loss = 7.577190323879845\n",
      "Batch 36 / 157\n",
      "training loss = 7.455989360809326\n",
      "validation loss = 7.555107693923147\n",
      "Batch 37 / 157\n",
      "training loss = 7.671360015869141\n",
      "validation loss = 7.544022635409706\n",
      "Batch 38 / 157\n",
      "training loss = 7.592377662658691\n",
      "validation loss = 7.531351767088237\n",
      "Batch 39 / 157\n",
      "training loss = 7.483705520629883\n",
      "validation loss = 7.538718901182476\n",
      "Batch 40 / 157\n",
      "training loss = 7.376990795135498\n",
      "validation loss = 7.5453550439131885\n",
      "Batch 41 / 157\n",
      "training loss = 7.85988712310791\n",
      "validation loss = 7.542166433836284\n",
      "Batch 42 / 157\n",
      "training loss = 7.515263557434082\n",
      "validation loss = 7.533126404410915\n",
      "Batch 43 / 157\n",
      "training loss = 7.707468509674072\n",
      "validation loss = 7.52052660992271\n",
      "Batch 44 / 157\n",
      "training loss = 7.593804359436035\n",
      "validation loss = 7.511601975089626\n",
      "Batch 45 / 157\n",
      "training loss = 7.329003810882568\n",
      "validation loss = 7.509696834965756\n",
      "Batch 46 / 157\n",
      "training loss = 7.568839073181152\n",
      "validation loss = 7.512779637386925\n",
      "Batch 47 / 157\n",
      "training loss = 7.50874137878418\n",
      "validation loss = 7.511719703674316\n",
      "Batch 48 / 157\n",
      "training loss = 7.203873157501221\n",
      "validation loss = 7.502628376609401\n",
      "Batch 49 / 157\n",
      "training loss = 7.139836311340332\n",
      "validation loss = 7.508073580892463\n",
      "Batch 50 / 157\n",
      "training loss = 7.334837436676025\n",
      "validation loss = 7.512053539878444\n",
      "Batch 51 / 157\n",
      "training loss = 7.535386562347412\n",
      "validation loss = 7.505647558914988\n",
      "Batch 52 / 157\n",
      "training loss = 7.3764448165893555\n",
      "validation loss = 7.504812039827046\n",
      "Batch 53 / 157\n",
      "training loss = 7.804461479187012\n",
      "validation loss = 7.509949558659604\n",
      "Batch 54 / 157\n",
      "training loss = 7.410295009613037\n",
      "validation loss = 7.500817775726318\n",
      "Batch 55 / 157\n",
      "training loss = 7.426766872406006\n",
      "validation loss = 7.482981957887349\n",
      "Batch 56 / 157\n",
      "training loss = 7.433426380157471\n",
      "validation loss = 7.472632884979248\n",
      "Batch 57 / 157\n",
      "training loss = 7.444808006286621\n",
      "validation loss = 7.4805992277044995\n",
      "Batch 58 / 157\n",
      "training loss = 7.3929877281188965\n",
      "validation loss = 7.483563874897204\n",
      "Batch 59 / 157\n",
      "training loss = 7.596187114715576\n",
      "validation loss = 7.47271550329108\n",
      "Batch 60 / 157\n",
      "training loss = 7.531859397888184\n",
      "validation loss = 7.47531459206029\n",
      "Batch 61 / 157\n",
      "training loss = 7.8742194175720215\n",
      "validation loss = 7.483757019042969\n",
      "Batch 62 / 157\n",
      "training loss = 7.202294826507568\n",
      "validation loss = 7.476334872998689\n",
      "Batch 63 / 157\n",
      "training loss = 7.387014865875244\n",
      "validation loss = 7.457697215833162\n",
      "Batch 64 / 157\n",
      "training loss = 7.334886074066162\n",
      "validation loss = 7.454619884490967\n",
      "Batch 65 / 157\n",
      "training loss = 7.546172142028809\n",
      "validation loss = 7.4684936874791195\n",
      "Batch 66 / 157\n",
      "training loss = 7.450122833251953\n",
      "validation loss = 7.470242901852257\n",
      "Batch 67 / 157\n",
      "training loss = 7.6264448165893555\n",
      "validation loss = 7.459357035787482\n",
      "Batch 68 / 157\n",
      "training loss = 7.646718978881836\n",
      "validation loss = 7.449690567819696\n",
      "Batch 69 / 157\n",
      "training loss = 7.428176403045654\n",
      "validation loss = 7.447878536425139\n",
      "Batch 70 / 157\n",
      "training loss = 7.457989692687988\n",
      "validation loss = 7.445674871143542\n",
      "Batch 71 / 157\n",
      "training loss = 7.522676467895508\n",
      "validation loss = 7.4400840056569955\n",
      "Batch 72 / 157\n",
      "training loss = 7.274632930755615\n",
      "validation loss = 7.435153333764327\n",
      "Batch 73 / 157\n",
      "training loss = 7.636668682098389\n",
      "validation loss = 7.430731873763235\n",
      "Batch 74 / 157\n",
      "training loss = 7.4032206535339355\n",
      "validation loss = 7.424209017502634\n",
      "Batch 75 / 157\n",
      "training loss = 7.229536056518555\n",
      "validation loss = 7.417812121541877\n",
      "Batch 76 / 157\n",
      "training loss = 7.346874713897705\n",
      "validation loss = 7.419326104615864\n",
      "Batch 77 / 157\n",
      "training loss = 7.326335430145264\n",
      "validation loss = 7.418476506283409\n",
      "Batch 78 / 157\n",
      "training loss = 7.45734167098999\n",
      "validation loss = 7.414118540914435\n",
      "Batch 79 / 157\n",
      "training loss = 7.612712383270264\n",
      "validation loss = 7.406839245244076\n",
      "Batch 80 / 157\n",
      "training loss = 7.401418685913086\n",
      "validation loss = 7.4066371415790755\n",
      "Batch 81 / 157\n",
      "training loss = 7.351685047149658\n",
      "validation loss = 7.399962525618704\n",
      "Batch 82 / 157\n",
      "training loss = 7.345438480377197\n",
      "validation loss = 7.383272748244436\n",
      "Batch 83 / 157\n",
      "training loss = 7.789435386657715\n",
      "validation loss = 7.386586641010485\n",
      "Batch 84 / 157\n",
      "training loss = 7.3871002197265625\n",
      "validation loss = 7.387857738294099\n",
      "Batch 85 / 157\n",
      "training loss = 7.30341100692749\n",
      "validation loss = 7.381753796025326\n",
      "Batch 86 / 157\n",
      "training loss = 7.555131912231445\n",
      "validation loss = 7.376460878472579\n",
      "Batch 87 / 157\n",
      "training loss = 7.3918867111206055\n",
      "validation loss = 7.375684211128636\n",
      "Batch 88 / 157\n",
      "training loss = 7.486572265625\n",
      "validation loss = 7.372930702410247\n",
      "Batch 89 / 157\n",
      "training loss = 7.357276916503906\n",
      "validation loss = 7.367235635456286\n",
      "Batch 90 / 157\n",
      "training loss = 7.28259801864624\n",
      "validation loss = 7.3851292007847835\n",
      "Batch 91 / 157\n",
      "training loss = 7.337405681610107\n",
      "validation loss = 7.387595126503392\n",
      "Batch 92 / 157\n",
      "training loss = 7.2966413497924805\n",
      "validation loss = 7.37229942020617\n",
      "Batch 93 / 157\n",
      "training loss = 7.343947887420654\n",
      "validation loss = 7.363795782390394\n",
      "Batch 94 / 157\n",
      "training loss = 7.5664591789245605\n",
      "validation loss = 7.3817650142468905\n",
      "Batch 95 / 157\n",
      "training loss = 7.30264949798584\n",
      "validation loss = 7.397261569374486\n",
      "Batch 96 / 157\n",
      "training loss = 7.289597511291504\n",
      "validation loss = 7.3923805638363485\n",
      "Batch 97 / 157\n",
      "training loss = 7.241322994232178\n",
      "validation loss = 7.371420082293059\n",
      "Batch 98 / 157\n",
      "training loss = 7.283504486083984\n",
      "validation loss = 7.36708937193218\n",
      "Batch 99 / 157\n",
      "training loss = 7.549086093902588\n",
      "validation loss = 7.3585491933320695\n",
      "Batch 100 / 157\n",
      "training loss = 7.2186079025268555\n",
      "validation loss = 7.369576830612986\n",
      "Batch 101 / 157\n",
      "training loss = 7.040138244628906\n",
      "validation loss = 7.375343473334062\n",
      "Batch 102 / 157\n",
      "training loss = 7.510848522186279\n",
      "validation loss = 7.358638211300499\n",
      "Batch 103 / 157\n",
      "training loss = 7.536231517791748\n",
      "validation loss = 7.352152598531623\n",
      "Batch 104 / 157\n",
      "training loss = 7.494579315185547\n",
      "validation loss = 7.357697160620439\n",
      "Batch 105 / 157\n",
      "training loss = 7.494487285614014\n",
      "validation loss = 7.353554725646973\n",
      "Batch 106 / 157\n",
      "training loss = 7.387454032897949\n",
      "validation loss = 7.346138703195672\n",
      "Batch 107 / 157\n",
      "training loss = 7.3689422607421875\n",
      "validation loss = 7.348574989720395\n",
      "Batch 108 / 157\n",
      "training loss = 6.961668014526367\n",
      "validation loss = 7.34941725981863\n",
      "Batch 109 / 157\n",
      "training loss = 7.1822919845581055\n",
      "validation loss = 7.347100433550383\n",
      "Batch 110 / 157\n",
      "training loss = 7.336998462677002\n",
      "validation loss = 7.349309896167956\n",
      "Batch 111 / 157\n",
      "training loss = 7.6980719566345215\n",
      "validation loss = 7.349331905967311\n",
      "Batch 112 / 157\n",
      "training loss = 7.2316131591796875\n",
      "validation loss = 7.342355703052721\n",
      "Batch 113 / 157\n",
      "training loss = 7.440885066986084\n",
      "validation loss = 7.3457081694352\n",
      "Batch 114 / 157\n",
      "training loss = 7.33083438873291\n",
      "validation loss = 7.343151870526765\n",
      "Batch 115 / 157\n",
      "training loss = 7.329445838928223\n",
      "validation loss = 7.347126734884162\n",
      "Batch 116 / 157\n",
      "training loss = 7.195362091064453\n",
      "validation loss = 7.33801784013447\n",
      "Batch 117 / 157\n",
      "training loss = 7.396441459655762\n",
      "validation loss = 7.325431648053621\n",
      "Batch 118 / 157\n",
      "training loss = 7.368131637573242\n",
      "validation loss = 7.324787139892578\n",
      "Batch 119 / 157\n",
      "training loss = 7.265749454498291\n",
      "validation loss = 7.319657124971089\n",
      "Batch 120 / 157\n",
      "training loss = 7.330399513244629\n",
      "validation loss = 7.315553138130589\n",
      "Batch 121 / 157\n",
      "training loss = 7.322709560394287\n",
      "validation loss = 7.3190786211114185\n",
      "Batch 122 / 157\n",
      "training loss = 7.204333782196045\n",
      "validation loss = 7.307811109643233\n",
      "Batch 123 / 157\n",
      "training loss = 7.456793785095215\n",
      "validation loss = 7.307968792162444\n",
      "Batch 124 / 157\n",
      "training loss = 7.45114278793335\n",
      "validation loss = 7.314997547551205\n",
      "Batch 125 / 157\n",
      "training loss = 7.1563849449157715\n",
      "validation loss = 7.307296401575992\n",
      "Batch 126 / 157\n",
      "training loss = 7.4753804206848145\n",
      "validation loss = 7.303758194572048\n",
      "Batch 127 / 157\n",
      "training loss = 7.258995532989502\n",
      "validation loss = 7.291369463268079\n",
      "Batch 128 / 157\n",
      "training loss = 7.336848258972168\n",
      "validation loss = 7.305323625865736\n",
      "Batch 129 / 157\n",
      "training loss = 7.122481822967529\n",
      "validation loss = 7.302248804192794\n",
      "Batch 130 / 157\n",
      "training loss = 7.220106601715088\n",
      "validation loss = 7.294574135228207\n",
      "Batch 131 / 157\n",
      "training loss = 7.386673927307129\n",
      "validation loss = 7.288208032909193\n",
      "Batch 132 / 157\n",
      "training loss = 7.312202453613281\n",
      "validation loss = 7.285574385994359\n",
      "Batch 133 / 157\n",
      "training loss = 7.232758045196533\n",
      "validation loss = 7.286665891346178\n",
      "Batch 134 / 157\n",
      "training loss = 7.358105182647705\n",
      "validation loss = 7.281933006487395\n",
      "Batch 135 / 157\n",
      "training loss = 6.951289176940918\n",
      "validation loss = 7.283416547273335\n",
      "Batch 136 / 157\n",
      "training loss = 7.052218914031982\n",
      "validation loss = 7.279077630293997\n",
      "Batch 137 / 157\n",
      "training loss = 7.347764015197754\n",
      "validation loss = 7.279630686107435\n",
      "Batch 138 / 157\n",
      "training loss = 7.304572105407715\n",
      "validation loss = 7.273302103343763\n",
      "Batch 139 / 157\n",
      "training loss = 7.245101451873779\n",
      "validation loss = 7.278843528346012\n",
      "Batch 140 / 157\n",
      "training loss = 7.301625728607178\n",
      "validation loss = 7.276351351487009\n",
      "Batch 141 / 157\n",
      "training loss = 7.100340843200684\n",
      "validation loss = 7.272429792504561\n",
      "Batch 142 / 157\n",
      "training loss = 7.06137228012085\n",
      "validation loss = 7.273250805704217\n",
      "Batch 143 / 157\n",
      "training loss = 6.943633556365967\n",
      "validation loss = 7.273697451541298\n",
      "Batch 144 / 157\n",
      "training loss = 7.291146278381348\n",
      "validation loss = 7.277332832938747\n",
      "Batch 145 / 157\n",
      "training loss = 6.782070159912109\n",
      "validation loss = 7.273803736034193\n",
      "Batch 146 / 157\n",
      "training loss = 7.190767765045166\n",
      "validation loss = 7.266627612866853\n",
      "Batch 147 / 157\n",
      "training loss = 7.276986598968506\n",
      "validation loss = 7.264393731167442\n",
      "Batch 148 / 157\n",
      "training loss = 7.2344770431518555\n",
      "validation loss = 7.259239698711195\n",
      "Batch 149 / 157\n",
      "training loss = 7.222027778625488\n",
      "validation loss = 7.264561251590126\n",
      "Batch 150 / 157\n",
      "training loss = 6.991538047790527\n",
      "validation loss = 7.253863033495452\n",
      "Batch 151 / 157\n",
      "training loss = 7.085617542266846\n",
      "validation loss = 7.251655402936433\n",
      "Batch 152 / 157\n",
      "training loss = 7.366127014160156\n",
      "validation loss = 7.249862545414975\n",
      "Batch 153 / 157\n",
      "training loss = 7.415589809417725\n",
      "validation loss = 7.250455254002621\n",
      "Batch 154 / 157\n",
      "training loss = 7.049488544464111\n",
      "validation loss = 7.253996246739438\n",
      "Batch 155 / 157\n",
      "training loss = 7.323826789855957\n",
      "validation loss = 7.252119089427747\n",
      "Batch 156 / 157\n",
      "training loss = 6.961824893951416\n",
      "validation loss = 7.23821695227372\n",
      "Batch 157 / 157\n",
      "training loss = 7.3168110847473145\n",
      "validation loss = 7.241615295410156\n",
      "Average training loss: 7.52\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.368144989013672\n",
      "validation loss = 10.14078496631823\n",
      "Batch 2 / 157\n",
      "training loss = 10.166011810302734\n",
      "validation loss = 9.706256715874924\n",
      "Batch 3 / 157\n",
      "training loss = 9.746667861938477\n",
      "validation loss = 9.364085448415656\n",
      "Batch 4 / 157\n",
      "training loss = 9.360369682312012\n",
      "validation loss = 9.036774083187705\n",
      "Batch 5 / 157\n",
      "training loss = 8.982776641845703\n",
      "validation loss = 8.747211606878983\n",
      "Batch 6 / 157\n",
      "training loss = 8.689399719238281\n",
      "validation loss = 8.481302512319465\n",
      "Batch 7 / 157\n",
      "training loss = 8.643807411193848\n",
      "validation loss = 8.279894602926154\n",
      "Batch 8 / 157\n",
      "training loss = 8.285165786743164\n",
      "validation loss = 8.159229102887606\n",
      "Batch 9 / 157\n",
      "training loss = 8.003961563110352\n",
      "validation loss = 8.100300035978618\n",
      "Batch 10 / 157\n",
      "training loss = 8.082964897155762\n",
      "validation loss = 8.114134412062796\n",
      "Batch 11 / 157\n",
      "training loss = 7.9827351570129395\n",
      "validation loss = 8.134729360279284\n",
      "Batch 12 / 157\n",
      "training loss = 8.06266975402832\n",
      "validation loss = 8.10904766383924\n",
      "Batch 13 / 157\n",
      "training loss = 8.088297843933105\n",
      "validation loss = 8.002168404428582\n",
      "Batch 14 / 157\n",
      "training loss = 8.047956466674805\n",
      "validation loss = 9.327060448495965\n",
      "Batch 15 / 157\n",
      "training loss = 9.47687816619873\n",
      "validation loss = 7.981537517748381\n",
      "Batch 16 / 157\n",
      "training loss = 7.450961589813232\n",
      "validation loss = 8.074136106591476\n",
      "Batch 17 / 157\n",
      "training loss = 8.887676239013672\n",
      "validation loss = 8.102171847694798\n",
      "Batch 18 / 157\n",
      "training loss = 8.247780799865723\n",
      "validation loss = 8.064207629153604\n",
      "Batch 19 / 157\n",
      "training loss = 7.596477031707764\n",
      "validation loss = 8.011533084668612\n",
      "Batch 20 / 157\n",
      "training loss = 8.034428596496582\n",
      "validation loss = 7.949025982304623\n",
      "Batch 21 / 157\n",
      "training loss = 8.088435173034668\n",
      "validation loss = 7.924656968367727\n",
      "Batch 22 / 157\n",
      "training loss = 7.543725967407227\n",
      "validation loss = 7.945075838189376\n",
      "Batch 23 / 157\n",
      "training loss = 8.143619537353516\n",
      "validation loss = 7.95350268012599\n",
      "Batch 24 / 157\n",
      "training loss = 7.911660194396973\n",
      "validation loss = 7.94239942651046\n",
      "Batch 25 / 157\n",
      "training loss = 7.967010021209717\n",
      "validation loss = 7.917534451735647\n",
      "Batch 26 / 157\n",
      "training loss = 7.7930755615234375\n",
      "validation loss = 7.913265429045024\n",
      "Batch 27 / 157\n",
      "training loss = 7.845395088195801\n",
      "validation loss = 7.912352737627532\n",
      "Batch 28 / 157\n",
      "training loss = 7.594669342041016\n",
      "validation loss = 7.923590032677901\n",
      "Batch 29 / 157\n",
      "training loss = 8.12043285369873\n",
      "validation loss = 7.921664263072767\n",
      "Batch 30 / 157\n",
      "training loss = 7.888350009918213\n",
      "validation loss = 7.898268950612922\n",
      "Batch 31 / 157\n",
      "training loss = 8.102997779846191\n",
      "validation loss = 7.877673349882427\n",
      "Batch 32 / 157\n",
      "training loss = 7.6386494636535645\n",
      "validation loss = 7.872672858991121\n",
      "Batch 33 / 157\n",
      "training loss = 7.561514377593994\n",
      "validation loss = 7.871527044396651\n",
      "Batch 34 / 157\n",
      "training loss = 8.046113014221191\n",
      "validation loss = 7.877293185183876\n",
      "Batch 35 / 157\n",
      "training loss = 7.922107696533203\n",
      "validation loss = 7.871271660453395\n",
      "Batch 36 / 157\n",
      "training loss = 7.858299732208252\n",
      "validation loss = 7.852149787702058\n",
      "Batch 37 / 157\n",
      "training loss = 7.717508792877197\n",
      "validation loss = 7.841814467781468\n",
      "Batch 38 / 157\n",
      "training loss = 8.135242462158203\n",
      "validation loss = 7.826938077023155\n",
      "Batch 39 / 157\n",
      "training loss = 7.603077411651611\n",
      "validation loss = 7.815214859811883\n",
      "Batch 40 / 157\n",
      "training loss = 7.890311241149902\n",
      "validation loss = 7.796806134675679\n",
      "Batch 41 / 157\n",
      "training loss = 7.649844169616699\n",
      "validation loss = 7.7772903191415885\n",
      "Batch 42 / 157\n",
      "training loss = 7.834798336029053\n",
      "validation loss = 7.7847980449074194\n",
      "Batch 43 / 157\n",
      "training loss = 7.714024543762207\n",
      "validation loss = 7.759340587415193\n",
      "Batch 44 / 157\n",
      "training loss = 8.016176223754883\n",
      "validation loss = 7.746959861956145\n",
      "Batch 45 / 157\n",
      "training loss = 7.720320701599121\n",
      "validation loss = 7.743409357572856\n",
      "Batch 46 / 157\n",
      "training loss = 7.356365203857422\n",
      "validation loss = 7.755253490648772\n",
      "Batch 47 / 157\n",
      "training loss = 7.604398727416992\n",
      "validation loss = 7.749809214943333\n",
      "Batch 48 / 157\n",
      "training loss = 7.92706298828125\n",
      "validation loss = 7.734590128848427\n",
      "Batch 49 / 157\n",
      "training loss = 7.841116428375244\n",
      "validation loss = 7.782025864249782\n",
      "Batch 50 / 157\n",
      "training loss = 7.670705795288086\n",
      "validation loss = 7.743551229175768\n",
      "Batch 51 / 157\n",
      "training loss = 7.9077839851379395\n",
      "validation loss = 7.769606314207378\n",
      "Batch 52 / 157\n",
      "training loss = 7.397968292236328\n",
      "validation loss = 7.778771952578896\n",
      "Batch 53 / 157\n",
      "training loss = 8.053726196289062\n",
      "validation loss = 7.743330328088057\n",
      "Batch 54 / 157\n",
      "training loss = 7.943905830383301\n",
      "validation loss = 7.791599072908101\n",
      "Batch 55 / 157\n",
      "training loss = 7.885867118835449\n",
      "validation loss = 7.7742250342118115\n",
      "Batch 56 / 157\n",
      "training loss = 7.923875331878662\n",
      "validation loss = 7.741528310273823\n",
      "Batch 57 / 157\n",
      "training loss = 7.600351810455322\n",
      "validation loss = 7.762959455188952\n",
      "Batch 58 / 157\n",
      "training loss = 7.526801586151123\n",
      "validation loss = 7.7800083662334245\n",
      "Batch 59 / 157\n",
      "training loss = 7.745853900909424\n",
      "validation loss = 7.738933914586117\n",
      "Batch 60 / 157\n",
      "training loss = 7.772514820098877\n",
      "validation loss = 7.722856822766755\n",
      "Batch 61 / 157\n",
      "training loss = 7.391840934753418\n",
      "validation loss = 7.723578302483809\n",
      "Batch 62 / 157\n",
      "training loss = 7.856341361999512\n",
      "validation loss = 7.710214589771471\n",
      "Batch 63 / 157\n",
      "training loss = 7.756651401519775\n",
      "validation loss = 7.703142893941779\n",
      "Batch 64 / 157\n",
      "training loss = 7.574277877807617\n",
      "validation loss = 7.705848643654271\n",
      "Batch 65 / 157\n",
      "training loss = 7.732972621917725\n",
      "validation loss = 7.688941177568938\n",
      "Batch 66 / 157\n",
      "training loss = 7.7505598068237305\n",
      "validation loss = 7.678011593065764\n",
      "Batch 67 / 157\n",
      "training loss = 7.876446723937988\n",
      "validation loss = 7.711374232643529\n",
      "Batch 68 / 157\n",
      "training loss = 7.909747123718262\n",
      "validation loss = 7.7072759427522355\n",
      "Batch 69 / 157\n",
      "training loss = 7.661680221557617\n",
      "validation loss = 7.697203862039666\n",
      "Batch 70 / 157\n",
      "training loss = 7.824860572814941\n",
      "validation loss = 7.66385891563014\n",
      "Batch 71 / 157\n",
      "training loss = 7.99508810043335\n",
      "validation loss = 7.671473302339253\n",
      "Batch 72 / 157\n",
      "training loss = 7.7061591148376465\n",
      "validation loss = 7.691548071409526\n",
      "Batch 73 / 157\n",
      "training loss = 7.4001264572143555\n",
      "validation loss = 7.669036087236907\n",
      "Batch 74 / 157\n",
      "training loss = 7.478907585144043\n",
      "validation loss = 7.705322792655544\n",
      "Batch 75 / 157\n",
      "training loss = 7.807416915893555\n",
      "validation loss = 7.685202949925473\n",
      "Batch 76 / 157\n",
      "training loss = 7.781374931335449\n",
      "validation loss = 7.643308313269364\n",
      "Batch 77 / 157\n",
      "training loss = 7.766354560852051\n",
      "validation loss = 7.682187657607229\n",
      "Batch 78 / 157\n",
      "training loss = 7.763283729553223\n",
      "validation loss = 7.678307407780697\n",
      "Batch 79 / 157\n",
      "training loss = 7.799075603485107\n",
      "validation loss = 7.632261577405427\n",
      "Batch 80 / 157\n",
      "training loss = 7.528201103210449\n",
      "validation loss = 7.658979265313399\n",
      "Batch 81 / 157\n",
      "training loss = 7.383404731750488\n",
      "validation loss = 7.684202796534488\n",
      "Batch 82 / 157\n",
      "training loss = 7.367894172668457\n",
      "validation loss = 7.661264896392822\n",
      "Batch 83 / 157\n",
      "training loss = 7.937994480133057\n",
      "validation loss = 7.611368380094829\n",
      "Batch 84 / 157\n",
      "training loss = 7.421969413757324\n",
      "validation loss = 7.651961502275969\n",
      "Batch 85 / 157\n",
      "training loss = 7.82059383392334\n",
      "validation loss = 7.67046258324071\n",
      "Batch 86 / 157\n",
      "training loss = 8.06094741821289\n",
      "validation loss = 7.640969803458766\n",
      "Batch 87 / 157\n",
      "training loss = 7.869714260101318\n",
      "validation loss = 7.6031518484416765\n",
      "Batch 88 / 157\n",
      "training loss = 7.356105327606201\n",
      "validation loss = 7.631863845022101\n",
      "Batch 89 / 157\n",
      "training loss = 7.768306255340576\n",
      "validation loss = 7.632659811722605\n",
      "Batch 90 / 157\n",
      "training loss = 7.264981269836426\n",
      "validation loss = 7.608383730838173\n",
      "Batch 91 / 157\n",
      "training loss = 7.356598377227783\n",
      "validation loss = 7.580387240961978\n",
      "Batch 92 / 157\n",
      "training loss = 7.812150955200195\n",
      "validation loss = 7.5974876002261515\n",
      "Batch 93 / 157\n",
      "training loss = 7.459410667419434\n",
      "validation loss = 7.588971464257491\n",
      "Batch 94 / 157\n",
      "training loss = 7.740496635437012\n",
      "validation loss = 7.566994140022679\n",
      "Batch 95 / 157\n",
      "training loss = 7.989943504333496\n",
      "validation loss = 7.566830785650956\n",
      "Batch 96 / 157\n",
      "training loss = 7.587889194488525\n",
      "validation loss = 7.575022170418187\n",
      "Batch 97 / 157\n",
      "training loss = 7.258974552154541\n",
      "validation loss = 7.564946124428197\n",
      "Batch 98 / 157\n",
      "training loss = 7.630949974060059\n",
      "validation loss = 7.546864760549445\n",
      "Batch 99 / 157\n",
      "training loss = 7.455894947052002\n",
      "validation loss = 7.540609761288292\n",
      "Batch 100 / 157\n",
      "training loss = 7.945056438446045\n",
      "validation loss = 7.574230545445492\n",
      "Batch 101 / 157\n",
      "training loss = 7.156188011169434\n",
      "validation loss = 7.521141729856792\n",
      "Batch 102 / 157\n",
      "training loss = 7.828633785247803\n",
      "validation loss = 7.53669073707179\n",
      "Batch 103 / 157\n",
      "training loss = 7.472512722015381\n",
      "validation loss = 7.526700095126503\n",
      "Batch 104 / 157\n",
      "training loss = 7.744063854217529\n",
      "validation loss = 7.50823638313695\n",
      "Batch 105 / 157\n",
      "training loss = 7.397255897521973\n",
      "validation loss = 7.517480298092491\n",
      "Batch 106 / 157\n",
      "training loss = 7.667501926422119\n",
      "validation loss = 7.507811922776072\n",
      "Batch 107 / 157\n",
      "training loss = 7.768383979797363\n",
      "validation loss = 7.503108175177323\n",
      "Batch 108 / 157\n",
      "training loss = 7.753086566925049\n",
      "validation loss = 7.502817354704204\n",
      "Batch 109 / 157\n",
      "training loss = 7.588505268096924\n",
      "validation loss = 7.499917080527858\n",
      "Batch 110 / 157\n",
      "training loss = 8.053801536560059\n",
      "validation loss = 7.511587695071571\n",
      "Batch 111 / 157\n",
      "training loss = 7.449625492095947\n",
      "validation loss = 7.479867282666658\n",
      "Batch 112 / 157\n",
      "training loss = 6.761011600494385\n",
      "validation loss = 7.491367440474661\n",
      "Batch 113 / 157\n",
      "training loss = 7.572016716003418\n",
      "validation loss = 7.493376731872559\n",
      "Batch 114 / 157\n",
      "training loss = 7.36484432220459\n",
      "validation loss = 7.490304645739104\n",
      "Batch 115 / 157\n",
      "training loss = 6.811570644378662\n",
      "validation loss = 7.485993786862022\n",
      "Batch 116 / 157\n",
      "training loss = 7.5104451179504395\n",
      "validation loss = 7.468836307525635\n",
      "Batch 117 / 157\n",
      "training loss = 7.741224765777588\n",
      "validation loss = 7.477428536666067\n",
      "Batch 118 / 157\n",
      "training loss = 7.682403564453125\n",
      "validation loss = 7.480381237833123\n",
      "Batch 119 / 157\n",
      "training loss = 7.892643928527832\n",
      "validation loss = 7.457818181891191\n",
      "Batch 120 / 157\n",
      "training loss = 7.157061576843262\n",
      "validation loss = 7.494379570609645\n",
      "Batch 121 / 157\n",
      "training loss = 7.393593788146973\n",
      "validation loss = 7.481825025458085\n",
      "Batch 122 / 157\n",
      "training loss = 7.862175941467285\n",
      "validation loss = 7.502099664587724\n",
      "Batch 123 / 157\n",
      "training loss = 7.387681484222412\n",
      "validation loss = 7.469823636506733\n",
      "Batch 124 / 157\n",
      "training loss = 7.30035924911499\n",
      "validation loss = 7.467088548760665\n",
      "Batch 125 / 157\n",
      "training loss = 7.719005107879639\n",
      "validation loss = 7.447387569829037\n",
      "Batch 126 / 157\n",
      "training loss = 6.942873954772949\n",
      "validation loss = 7.453786925265663\n",
      "Batch 127 / 157\n",
      "training loss = 7.230533599853516\n",
      "validation loss = 7.448356427644429\n",
      "Batch 128 / 157\n",
      "training loss = 7.318398475646973\n",
      "validation loss = 7.4522688263341\n",
      "Batch 129 / 157\n",
      "training loss = 7.550938129425049\n",
      "validation loss = 7.459711024635716\n",
      "Batch 130 / 157\n",
      "training loss = 7.435293674468994\n",
      "validation loss = 7.44812358053107\n",
      "Batch 131 / 157\n",
      "training loss = 7.100214958190918\n",
      "validation loss = 7.450924195741353\n",
      "Batch 132 / 157\n",
      "training loss = 7.399444103240967\n",
      "validation loss = 7.424902765374434\n",
      "Batch 133 / 157\n",
      "training loss = 7.304267883300781\n",
      "validation loss = 7.4126709636889005\n",
      "Batch 134 / 157\n",
      "training loss = 7.1905317306518555\n",
      "validation loss = 7.41572706322921\n",
      "Batch 135 / 157\n",
      "training loss = 7.250173091888428\n",
      "validation loss = 7.395077128159373\n",
      "Batch 136 / 157\n",
      "training loss = 7.4640421867370605\n",
      "validation loss = 7.394981685437654\n",
      "Batch 137 / 157\n",
      "training loss = 7.200191020965576\n",
      "validation loss = 7.388084311234324\n",
      "Batch 138 / 157\n",
      "training loss = 7.186586380004883\n",
      "validation loss = 7.37784975453427\n",
      "Batch 139 / 157\n",
      "training loss = 7.247597694396973\n",
      "validation loss = 7.3891065246180485\n",
      "Batch 140 / 157\n",
      "training loss = 7.002105712890625\n",
      "validation loss = 7.378518982937462\n",
      "Batch 141 / 157\n",
      "training loss = 7.28285551071167\n",
      "validation loss = 7.373341710943925\n",
      "Batch 142 / 157\n",
      "training loss = 7.544665336608887\n",
      "validation loss = 7.3527350425720215\n",
      "Batch 143 / 157\n",
      "training loss = 7.264840126037598\n",
      "validation loss = 7.39031834351389\n",
      "Batch 144 / 157\n",
      "training loss = 7.821345806121826\n",
      "validation loss = 7.363638024581106\n",
      "Batch 145 / 157\n",
      "training loss = 7.275246620178223\n",
      "validation loss = 7.367729488172029\n",
      "Batch 146 / 157\n",
      "training loss = 7.153853416442871\n",
      "validation loss = 7.369127800590114\n",
      "Batch 147 / 157\n",
      "training loss = 7.041233539581299\n",
      "validation loss = 7.396724274283962\n",
      "Batch 148 / 157\n",
      "training loss = 7.574946880340576\n",
      "validation loss = 7.37077155866121\n",
      "Batch 149 / 157\n",
      "training loss = 7.388899326324463\n",
      "validation loss = 7.381076938227603\n",
      "Batch 150 / 157\n",
      "training loss = 7.487420558929443\n",
      "validation loss = 7.3807524379931\n",
      "Batch 151 / 157\n",
      "training loss = 7.653632640838623\n",
      "validation loss = 7.352190243570428\n",
      "Batch 152 / 157\n",
      "training loss = 7.49726676940918\n",
      "validation loss = 7.3702426207693\n",
      "Batch 153 / 157\n",
      "training loss = 7.2651543617248535\n",
      "validation loss = 7.3674289050855135\n",
      "Batch 154 / 157\n",
      "training loss = 7.238163948059082\n",
      "validation loss = 7.328351999583997\n",
      "Batch 155 / 157\n",
      "training loss = 7.411283493041992\n",
      "validation loss = 7.357047306863885\n",
      "Batch 156 / 157\n",
      "training loss = 7.585815906524658\n",
      "validation loss = 7.343539438749614\n",
      "Batch 157 / 157\n",
      "training loss = 7.7625298500061035\n",
      "validation loss = 7.3687037919697005\n",
      "Average training loss: 7.74\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.39028549194336\n",
      "validation loss = 10.201684349461607\n",
      "Batch 2 / 157\n",
      "training loss = 10.219209671020508\n",
      "validation loss = 9.676491636978952\n",
      "Batch 3 / 157\n",
      "training loss = 9.704262733459473\n",
      "validation loss = 9.266256984911466\n",
      "Batch 4 / 157\n",
      "training loss = 9.236340522766113\n",
      "validation loss = 8.91864239542108\n",
      "Batch 5 / 157\n",
      "training loss = 8.913695335388184\n",
      "validation loss = 8.617967455010666\n",
      "Batch 6 / 157\n",
      "training loss = 8.607368469238281\n",
      "validation loss = 8.370584638495194\n",
      "Batch 7 / 157\n",
      "training loss = 8.472514152526855\n",
      "validation loss = 8.189593465704666\n",
      "Batch 8 / 157\n",
      "training loss = 8.148333549499512\n",
      "validation loss = 8.06633304294787\n",
      "Batch 9 / 157\n",
      "training loss = 8.015358924865723\n",
      "validation loss = 7.989837521000912\n",
      "Batch 10 / 157\n",
      "training loss = 7.978244304656982\n",
      "validation loss = 7.960208165018182\n",
      "Batch 11 / 157\n",
      "training loss = 8.05164909362793\n",
      "validation loss = 7.952620531383314\n",
      "Batch 12 / 157\n",
      "training loss = 8.254020690917969\n",
      "validation loss = 7.941205677233245\n",
      "Batch 13 / 157\n",
      "training loss = 7.906741142272949\n",
      "validation loss = 7.910783566926655\n",
      "Batch 14 / 157\n",
      "training loss = 8.000041961669922\n",
      "validation loss = 7.8548016799123666\n",
      "Batch 15 / 157\n",
      "training loss = 7.746084213256836\n",
      "validation loss = 7.905281342958149\n",
      "Batch 16 / 157\n",
      "training loss = 7.891798496246338\n",
      "validation loss = 7.832684542003431\n",
      "Batch 17 / 157\n",
      "training loss = 7.975000858306885\n",
      "validation loss = 7.840530019057424\n",
      "Batch 18 / 157\n",
      "training loss = 7.729748249053955\n",
      "validation loss = 7.840897886376632\n",
      "Batch 19 / 157\n",
      "training loss = 7.593982219696045\n",
      "validation loss = 7.816226432197972\n",
      "Batch 20 / 157\n",
      "training loss = 7.847949981689453\n",
      "validation loss = 7.787157460262901\n",
      "Batch 21 / 157\n",
      "training loss = 7.71242094039917\n",
      "validation loss = 7.78138522097939\n",
      "Batch 22 / 157\n",
      "training loss = 7.760273456573486\n",
      "validation loss = 7.779485150387413\n",
      "Batch 23 / 157\n",
      "training loss = 7.862553596496582\n",
      "validation loss = 7.767447923359118\n",
      "Batch 24 / 157\n",
      "training loss = 7.956396579742432\n",
      "validation loss = 7.748875919141267\n",
      "Batch 25 / 157\n",
      "training loss = 7.989609718322754\n",
      "validation loss = 7.73655514968069\n",
      "Batch 26 / 157\n",
      "training loss = 7.761111259460449\n",
      "validation loss = 7.723155724374871\n",
      "Batch 27 / 157\n",
      "training loss = 7.781198978424072\n",
      "validation loss = 7.709382132480019\n",
      "Batch 28 / 157\n",
      "training loss = 7.682210445404053\n",
      "validation loss = 7.697329245115581\n",
      "Batch 29 / 157\n",
      "training loss = 7.761927604675293\n",
      "validation loss = 7.690165845971358\n",
      "Batch 30 / 157\n",
      "training loss = 7.670433521270752\n",
      "validation loss = 7.68100791228445\n",
      "Batch 31 / 157\n",
      "training loss = 7.817969799041748\n",
      "validation loss = 7.671140118649132\n",
      "Batch 32 / 157\n",
      "training loss = 7.688324928283691\n",
      "validation loss = 7.6557769775390625\n",
      "Batch 33 / 157\n",
      "training loss = 7.810775279998779\n",
      "validation loss = 7.65163527036968\n",
      "Batch 34 / 157\n",
      "training loss = 7.535837173461914\n",
      "validation loss = 7.639804865184583\n",
      "Batch 35 / 157\n",
      "training loss = 7.481809616088867\n",
      "validation loss = 7.651107311248779\n",
      "Batch 36 / 157\n",
      "training loss = 7.640385627746582\n",
      "validation loss = 7.658177451083534\n",
      "Batch 37 / 157\n",
      "training loss = 7.547360420227051\n",
      "validation loss = 7.649251611609208\n",
      "Batch 38 / 157\n",
      "training loss = 7.493659973144531\n",
      "validation loss = 7.627960957978901\n",
      "Batch 39 / 157\n",
      "training loss = 7.713533401489258\n",
      "validation loss = 7.612255899529708\n",
      "Batch 40 / 157\n",
      "training loss = 7.73109769821167\n",
      "validation loss = 7.634297019556949\n",
      "Batch 41 / 157\n",
      "training loss = 7.819856643676758\n",
      "validation loss = 7.60911246349937\n",
      "Batch 42 / 157\n",
      "training loss = 7.306214332580566\n",
      "validation loss = 7.593907004908512\n",
      "Batch 43 / 157\n",
      "training loss = 7.773496627807617\n",
      "validation loss = 7.607362446032073\n",
      "Batch 44 / 157\n",
      "training loss = 7.554348945617676\n",
      "validation loss = 7.60067111567447\n",
      "Batch 45 / 157\n",
      "training loss = 7.614467144012451\n",
      "validation loss = 7.587633659965114\n",
      "Batch 46 / 157\n",
      "training loss = 7.596144199371338\n",
      "validation loss = 7.595604670675177\n",
      "Batch 47 / 157\n",
      "training loss = 7.369504928588867\n",
      "validation loss = 7.591408478586297\n",
      "Batch 48 / 157\n",
      "training loss = 7.697766304016113\n",
      "validation loss = 7.563840088091399\n",
      "Batch 49 / 157\n",
      "training loss = 7.4705810546875\n",
      "validation loss = 7.566105516333329\n",
      "Batch 50 / 157\n",
      "training loss = 7.597754001617432\n",
      "validation loss = 7.568395865590949\n",
      "Batch 51 / 157\n",
      "training loss = 7.375404357910156\n",
      "validation loss = 7.554748259092632\n",
      "Batch 52 / 157\n",
      "training loss = 7.5334696769714355\n",
      "validation loss = 7.532927538219251\n",
      "Batch 53 / 157\n",
      "training loss = 7.518013000488281\n",
      "validation loss = 7.5292106427644425\n",
      "Batch 54 / 157\n",
      "training loss = 7.6449103355407715\n",
      "validation loss = 7.526778447000604\n",
      "Batch 55 / 157\n",
      "training loss = 7.641632080078125\n",
      "validation loss = 7.518385912242689\n",
      "Batch 56 / 157\n",
      "training loss = 7.669663906097412\n",
      "validation loss = 7.506305343226383\n",
      "Batch 57 / 157\n",
      "training loss = 7.720195770263672\n",
      "validation loss = 7.508411934501247\n",
      "Batch 58 / 157\n",
      "training loss = 7.672646522521973\n",
      "validation loss = 7.506992239701121\n",
      "Batch 59 / 157\n",
      "training loss = 7.618274211883545\n",
      "validation loss = 7.501505425101833\n",
      "Batch 60 / 157\n",
      "training loss = 7.463309288024902\n",
      "validation loss = 7.491778825458727\n",
      "Batch 61 / 157\n",
      "training loss = 7.564253807067871\n",
      "validation loss = 7.483702734897011\n",
      "Batch 62 / 157\n",
      "training loss = 7.247891426086426\n",
      "validation loss = 7.482889401285272\n",
      "Batch 63 / 157\n",
      "training loss = 7.617928981781006\n",
      "validation loss = 7.472687545575593\n",
      "Batch 64 / 157\n",
      "training loss = 7.554900646209717\n",
      "validation loss = 7.474090174624794\n",
      "Batch 65 / 157\n",
      "training loss = 7.3974409103393555\n",
      "validation loss = 7.46567281923796\n",
      "Batch 66 / 157\n",
      "training loss = 7.46356725692749\n",
      "validation loss = 7.453656046014083\n",
      "Batch 67 / 157\n",
      "training loss = 7.432918548583984\n",
      "validation loss = 7.449167879004228\n",
      "Batch 68 / 157\n",
      "training loss = 7.373965740203857\n",
      "validation loss = 7.443478007065623\n",
      "Batch 69 / 157\n",
      "training loss = 7.732074737548828\n",
      "validation loss = 7.441348954250938\n",
      "Batch 70 / 157\n",
      "training loss = 7.265543460845947\n",
      "validation loss = 7.437044695803993\n",
      "Batch 71 / 157\n",
      "training loss = 7.401449203491211\n",
      "validation loss = 7.437921298177619\n",
      "Batch 72 / 157\n",
      "training loss = 7.524353981018066\n",
      "validation loss = 7.426732665614078\n",
      "Batch 73 / 157\n",
      "training loss = 7.511582851409912\n",
      "validation loss = 7.428167368236341\n",
      "Batch 74 / 157\n",
      "training loss = 7.469391345977783\n",
      "validation loss = 7.430144309997559\n",
      "Batch 75 / 157\n",
      "training loss = 7.480742454528809\n",
      "validation loss = 7.421077502401252\n",
      "Batch 76 / 157\n",
      "training loss = 7.435760974884033\n",
      "validation loss = 7.414050955521433\n",
      "Batch 77 / 157\n",
      "training loss = 7.270606994628906\n",
      "validation loss = 7.41081398411801\n",
      "Batch 78 / 157\n",
      "training loss = 7.489806652069092\n",
      "validation loss = 7.410256310513145\n",
      "Batch 79 / 157\n",
      "training loss = 7.405577182769775\n",
      "validation loss = 7.414836080450761\n",
      "Batch 80 / 157\n",
      "training loss = 7.937186241149902\n",
      "validation loss = 7.477380250629626\n",
      "Batch 81 / 157\n",
      "training loss = 7.507993221282959\n",
      "validation loss = 7.413448634900544\n",
      "Batch 82 / 157\n",
      "training loss = 7.340003490447998\n",
      "validation loss = 7.446313807838841\n",
      "Batch 83 / 157\n",
      "training loss = 7.755178928375244\n",
      "validation loss = 7.426028603001645\n",
      "Batch 84 / 157\n",
      "training loss = 7.237929821014404\n",
      "validation loss = 7.419588515633031\n",
      "Batch 85 / 157\n",
      "training loss = 7.68263053894043\n",
      "validation loss = 7.47123939112613\n",
      "Batch 86 / 157\n",
      "training loss = 7.251686096191406\n",
      "validation loss = 7.4129531759964795\n",
      "Batch 87 / 157\n",
      "training loss = 7.316483974456787\n",
      "validation loss = 7.414000962909899\n",
      "Batch 88 / 157\n",
      "training loss = 7.369545936584473\n",
      "validation loss = 7.442758660567434\n",
      "Batch 89 / 157\n",
      "training loss = 7.491245746612549\n",
      "validation loss = 7.450381429571855\n",
      "Batch 90 / 157\n",
      "training loss = 7.460310935974121\n",
      "validation loss = 7.42296670612536\n",
      "Batch 91 / 157\n",
      "training loss = 7.554709434509277\n",
      "validation loss = 7.39424803382472\n",
      "Batch 92 / 157\n",
      "training loss = 7.4932684898376465\n",
      "validation loss = 7.393172715839587\n",
      "Batch 93 / 157\n",
      "training loss = 7.439915657043457\n",
      "validation loss = 7.403009640543084\n",
      "Batch 94 / 157\n",
      "training loss = 7.422397613525391\n",
      "validation loss = 7.40264526166414\n",
      "Batch 95 / 157\n",
      "training loss = 7.662726879119873\n",
      "validation loss = 7.407741446244089\n",
      "Batch 96 / 157\n",
      "training loss = 7.673080921173096\n",
      "validation loss = 7.398223575792815\n",
      "Batch 97 / 157\n",
      "training loss = 7.444812297821045\n",
      "validation loss = 7.399371172252454\n",
      "Batch 98 / 157\n",
      "training loss = 7.4112982749938965\n",
      "validation loss = 7.3932693381058545\n",
      "Batch 99 / 157\n",
      "training loss = 7.199610233306885\n",
      "validation loss = 7.382587884601794\n",
      "Batch 100 / 157\n",
      "training loss = 7.445292949676514\n",
      "validation loss = 7.384971919812654\n",
      "Batch 101 / 157\n",
      "training loss = 7.438223361968994\n",
      "validation loss = 7.3815632117422005\n",
      "Batch 102 / 157\n",
      "training loss = 7.32665491104126\n",
      "validation loss = 7.369162333639045\n",
      "Batch 103 / 157\n",
      "training loss = 7.402709484100342\n",
      "validation loss = 7.364792146180806\n",
      "Batch 104 / 157\n",
      "training loss = 7.657102108001709\n",
      "validation loss = 7.366862121381257\n",
      "Batch 105 / 157\n",
      "training loss = 7.5460076332092285\n",
      "validation loss = 7.364105550866378\n",
      "Batch 106 / 157\n",
      "training loss = 7.406086444854736\n",
      "validation loss = 7.363227668561433\n",
      "Batch 107 / 157\n",
      "training loss = 7.181572437286377\n",
      "validation loss = 7.351789825841\n",
      "Batch 108 / 157\n",
      "training loss = 7.286966323852539\n",
      "validation loss = 7.355915897770932\n",
      "Batch 109 / 157\n",
      "training loss = 7.375342845916748\n",
      "validation loss = 7.344517130600779\n",
      "Batch 110 / 157\n",
      "training loss = 7.633599281311035\n",
      "validation loss = 7.351085311488101\n",
      "Batch 111 / 157\n",
      "training loss = 7.418582439422607\n",
      "validation loss = 7.350894350754587\n",
      "Batch 112 / 157\n",
      "training loss = 7.3750481605529785\n",
      "validation loss = 7.343713333732204\n",
      "Batch 113 / 157\n",
      "training loss = 7.529764175415039\n",
      "validation loss = 7.342090531399376\n",
      "Batch 114 / 157\n",
      "training loss = 7.447327136993408\n",
      "validation loss = 7.349565380497983\n",
      "Batch 115 / 157\n",
      "training loss = 7.134607791900635\n",
      "validation loss = 7.341249842392771\n",
      "Batch 116 / 157\n",
      "training loss = 7.343405723571777\n",
      "validation loss = 7.33500327562031\n",
      "Batch 117 / 157\n",
      "training loss = 7.237123489379883\n",
      "validation loss = 7.3304706372712785\n",
      "Batch 118 / 157\n",
      "training loss = 7.213316440582275\n",
      "validation loss = 7.339529539409437\n",
      "Batch 119 / 157\n",
      "training loss = 7.543121814727783\n",
      "validation loss = 7.332309773093776\n",
      "Batch 120 / 157\n",
      "training loss = 7.194225788116455\n",
      "validation loss = 7.329244513260691\n",
      "Batch 121 / 157\n",
      "training loss = 7.65028715133667\n",
      "validation loss = 7.3296999429401595\n",
      "Batch 122 / 157\n",
      "training loss = 7.5816168785095215\n",
      "validation loss = 7.339325352718956\n",
      "Batch 123 / 157\n",
      "training loss = 7.411490440368652\n",
      "validation loss = 7.337072397533216\n",
      "Batch 124 / 157\n",
      "training loss = 7.191740036010742\n",
      "validation loss = 7.333919324372944\n",
      "Batch 125 / 157\n",
      "training loss = 7.113392353057861\n",
      "validation loss = 7.351067116386012\n",
      "Batch 126 / 157\n",
      "training loss = 7.278660297393799\n",
      "validation loss = 7.34207326487491\n",
      "Batch 127 / 157\n",
      "training loss = 7.404046058654785\n",
      "validation loss = 7.324076727816933\n",
      "Batch 128 / 157\n",
      "training loss = 7.546067714691162\n",
      "validation loss = 7.323795770343981\n",
      "Batch 129 / 157\n",
      "training loss = 7.357326984405518\n",
      "validation loss = 7.3380582709061475\n",
      "Batch 130 / 157\n",
      "training loss = 7.512877464294434\n",
      "validation loss = 7.316135306107371\n",
      "Batch 131 / 157\n",
      "training loss = 7.331120014190674\n",
      "validation loss = 7.301600933074951\n",
      "Batch 132 / 157\n",
      "training loss = 7.066071033477783\n",
      "validation loss = 7.3150146133021305\n",
      "Batch 133 / 157\n",
      "training loss = 7.247414588928223\n",
      "validation loss = 7.3153862953186035\n",
      "Batch 134 / 157\n",
      "training loss = 7.412465572357178\n",
      "validation loss = 7.3015956376728255\n",
      "Batch 135 / 157\n",
      "training loss = 6.991894721984863\n",
      "validation loss = 7.293732768610904\n",
      "Batch 136 / 157\n",
      "training loss = 7.267050266265869\n",
      "validation loss = 7.29806651567158\n",
      "Batch 137 / 157\n",
      "training loss = 7.345530986785889\n",
      "validation loss = 7.295755135385614\n",
      "Batch 138 / 157\n",
      "training loss = 7.523913860321045\n",
      "validation loss = 7.287135174399928\n",
      "Batch 139 / 157\n",
      "training loss = 7.285649299621582\n",
      "validation loss = 7.284394088544343\n",
      "Batch 140 / 157\n",
      "training loss = 7.333974838256836\n",
      "validation loss = 7.291722247475072\n",
      "Batch 141 / 157\n",
      "training loss = 7.261263370513916\n",
      "validation loss = 7.287034436276085\n",
      "Batch 142 / 157\n",
      "training loss = 7.684691905975342\n",
      "validation loss = 7.284510913648103\n",
      "Batch 143 / 157\n",
      "training loss = 7.359259605407715\n",
      "validation loss = 7.286172440177516\n",
      "Batch 144 / 157\n",
      "training loss = 7.4392781257629395\n",
      "validation loss = 7.297336854432759\n",
      "Batch 145 / 157\n",
      "training loss = 7.473576545715332\n",
      "validation loss = 7.274259115520277\n",
      "Batch 146 / 157\n",
      "training loss = 7.240851402282715\n",
      "validation loss = 7.262860699703819\n",
      "Batch 147 / 157\n",
      "training loss = 7.274552345275879\n",
      "validation loss = 7.269952096437153\n",
      "Batch 148 / 157\n",
      "training loss = 7.238165378570557\n",
      "validation loss = 7.283401966094971\n",
      "Batch 149 / 157\n",
      "training loss = 7.3003363609313965\n",
      "validation loss = 7.2738901439465975\n",
      "Batch 150 / 157\n",
      "training loss = 7.420464515686035\n",
      "validation loss = 7.262229819046824\n",
      "Batch 151 / 157\n",
      "training loss = 7.526851654052734\n",
      "validation loss = 7.272432678624203\n",
      "Batch 152 / 157\n",
      "training loss = 7.265717506408691\n",
      "validation loss = 7.269669281808953\n",
      "Batch 153 / 157\n",
      "training loss = 6.990407943725586\n",
      "validation loss = 7.2602407304864185\n",
      "Batch 154 / 157\n",
      "training loss = 7.209600925445557\n",
      "validation loss = 7.251475911391409\n",
      "Batch 155 / 157\n",
      "training loss = 7.474021911621094\n",
      "validation loss = 7.255219810887387\n",
      "Batch 156 / 157\n",
      "training loss = 7.196105480194092\n",
      "validation loss = 7.243968160528886\n",
      "Batch 157 / 157\n",
      "training loss = 7.350836277008057\n",
      "validation loss = 7.2402136451319645\n",
      "Average training loss: 7.60\n",
      "Start search... \n",
      "Batch 1 / 157\n",
      "training loss = 10.383506774902344\n",
      "validation loss = 10.141692061173288\n",
      "Batch 2 / 157\n",
      "training loss = 10.131142616271973\n",
      "validation loss = 9.584952454817923\n",
      "Batch 3 / 157\n",
      "training loss = 9.617485046386719\n",
      "validation loss = 9.175028098256965\n",
      "Batch 4 / 157\n",
      "training loss = 9.212335586547852\n",
      "validation loss = 8.839645536322342\n",
      "Batch 5 / 157\n",
      "training loss = 8.814444541931152\n",
      "validation loss = 8.559443774976229\n",
      "Batch 6 / 157\n",
      "training loss = 8.55466365814209\n",
      "validation loss = 8.31426013143439\n",
      "Batch 7 / 157\n",
      "training loss = 8.297050476074219\n",
      "validation loss = 8.135722637176514\n",
      "Batch 8 / 157\n",
      "training loss = 8.098626136779785\n",
      "validation loss = 8.00052211159154\n",
      "Batch 9 / 157\n",
      "training loss = 7.954770565032959\n",
      "validation loss = 7.899771138241417\n",
      "Batch 10 / 157\n",
      "training loss = 7.887174606323242\n",
      "validation loss = 7.841977520992882\n",
      "Batch 11 / 157\n",
      "training loss = 8.11386775970459\n",
      "validation loss = 7.7987465607492545\n",
      "Batch 12 / 157\n",
      "training loss = 7.8319172859191895\n",
      "validation loss = 7.782037333438271\n",
      "Batch 13 / 157\n",
      "training loss = 7.8885345458984375\n",
      "validation loss = 7.768251695131001\n",
      "Batch 14 / 157\n",
      "training loss = 7.611762523651123\n",
      "validation loss = 7.751657184801604\n",
      "Batch 15 / 157\n",
      "training loss = 8.21928882598877\n",
      "validation loss = 7.756216626418264\n",
      "Batch 16 / 157\n",
      "training loss = 8.007707595825195\n",
      "validation loss = 7.808213535108064\n",
      "Batch 17 / 157\n",
      "training loss = 7.787145614624023\n",
      "validation loss = 7.7197971845927995\n",
      "Batch 18 / 157\n",
      "training loss = 7.647700309753418\n",
      "validation loss = 7.748737535978618\n",
      "Batch 19 / 157\n",
      "training loss = 7.73428201675415\n",
      "validation loss = 7.763832393445466\n",
      "Batch 20 / 157\n",
      "training loss = 7.8535566329956055\n",
      "validation loss = 7.7202472937734505\n",
      "Batch 21 / 157\n",
      "training loss = 7.738563060760498\n",
      "validation loss = 7.6898761548494035\n",
      "Batch 22 / 157\n",
      "training loss = 7.817669868469238\n",
      "validation loss = 7.676841735839844\n",
      "Batch 23 / 157\n",
      "training loss = 7.62168550491333\n",
      "validation loss = 7.68473858582346\n",
      "Batch 24 / 157\n",
      "training loss = 7.6264214515686035\n",
      "validation loss = 7.6759623226366545\n",
      "Batch 25 / 157\n",
      "training loss = 7.73873233795166\n",
      "validation loss = 7.660411433169716\n",
      "Batch 26 / 157\n",
      "training loss = 7.716162204742432\n",
      "validation loss = 7.6410907695167944\n",
      "Batch 27 / 157\n",
      "training loss = 7.529271125793457\n",
      "validation loss = 7.635169681749846\n",
      "Batch 28 / 157\n",
      "training loss = 7.647799491882324\n",
      "validation loss = 7.632114862140856\n",
      "Batch 29 / 157\n",
      "training loss = 7.654868125915527\n",
      "validation loss = 7.615012118690892\n",
      "Batch 30 / 157\n",
      "training loss = 7.7446746826171875\n",
      "validation loss = 7.6006666986565845\n",
      "Batch 31 / 157\n",
      "training loss = 7.385627269744873\n",
      "validation loss = 7.614184279190867\n",
      "Batch 32 / 157\n",
      "training loss = 7.570051193237305\n",
      "validation loss = 7.577950427406712\n",
      "Batch 33 / 157\n",
      "training loss = 7.401129245758057\n",
      "validation loss = 7.576276252144261\n",
      "Batch 34 / 157\n",
      "training loss = 7.573704719543457\n",
      "validation loss = 7.573615475704796\n",
      "Batch 35 / 157\n",
      "training loss = 7.4373016357421875\n",
      "validation loss = 7.5714287757873535\n",
      "Batch 36 / 157\n",
      "training loss = 7.612880229949951\n",
      "validation loss = 7.548677318974545\n",
      "Batch 37 / 157\n",
      "training loss = 7.216500282287598\n",
      "validation loss = 7.544854164123535\n",
      "Batch 38 / 157\n",
      "training loss = 7.595409393310547\n",
      "validation loss = 7.533928293930857\n",
      "Batch 39 / 157\n",
      "training loss = 7.639606475830078\n",
      "validation loss = 7.540233913220857\n",
      "Batch 40 / 157\n",
      "training loss = 7.630771160125732\n",
      "validation loss = 7.537565858740556\n",
      "Batch 41 / 157\n",
      "training loss = 7.5695037841796875\n",
      "validation loss = 7.508195023787649\n",
      "Batch 42 / 157\n",
      "training loss = 7.5200934410095215\n",
      "validation loss = 7.518360966130307\n",
      "Batch 43 / 157\n",
      "training loss = 7.814428329467773\n",
      "validation loss = 7.511612917247572\n",
      "Batch 44 / 157\n",
      "training loss = 7.494421482086182\n",
      "validation loss = 7.505027168675473\n",
      "Batch 45 / 157\n",
      "training loss = 7.273376941680908\n",
      "validation loss = 7.500111052864476\n",
      "Batch 46 / 157\n",
      "training loss = 7.411626815795898\n",
      "validation loss = 7.500070019772179\n",
      "Batch 47 / 157\n",
      "training loss = 7.413779258728027\n",
      "validation loss = 7.493279256318745\n",
      "Batch 48 / 157\n",
      "training loss = 7.482130527496338\n",
      "validation loss = 7.488198556398091\n",
      "Batch 49 / 157\n",
      "training loss = 7.582192420959473\n",
      "validation loss = 7.490130324112742\n",
      "Batch 50 / 157\n",
      "training loss = 7.492986679077148\n",
      "validation loss = 7.474038876985249\n",
      "Batch 51 / 157\n",
      "training loss = 7.3247151374816895\n",
      "validation loss = 7.476283123618678\n",
      "Batch 52 / 157\n",
      "training loss = 7.18148946762085\n",
      "validation loss = 7.477735268442254\n",
      "Batch 53 / 157\n",
      "training loss = 7.329630374908447\n",
      "validation loss = 7.469973212794254\n",
      "Batch 54 / 157\n",
      "training loss = 7.334803104400635\n",
      "validation loss = 7.4625021031028345\n",
      "Batch 55 / 157\n",
      "training loss = 7.546328544616699\n",
      "validation loss = 7.464058022750051\n",
      "Batch 56 / 157\n",
      "training loss = 7.405106544494629\n",
      "validation loss = 7.4496760619314095\n",
      "Batch 57 / 157\n",
      "training loss = 7.416903495788574\n",
      "validation loss = 7.445710884897332\n",
      "Batch 58 / 157\n",
      "training loss = 7.495342254638672\n",
      "validation loss = 7.4612781123111125\n",
      "Batch 59 / 157\n",
      "training loss = 7.379709720611572\n",
      "validation loss = 7.459957750220048\n",
      "Batch 60 / 157\n",
      "training loss = 7.469944477081299\n",
      "validation loss = 7.440992857280531\n",
      "Batch 61 / 157\n",
      "training loss = 7.446915626525879\n",
      "validation loss = 7.444662646243446\n",
      "Batch 62 / 157\n",
      "training loss = 7.19396448135376\n",
      "validation loss = 7.436223858281186\n",
      "Batch 63 / 157\n",
      "training loss = 7.254946231842041\n",
      "validation loss = 7.441896062148245\n",
      "Batch 64 / 157\n",
      "training loss = 7.726682186126709\n",
      "validation loss = 7.45406258733649\n",
      "Batch 65 / 157\n",
      "training loss = 7.2336835861206055\n",
      "validation loss = 7.44966301165129\n",
      "Batch 66 / 157\n",
      "training loss = 7.903538227081299\n",
      "validation loss = 7.4422667653937085\n",
      "Batch 67 / 157\n",
      "training loss = 7.384997367858887\n",
      "validation loss = 7.430329649071944\n",
      "Batch 68 / 157\n",
      "training loss = 7.383200645446777\n",
      "validation loss = 7.427858804401598\n",
      "Batch 69 / 157\n",
      "training loss = 7.313968181610107\n",
      "validation loss = 7.435722100107293\n",
      "Batch 70 / 157\n",
      "training loss = 7.498891830444336\n",
      "validation loss = 7.409957961032265\n",
      "Batch 71 / 157\n",
      "training loss = 7.560276508331299\n",
      "validation loss = 7.416657899555407\n",
      "Batch 72 / 157\n",
      "training loss = 7.1775593757629395\n",
      "validation loss = 7.41866724114669\n",
      "Batch 73 / 157\n",
      "training loss = 7.512318134307861\n",
      "validation loss = 7.399913662358334\n",
      "Batch 74 / 157\n",
      "training loss = 7.284894943237305\n",
      "validation loss = 7.391363921918367\n",
      "Batch 75 / 157\n",
      "training loss = 7.1510186195373535\n",
      "validation loss = 7.389926333176462\n",
      "Batch 76 / 157\n",
      "training loss = 7.480544090270996\n",
      "validation loss = 7.388666102760716\n",
      "Batch 77 / 157\n",
      "training loss = 7.077019214630127\n",
      "validation loss = 7.384524822235107\n",
      "Batch 78 / 157\n",
      "training loss = 7.297085762023926\n",
      "validation loss = 7.372484257346706\n",
      "Batch 79 / 157\n",
      "training loss = 7.30271053314209\n",
      "validation loss = 7.378301394613166\n",
      "Batch 80 / 157\n",
      "training loss = 7.367664813995361\n",
      "validation loss = 7.375471516659386\n",
      "Batch 81 / 157\n",
      "training loss = 7.254084587097168\n",
      "validation loss = 7.367066032008121\n",
      "Batch 82 / 157\n",
      "training loss = 7.403388500213623\n",
      "validation loss = 7.369392294632761\n",
      "Batch 83 / 157\n",
      "training loss = 7.56105899810791\n",
      "validation loss = 7.365618479879279\n",
      "Batch 84 / 157\n",
      "training loss = 7.408689975738525\n",
      "validation loss = 7.365266799926758\n",
      "Batch 85 / 157\n",
      "training loss = 7.329551696777344\n",
      "validation loss = 7.353195591976768\n",
      "Batch 86 / 157\n",
      "training loss = 7.359017848968506\n",
      "validation loss = 7.363557012457597\n",
      "Batch 87 / 157\n",
      "training loss = 7.2228922843933105\n",
      "validation loss = 7.350395654377184\n",
      "Batch 88 / 157\n",
      "training loss = 7.386362075805664\n",
      "validation loss = 7.344371946234452\n",
      "Batch 89 / 157\n",
      "training loss = 7.626345634460449\n",
      "validation loss = 7.354459386122854\n",
      "Batch 90 / 157\n",
      "training loss = 7.544728755950928\n",
      "validation loss = 7.35238820628116\n",
      "Batch 91 / 157\n",
      "training loss = 7.329486846923828\n",
      "validation loss = 7.331113037310149\n",
      "Batch 92 / 157\n",
      "training loss = 7.263722896575928\n",
      "validation loss = 7.34961095609163\n",
      "Batch 93 / 157\n",
      "training loss = 7.3359694480896\n",
      "validation loss = 7.342006432382684\n",
      "Batch 94 / 157\n",
      "training loss = 7.275506973266602\n",
      "validation loss = 7.334878093317935\n",
      "Batch 95 / 157\n",
      "training loss = 7.258825302124023\n",
      "validation loss = 7.3271814898440715\n",
      "Batch 96 / 157\n",
      "training loss = 7.443475246429443\n",
      "validation loss = 7.335009850953755\n",
      "Batch 97 / 157\n",
      "training loss = 7.254120826721191\n",
      "validation loss = 7.328828811645508\n",
      "Batch 98 / 157\n",
      "training loss = 7.016821384429932\n",
      "validation loss = 7.322999301709626\n",
      "Batch 99 / 157\n",
      "training loss = 7.284297943115234\n",
      "validation loss = 7.319858199671695\n",
      "Batch 100 / 157\n",
      "training loss = 7.348923206329346\n",
      "validation loss = 7.322201477853875\n",
      "Batch 101 / 157\n",
      "training loss = 7.644454002380371\n",
      "validation loss = 7.316162937565854\n",
      "Batch 102 / 157\n",
      "training loss = 7.371569633483887\n",
      "validation loss = 7.315748440591912\n",
      "Batch 103 / 157\n",
      "training loss = 7.311949253082275\n",
      "validation loss = 7.31129789352417\n",
      "Batch 104 / 157\n",
      "training loss = 7.157404899597168\n",
      "validation loss = 7.299718430167751\n",
      "Batch 105 / 157\n",
      "training loss = 7.482328414916992\n",
      "validation loss = 7.2965444263659025\n",
      "Batch 106 / 157\n",
      "training loss = 7.272854804992676\n",
      "validation loss = 7.298482317673533\n",
      "Batch 107 / 157\n",
      "training loss = 7.364833831787109\n",
      "validation loss = 7.294887291757684\n",
      "Batch 108 / 157\n",
      "training loss = 7.2764201164245605\n",
      "validation loss = 7.2928569693314405\n",
      "Batch 109 / 157\n",
      "training loss = 7.224867820739746\n",
      "validation loss = 7.2968496021471525\n",
      "Batch 110 / 157\n",
      "training loss = 7.035438537597656\n",
      "validation loss = 7.2861782877068775\n",
      "Batch 111 / 157\n",
      "training loss = 7.397343158721924\n",
      "validation loss = 7.293253045333059\n",
      "Batch 112 / 157\n",
      "training loss = 7.494838237762451\n",
      "validation loss = 7.295891084169087\n",
      "Batch 113 / 157\n",
      "training loss = 7.257066249847412\n",
      "validation loss = 7.278388826470626\n",
      "Batch 114 / 157\n",
      "training loss = 7.176809787750244\n",
      "validation loss = 7.281515096363268\n",
      "Batch 115 / 157\n",
      "training loss = 7.2783331871032715\n",
      "validation loss = 7.277076570611251\n",
      "Batch 116 / 157\n",
      "training loss = 7.2866740226745605\n",
      "validation loss = 7.271759936684056\n",
      "Batch 117 / 157\n",
      "training loss = 6.939080715179443\n",
      "validation loss = 7.268297898141961\n",
      "Batch 118 / 157\n",
      "training loss = 7.1215057373046875\n",
      "validation loss = 7.268853262851112\n",
      "Batch 119 / 157\n",
      "training loss = 7.475456714630127\n",
      "validation loss = 7.273275601236444\n",
      "Batch 120 / 157\n",
      "training loss = 7.219159126281738\n",
      "validation loss = 7.274156444951108\n",
      "Batch 121 / 157\n",
      "training loss = 7.228579521179199\n",
      "validation loss = 7.2736816908183854\n",
      "Batch 122 / 157\n",
      "training loss = 7.327669143676758\n",
      "validation loss = 7.2727935188695\n",
      "Batch 123 / 157\n",
      "training loss = 7.4642415046691895\n",
      "validation loss = 7.265001071126838\n",
      "Batch 124 / 157\n",
      "training loss = 7.049577713012695\n",
      "validation loss = 7.2562459142584546\n",
      "Batch 125 / 157\n",
      "training loss = 7.346181869506836\n",
      "validation loss = 7.244571183857165\n",
      "Batch 126 / 157\n",
      "training loss = 7.1873393058776855\n",
      "validation loss = 7.2426964107312655\n",
      "Batch 127 / 157\n",
      "training loss = 7.137640953063965\n",
      "validation loss = 7.248278417085347\n",
      "Batch 128 / 157\n",
      "training loss = 7.140897274017334\n",
      "validation loss = 7.236884920220626\n",
      "Batch 129 / 157\n",
      "training loss = 7.117887020111084\n",
      "validation loss = 7.232648297360069\n",
      "Batch 130 / 157\n",
      "training loss = 7.4804558753967285\n",
      "validation loss = 7.2321659640262\n",
      "Batch 131 / 157\n",
      "training loss = 7.198000907897949\n",
      "validation loss = 7.233141748528731\n",
      "Batch 132 / 157\n",
      "training loss = 7.1695709228515625\n",
      "validation loss = 7.236509900344045\n",
      "Batch 133 / 157\n",
      "training loss = 7.287239074707031\n",
      "validation loss = 7.234133670204564\n",
      "Batch 134 / 157\n",
      "training loss = 7.273523330688477\n",
      "validation loss = 7.2278952849538705\n",
      "Batch 135 / 157\n",
      "training loss = 7.32219123840332\n",
      "validation loss = 7.234358185216\n",
      "Batch 136 / 157\n",
      "training loss = 7.18034029006958\n",
      "validation loss = 7.233722134640343\n",
      "Batch 137 / 157\n",
      "training loss = 7.177577495574951\n",
      "validation loss = 7.2231827535127335\n",
      "Batch 138 / 157\n",
      "training loss = 7.098609924316406\n",
      "validation loss = 7.22511090730366\n",
      "Batch 139 / 157\n",
      "training loss = 6.9954752922058105\n",
      "validation loss = 7.233465721732692\n",
      "Batch 140 / 157\n",
      "training loss = 6.875209331512451\n",
      "validation loss = 7.231811397954037\n",
      "Batch 141 / 157\n",
      "training loss = 7.166367530822754\n",
      "validation loss = 7.223888748570492\n",
      "Batch 142 / 157\n",
      "training loss = 7.299960613250732\n",
      "validation loss = 7.222491113763106\n",
      "Batch 143 / 157\n",
      "training loss = 7.017282009124756\n",
      "validation loss = 7.223138733914024\n",
      "Batch 144 / 157\n",
      "training loss = 7.459357261657715\n",
      "validation loss = 7.225912821920295\n",
      "Batch 145 / 157\n",
      "training loss = 6.898313999176025\n",
      "validation loss = 7.2196297896535775\n",
      "Batch 146 / 157\n",
      "training loss = 7.419376373291016\n",
      "validation loss = 7.213385155326442\n",
      "Batch 147 / 157\n",
      "training loss = 7.187028408050537\n",
      "validation loss = 7.21173085664448\n",
      "Batch 148 / 157\n",
      "training loss = 7.097123622894287\n",
      "validation loss = 7.2150725314491675\n",
      "Batch 149 / 157\n",
      "training loss = 7.361513137817383\n",
      "validation loss = 7.217278957366943\n",
      "Batch 150 / 157\n",
      "training loss = 7.1759934425354\n",
      "validation loss = 7.213135393042314\n",
      "Batch 151 / 157\n",
      "training loss = 7.495594501495361\n",
      "validation loss = 7.209290128005178\n",
      "Batch 152 / 157\n",
      "training loss = 7.247265815734863\n",
      "validation loss = 7.208028969011809\n",
      "Batch 153 / 157\n",
      "training loss = 7.02119255065918\n",
      "validation loss = 7.2094574978477075\n",
      "Batch 154 / 157\n",
      "training loss = 7.027381420135498\n",
      "validation loss = 7.208791004984002\n",
      "Batch 155 / 157\n",
      "training loss = 7.1096038818359375\n",
      "validation loss = 7.199479830892463\n",
      "Batch 156 / 157\n",
      "training loss = 7.259188652038574\n",
      "validation loss = 7.198153721658807\n",
      "Batch 157 / 157\n",
      "training loss = 7.328321933746338\n",
      "validation loss = 7.207244898143568\n",
      "Average training loss: 7.49\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"dim\": [128, 512],\n",
    "    \"n_layers\": [2, 8],\n",
    "    \"n_heads\": [2, 4, 8],\n",
    "    \"vocab_size\": [tokenizer.n_words],\n",
    "    \"multiple_of\": [128],\n",
    "    \"norm_eps\": [1e-5],\n",
    "    \"max_batch_size\": [MAX_BSZ],\n",
    "    \"max_seq_len\": [32, 128, 512],\n",
    "}\n",
    "\n",
    "history = []\n",
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)\n",
    "# Initialize smaller dataset\n",
    "train_texts, val_texts, test_texts = train_test_split(read_file(DATA_PATH, entries=3000), 2500, 300, 200)\n",
    "# GridSearch loop \n",
    "for dim in configs[\"dim\"]: \n",
    "    for n_layers in configs[\"n_layers\"]: \n",
    "        for n_heads in configs[\"n_heads\"]: \n",
    "            for max_seq_len in configs[\"max_seq_len\"]: \n",
    "                for vocab_size in configs[\"vocab_size\"]:\n",
    "                    for multiple_of in configs[\"multiple_of\"]: \n",
    "                        for norm_eps in configs[\"norm_eps\"]:\n",
    "                            print(\"Start search... \")\n",
    "                            # Initialize dataset\n",
    "                            train_dataset = TextDataset(train_texts, tokenizer, max_seq_len)\n",
    "                            val_dataset = TextDataset(val_texts, tokenizer, max_seq_len)\n",
    "                            # Transform to dataloader\n",
    "                            train_dataloader = DataLoader(train_dataset, batch_size=MAX_BSZ, shuffle=True, collate_fn=collate_fn)\n",
    "                            val_dataloader = DataLoader(val_dataset, batch_size=MAX_BSZ, shuffle=True, collate_fn=collate_fn)\n",
    "                            model_args = ModelArgs(\n",
    "                                dim=dim,\n",
    "                                n_layers=n_layers,\n",
    "                                n_heads=n_heads,\n",
    "                                vocab_size=vocab_size,\n",
    "                                multiple_of=multiple_of,\n",
    "                                norm_eps=norm_eps,\n",
    "                                max_batch_size=MAX_BSZ,\n",
    "                                max_seq_len=max_seq_len,\n",
    "                            )\n",
    "                            model = Transformer(model_args)\n",
    "                            optimizer = torch.optim.AdamW(model.parameters())\n",
    "                            loss_func = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "                            train_history, val_history = train(model, optimizer, loss_func, train_dataloader, val_dataloader)\n",
    "                            history.append({\n",
    "                                \"config\": {\n",
    "                                    \"dim\": dim,\n",
    "                                    \"n_layers\": n_layers,\n",
    "                                    \"n_heads\": n_heads,\n",
    "                                    \"vocab_size\": vocab_size,\n",
    "                                    \"multiple_of\": multiple_of,\n",
    "                                    \"norm_eps\": norm_eps,\n",
    "                                    \"max_batch_size\": MAX_BSZ,\n",
    "                                    \"max_seq_len\": max_seq_len,\n",
    "                                }, \n",
    "                                \"train\": train_history, \n",
    "                                \"val\": val_history\n",
    "                            })\n",
    "                            save_model_param(model, model_args)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def save_model_param(model, model_args):\n",
    "    direc = './ckpts/' + f\"model_dim_{model_args.dim}_n_layers_{model_args.n_layers}_n_heads_{model_args.n_heads}_max_seq_len_{model_args.max_seq_len}\"\n",
    "    if os.path.exists(direc):\n",
    "        shutil.rmtree(direc)\n",
    "    os.makedirs(direc)\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, direc + \"/model.pth\")\n",
    "\n",
    "    json_params = json.dumps({\n",
    "        \"dim\": model_args.dim, \n",
    "        \"n_layers\": model_args.n_layers, \n",
    "        \"n_heads\": model_args.n_heads, \n",
    "        \"vocab_size\": model_args.vocab_size, \n",
    "        \"multiple_of\": model_args.multiple_of, \n",
    "        \"norm_eps\": model_args.norm_eps, \n",
    "    }, indent=4)\n",
    "\n",
    "    with open(direc + \"/params.json\", \"w\") as outfile:\n",
    "        outfile.write(json_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'list'>\n",
      "Length: 27000\n",
      "First element type: <class 'str'>\n",
      "DATALOADER OVERVIEW\n",
      "Number of Batches: 1688\n",
      "Batch Size: 16\n",
      "\n",
      "First batch overview:\n",
      "Keys:  dict_keys(['input_ids', 'target_ids'])\n",
      "Shape of input_ids tensor: torch.Size([16, 511])\n",
      "Shape of target_ids tensor: torch.Size([16, 511])\n",
      "\n",
      "Dataset overview:\n",
      "Length of Dataset:  27000\n",
      "Example item:  {'input_ids': tensor([    1, 29301,   310,  6615,  1230, 18530,  4603,   472,   278,  4768,\n",
      "         5996,  1889,  3233, 29889,    13,  4617,  1070, 10174,   526,   451,\n",
      "        23968,  6471,   310,  4959, 29889, 25678, 29892,   297,  1556,  9200,\n",
      "         2378,  3483,   952,   267, 29892,   896, 10331,   304,   367, 14914,\n",
      "          408,  2317, 18785, 10340, 29889,  1763, 28453,  3578,   373,   920,\n",
      "         5164,  5633,   310,   278,  1006, 29113,  4768,  5996, 10174,   526,\n",
      "         6615,   630,   472,   278,  1301,  3395,  3233, 29892,   727,   338,\n",
      "          263,   817,   304,  6559,   278,  1546, 29899,  5441,  4603,   284,\n",
      "         9443,  4153, 29889,  1334,  2948,   445,  2228,   491,  3386,   292,\n",
      "          385,  2380,   310, 19869,   740,   304, 27769,   278,  5534,  4766,\n",
      "          310,  1302, 17471,  1546,  2531,   267,   515,   697,  1889,   322,\n",
      "         2531,   267,   515,   278,  4152,  2531,   608, 29889, 10554,   267,\n",
      "          411,  2788,  1804,  3698,   526,   769, 15659,   322,  2060,   287,\n",
      "          304,   263,  1889, 29899,   517, 29899,  5014, 15477,  3983, 29889,\n",
      "          910,  2246, 29899,  3204,  1158,  6511,   363, 13173, 18530, 29899,\n",
      "         5563,  7418,  1546,  9024, 10174,   304,  1101,   701, 29889,  5293,\n",
      "          278,  3038, 29899, 23090, 18530, 29899, 17471, 28723,   363, 15573,\n",
      "         3090, 16103,   778,   274,   406,  1730, 23395, 29892,   591,  3461,\n",
      "         1532, 29899,  6388,  1891, 14379,   310,  4768,  5996, 10174,   393,\n",
      "          723,   367,  5189,   304,  1284,  6467, 29889,  5293,  1790,  8783,\n",
      "        29892,   591,  3461,   263, 15301,   368,  1422,  3564,  3829, 23425,\n",
      "         3038,  1070, 20890,  1090, 29380, 22884, 29889,  1732,   597, 19501,\n",
      "          571, 29889,  6112, 29889,  1682,   433, 29889,  6085, 29914,  6984,\n",
      "        29906, 29914, 10382, 29914, 29968, 29931, 29918, 19303,   944, 29889,\n",
      "         5140]), 'target_ids': tensor([  310,  6615,  1230, 18530,  4603,   472,   278,  4768,  5996,  1889,\n",
      "         3233, 29889,    13,  4617,  1070, 10174,   526,   451, 23968,  6471,\n",
      "          310,  4959, 29889, 25678, 29892,   297,  1556,  9200,  2378,  3483,\n",
      "          952,   267, 29892,   896, 10331,   304,   367, 14914,   408,  2317,\n",
      "        18785, 10340, 29889,  1763, 28453,  3578,   373,   920,  5164,  5633,\n",
      "          310,   278,  1006, 29113,  4768,  5996, 10174,   526,  6615,   630,\n",
      "          472,   278,  1301,  3395,  3233, 29892,   727,   338,   263,   817,\n",
      "          304,  6559,   278,  1546, 29899,  5441,  4603,   284,  9443,  4153,\n",
      "        29889,  1334,  2948,   445,  2228,   491,  3386,   292,   385,  2380,\n",
      "          310, 19869,   740,   304, 27769,   278,  5534,  4766,   310,  1302,\n",
      "        17471,  1546,  2531,   267,   515,   697,  1889,   322,  2531,   267,\n",
      "          515,   278,  4152,  2531,   608, 29889, 10554,   267,   411,  2788,\n",
      "         1804,  3698,   526,   769, 15659,   322,  2060,   287,   304,   263,\n",
      "         1889, 29899,   517, 29899,  5014, 15477,  3983, 29889,   910,  2246,\n",
      "        29899,  3204,  1158,  6511,   363, 13173, 18530, 29899,  5563,  7418,\n",
      "         1546,  9024, 10174,   304,  1101,   701, 29889,  5293,   278,  3038,\n",
      "        29899, 23090, 18530, 29899, 17471, 28723,   363, 15573,  3090, 16103,\n",
      "          778,   274,   406,  1730, 23395, 29892,   591,  3461,  1532, 29899,\n",
      "         6388,  1891, 14379,   310,  4768,  5996, 10174,   393,   723,   367,\n",
      "         5189,   304,  1284,  6467, 29889,  5293,  1790,  8783, 29892,   591,\n",
      "         3461,   263, 15301,   368,  1422,  3564,  3829, 23425,  3038,  1070,\n",
      "        20890,  1090, 29380, 22884, 29889,  1732,   597, 19501,   571, 29889,\n",
      "         6112, 29889,  1682,   433, 29889,  6085, 29914,  6984, 29906, 29914,\n",
      "        10382, 29914, 29968, 29931, 29918, 19303,   944, 29889,  5140, 29889,\n",
      "            2])}\n"
     ]
    }
   ],
   "source": [
    "def examine_data(data):\n",
    "    '''debugging func'''\n",
    "    print('Input type: {}'.format(type(data)))\n",
    "    print('Length: {}'.format(len(data)))\n",
    "    print('First element type: {}'.format(type(data[0])))\n",
    "    if type(data[0])==dict:\n",
    "        print('Keys: {}'.format(data[0].keys()))\n",
    "    return\n",
    "\n",
    "#debugging tools\n",
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print('TENSOR OVERVIEW\\n'\n",
    "          'Type: {}\\n'\n",
    "          'Data Type: {}\\n'\n",
    "          'Shape: {}\\n'\n",
    "          'Number of Dimensions: {}\\n'\n",
    "          'Device: {}\\n'\n",
    "          'Requires Grad: {}\\n'\n",
    "          'Gradient: {}\\n'.format(tensor.type(), tensor.dtype, tensor.shape, tensor.ndim, tensor.device,\\\n",
    "                                  tensor.requires_grad, tensor.grad))\n",
    "    return\n",
    "\n",
    "def flag(msg='unspecified'):\n",
    "    print('FLAG: {}'.format(msg))\n",
    "    return\n",
    "\n",
    "def loop_summary(titles:tuple, tensors:tuple):\n",
    "    for i in range(len(titles)):\n",
    "        flag(titles[i])\n",
    "        examine_tensor(tensors[i])\n",
    "    return\n",
    "\n",
    "def examine_dataloader(dataloader):\n",
    "    '''debugging function'''\n",
    "    print('DATALOADER OVERVIEW\\n'\n",
    "          'Number of Batches: {}\\n'\n",
    "          'Batch Size: {}\\n'.format(len(dataloader), dataloader.batch_size))\n",
    "\n",
    "    # Examine the first batch in the dataloader\n",
    "    first_batch = next(iter(dataloader))\n",
    "    print('First batch overview:')\n",
    "    print('Keys: ', first_batch.keys())\n",
    "    \n",
    "    for key in first_batch.keys():\n",
    "        print('Shape of {} tensor: {}'.format(key, first_batch[key].shape))\n",
    "    \n",
    "    # Examine the dataset\n",
    "    print('\\nDataset overview:')\n",
    "    print('Length of Dataset: ', len(dataloader.dataset))\n",
    "\n",
    "    # Try getting an item from the dataset\n",
    "    try:\n",
    "        print('Example item: ', dataloader.dataset[0])\n",
    "    except Exception as e:\n",
    "        print('Could not retrieve item from dataset: ', str(e))\n",
    "    return\n",
    "\n",
    "examine_data(train_texts)\n",
    "examine_dataloader(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
