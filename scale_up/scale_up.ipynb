{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/'\n",
    "# TEST_PATH = '/home/patrick/Documents/CSE_599/HW/2/testset/test.jsonl'\n",
    "# VAL_PATH = '/home/patrick/Documents/CSE_599/HW/2/valset/val.jsonl'\n",
    "MODEL_PATH = '/Users/anderson/Desktop/Project/LLaMA-From-Inference-to-Training/' #folder with generation.py, model.py, and tokenizer.py\n",
    "TRAINED_SPM_PATH = './tokenizer.model' #downloaded from Ed post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Make sure INGESTED_SAMPLE_CNT % MAX_BSZ == 0; assymetric batches break something somewhere\n",
    "# MAX_SEQ_LEN = 2048\n",
    "MAX_SEQ_LEN = 512\n",
    "INGESTED_SAMPLE_CNT = 8\n",
    "MAX_BSZ = 16\n",
    "MINI_MODEL = False\n",
    "RESUME = False \n",
    "PRETRAINED_PATH = \"./ckpts/best_ckpt/model.pth\"\n",
    "PARAMS_JSON_PATH = \"./ckpts/best_ckpt/params.json\"\n",
    "DATA_PATH = \"/Users/anderson/Downloads/11.jsonl.zst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(MODEL_PATH)\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from generation import LLaMA\n",
    "from llama.model_train import ModelArgs, Transformer #ctrl+f and comment out cuda, all else same\n",
    "#from model import ModelArgs, Transformer #use this one if you have NVIDIA GPU\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextDataset.__init__() missing 1 required positional argument: 'max_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m# train_texts, val_texts, test_texts = train_test_split(\"../data/02_10000_entries.txt\", 25000, 100, 2000)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m train_texts, val_texts, test_texts \u001b[39m=\u001b[39m train_test_split(read_file(DATA_PATH, entries\u001b[39m=\u001b[39m\u001b[39m30000\u001b[39m), \u001b[39m27000\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m train_dataset \u001b[39m=\u001b[39m TextDataset(train_texts, tokenizer)\n\u001b[1;32m     99\u001b[0m val_dataset \u001b[39m=\u001b[39m TextDataset(val_texts, tokenizer)\n\u001b[1;32m    100\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mMAX_BSZ, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n",
      "\u001b[0;31mTypeError\u001b[0m: TextDataset.__init__() missing 1 required positional argument: 'max_seq_len'"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np \n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zstandard as zstd\n",
    "import io\n",
    "\n",
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)\n",
    "\n",
    "def read_line(line):\n",
    "    line = line.strip()\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return data['text']\n",
    "\n",
    "def read_file(file_path, gbs=1.0, entries=10000):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        decompressor = zstd.ZstdDecompressor()\n",
    "        stream_reader = decompressor.stream_reader(file)\n",
    "        stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "\n",
    "        lines = []\n",
    "        for line in stream:\n",
    "            line = read_line(line)\n",
    "            if line is not None:\n",
    "                lines.append(line)\n",
    "            if len(lines) == entries:\n",
    "                break\n",
    "    print(len(lines))\n",
    "    return lines\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_len, random_mask=False, n_mask=1):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_mask = random_mask\n",
    "        self.n_mask = n_mask\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoded_text = self.tokenizer.encode(text, bos=False, eos=False)\n",
    "        if not self.random_mask: \n",
    "            #Truncate the sequence to max_seq_len if it's too long\n",
    "            if len(encoded_text) > self.max_seq_len - 1:  #We subtract 2 to account for the BOS and EOS tokens\\\n",
    "                                                    #small modification: -1 instead\n",
    "                encoded_text = encoded_text[:self.max_seq_len - 1]           \n",
    "            return {\n",
    "                'input_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text[:-1], dtype=torch.long),\n",
    "                'target_ids': torch.tensor(encoded_text[1:] + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "            }\n",
    "        else: \n",
    "            if len(encoded_text) > self.max_seq_len - 2:\n",
    "                encoded_text = encoded_text[:self.max_seq_len - 2]\n",
    "            return {\n",
    "                'input_ids': torch.tensor([self.tokenizer.bos_id] + self.mask(encoded_text) + [self.tokenizer.eos_id], dtype=torch.long),\n",
    "                'target_ids': torch.tensor([self.tokenizer.bos_id] + encoded_text + [self.tokenizer.eos_id], dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    def mask(self, encoded_text): \n",
    "        idxs = np.random.choice(range(len(encoded_text)), size=self.n_mask)\n",
    "        encoded_text[idxs] = self.tokenizer.pad_id\n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'target_ids': target_ids\n",
    "    }\n",
    "\n",
    "def train_test_split(lines, train_n, valid_n, test_n): \n",
    "    train_texts = []\n",
    "    val_texts = []\n",
    "    test_texts = []\n",
    "    for idx, line in enumerate(lines): \n",
    "        if idx < train_n: \n",
    "            train_texts.append(line)\n",
    "        elif idx < train_n + valid_n: \n",
    "            val_texts.append(line)\n",
    "        elif idx < train_n + valid_n + test_n: \n",
    "            test_texts.append(line) \n",
    "        else: \n",
    "            break \n",
    "    return train_texts, val_texts, test_texts\n",
    "\n",
    "# train_texts, val_texts, test_texts = train_test_split(\"../data/02_10000_entries.txt\", 25000, 100, 2000)\n",
    "train_texts, val_texts, test_texts = train_test_split(read_file(DATA_PATH, entries=30000), 27000, 100, 200)\n",
    "\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=MAX_BSZ, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=MAX_BSZ, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RANK=0\n",
      "env: WORLD_SIZE=1\n",
      "env: MASTER_ADDR=localhost\n",
      "env: MASTER_PORT=0\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo')\n",
    "fs_init.initialize_model_parallel(1) #1 worker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "dict_keys(['model_state_dict', 'optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "mini_args = ModelArgs(\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=MAX_BSZ, #only works for 32; no idea why\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "if RESUME: \n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(PRETRAINED_PATH, map_location=\"cpu\")\n",
    "    print(checkpoint.keys())\n",
    "    with open(PARAMS_JSON_PATH, \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=MAX_SEQ_LEN, max_batch_size=MAX_BSZ, **params\n",
    "    )\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    model = Transformer(model_args)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "elif MINI_MODEL: #global var (2nd cell)\n",
    "    model = Transformer(mini_args)\n",
    "else:\n",
    "    model = Transformer(model_args)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  # ignores padding token for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, train_dataloader, val_dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        print(f\"Batch {i+1} / {len(train_dataloader)}\")\n",
    "        inputs = batch['input_ids'] #bsz x seq_len \n",
    "        targets = batch['target_ids'] #bsz x seq_len\n",
    "        # examine_tensor(targets)\n",
    "\n",
    "        outputs = model(inputs, start_pos=0) #bsz x sel_len x vocab_size\n",
    "\n",
    "        flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "\n",
    "        flat_targets = targets.view(-1) #(bsz*seq_len)\n",
    "\n",
    "        loss = loss_func(flat_outputs, flat_targets) #might be incorrect to flatten, idk\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        val_loss = 0\n",
    "\n",
    "        for val_batch in val_dataloader: \n",
    "            val_inputs = val_batch['input_ids']\n",
    "            val_targets = val_batch['target_ids']\n",
    "\n",
    "            outputs = model(val_inputs, start_pos=0)\n",
    "            flat_outputs = outputs.view(-1, outputs.size(-1)) #(bsz*seq_len) x vocab_size\n",
    "            flat_targets = val_targets.view(-1) #(bsz*seq_len)\n",
    "            val_loss += loss_func(flat_outputs, flat_targets).item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"training loss = {loss.item()}\")\n",
    "        print(f\"validation loss = {val_loss}\")\n",
    "        total_loss += loss.item()\n",
    "        train_history.append(loss.item())\n",
    "        val_history.append(val_loss)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    return train_history, val_history \n",
    "\n",
    "train_history, val_history = train(model, optimizer, loss_func, train_dataloader, val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def save_model_param(model, model_args):\n",
    "    direc = './ckpts/' + f\"model_dim_{model_args.dim}_n_layers_{model_args.n_layers}_n_heads_{model_args.n_heads}_max_seq_len_{model_args.max_seq_len}\"\n",
    "    if os.path.exists(direc):\n",
    "        shutil.rmtree(direc)\n",
    "    os.makedirs(direc)\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, direc + \"/model.pth\")\n",
    "\n",
    "    json_params = json.dumps({\n",
    "        \"dim\": model_args.dim, \n",
    "        \"n_layers\": model_args.n_layers, \n",
    "        \"n_heads\": model_args.n_heads, \n",
    "        \"vocab_size\": model_args.vocab_size, \n",
    "        \"multiple_of\": model_args.multiple_of, \n",
    "        \"norm_eps\": model_args.norm_eps, \n",
    "    }, indent=4)\n",
    "\n",
    "    with open(direc + \"/params.json\", \"w\") as outfile:\n",
    "        outfile.write(json_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'list'>\n",
      "Length: 27000\n",
      "First element type: <class 'str'>\n",
      "DATALOADER OVERVIEW\n",
      "Number of Batches: 1688\n",
      "Batch Size: 16\n",
      "\n",
      "First batch overview:\n",
      "Keys:  dict_keys(['input_ids', 'target_ids'])\n",
      "Shape of input_ids tensor: torch.Size([16, 511])\n",
      "Shape of target_ids tensor: torch.Size([16, 511])\n",
      "\n",
      "Dataset overview:\n",
      "Length of Dataset:  27000\n",
      "Example item:  {'input_ids': tensor([    1, 29301,   310,  6615,  1230, 18530,  4603,   472,   278,  4768,\n",
      "         5996,  1889,  3233, 29889,    13,  4617,  1070, 10174,   526,   451,\n",
      "        23968,  6471,   310,  4959, 29889, 25678, 29892,   297,  1556,  9200,\n",
      "         2378,  3483,   952,   267, 29892,   896, 10331,   304,   367, 14914,\n",
      "          408,  2317, 18785, 10340, 29889,  1763, 28453,  3578,   373,   920,\n",
      "         5164,  5633,   310,   278,  1006, 29113,  4768,  5996, 10174,   526,\n",
      "         6615,   630,   472,   278,  1301,  3395,  3233, 29892,   727,   338,\n",
      "          263,   817,   304,  6559,   278,  1546, 29899,  5441,  4603,   284,\n",
      "         9443,  4153, 29889,  1334,  2948,   445,  2228,   491,  3386,   292,\n",
      "          385,  2380,   310, 19869,   740,   304, 27769,   278,  5534,  4766,\n",
      "          310,  1302, 17471,  1546,  2531,   267,   515,   697,  1889,   322,\n",
      "         2531,   267,   515,   278,  4152,  2531,   608, 29889, 10554,   267,\n",
      "          411,  2788,  1804,  3698,   526,   769, 15659,   322,  2060,   287,\n",
      "          304,   263,  1889, 29899,   517, 29899,  5014, 15477,  3983, 29889,\n",
      "          910,  2246, 29899,  3204,  1158,  6511,   363, 13173, 18530, 29899,\n",
      "         5563,  7418,  1546,  9024, 10174,   304,  1101,   701, 29889,  5293,\n",
      "          278,  3038, 29899, 23090, 18530, 29899, 17471, 28723,   363, 15573,\n",
      "         3090, 16103,   778,   274,   406,  1730, 23395, 29892,   591,  3461,\n",
      "         1532, 29899,  6388,  1891, 14379,   310,  4768,  5996, 10174,   393,\n",
      "          723,   367,  5189,   304,  1284,  6467, 29889,  5293,  1790,  8783,\n",
      "        29892,   591,  3461,   263, 15301,   368,  1422,  3564,  3829, 23425,\n",
      "         3038,  1070, 20890,  1090, 29380, 22884, 29889,  1732,   597, 19501,\n",
      "          571, 29889,  6112, 29889,  1682,   433, 29889,  6085, 29914,  6984,\n",
      "        29906, 29914, 10382, 29914, 29968, 29931, 29918, 19303,   944, 29889,\n",
      "         5140]), 'target_ids': tensor([  310,  6615,  1230, 18530,  4603,   472,   278,  4768,  5996,  1889,\n",
      "         3233, 29889,    13,  4617,  1070, 10174,   526,   451, 23968,  6471,\n",
      "          310,  4959, 29889, 25678, 29892,   297,  1556,  9200,  2378,  3483,\n",
      "          952,   267, 29892,   896, 10331,   304,   367, 14914,   408,  2317,\n",
      "        18785, 10340, 29889,  1763, 28453,  3578,   373,   920,  5164,  5633,\n",
      "          310,   278,  1006, 29113,  4768,  5996, 10174,   526,  6615,   630,\n",
      "          472,   278,  1301,  3395,  3233, 29892,   727,   338,   263,   817,\n",
      "          304,  6559,   278,  1546, 29899,  5441,  4603,   284,  9443,  4153,\n",
      "        29889,  1334,  2948,   445,  2228,   491,  3386,   292,   385,  2380,\n",
      "          310, 19869,   740,   304, 27769,   278,  5534,  4766,   310,  1302,\n",
      "        17471,  1546,  2531,   267,   515,   697,  1889,   322,  2531,   267,\n",
      "          515,   278,  4152,  2531,   608, 29889, 10554,   267,   411,  2788,\n",
      "         1804,  3698,   526,   769, 15659,   322,  2060,   287,   304,   263,\n",
      "         1889, 29899,   517, 29899,  5014, 15477,  3983, 29889,   910,  2246,\n",
      "        29899,  3204,  1158,  6511,   363, 13173, 18530, 29899,  5563,  7418,\n",
      "         1546,  9024, 10174,   304,  1101,   701, 29889,  5293,   278,  3038,\n",
      "        29899, 23090, 18530, 29899, 17471, 28723,   363, 15573,  3090, 16103,\n",
      "          778,   274,   406,  1730, 23395, 29892,   591,  3461,  1532, 29899,\n",
      "         6388,  1891, 14379,   310,  4768,  5996, 10174,   393,   723,   367,\n",
      "         5189,   304,  1284,  6467, 29889,  5293,  1790,  8783, 29892,   591,\n",
      "         3461,   263, 15301,   368,  1422,  3564,  3829, 23425,  3038,  1070,\n",
      "        20890,  1090, 29380, 22884, 29889,  1732,   597, 19501,   571, 29889,\n",
      "         6112, 29889,  1682,   433, 29889,  6085, 29914,  6984, 29906, 29914,\n",
      "        10382, 29914, 29968, 29931, 29918, 19303,   944, 29889,  5140, 29889,\n",
      "            2])}\n"
     ]
    }
   ],
   "source": [
    "def examine_data(data):\n",
    "    '''debugging func'''\n",
    "    print('Input type: {}'.format(type(data)))\n",
    "    print('Length: {}'.format(len(data)))\n",
    "    print('First element type: {}'.format(type(data[0])))\n",
    "    if type(data[0])==dict:\n",
    "        print('Keys: {}'.format(data[0].keys()))\n",
    "    return\n",
    "\n",
    "#debugging tools\n",
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print('TENSOR OVERVIEW\\n'\n",
    "          'Type: {}\\n'\n",
    "          'Data Type: {}\\n'\n",
    "          'Shape: {}\\n'\n",
    "          'Number of Dimensions: {}\\n'\n",
    "          'Device: {}\\n'\n",
    "          'Requires Grad: {}\\n'\n",
    "          'Gradient: {}\\n'.format(tensor.type(), tensor.dtype, tensor.shape, tensor.ndim, tensor.device,\\\n",
    "                                  tensor.requires_grad, tensor.grad))\n",
    "    return\n",
    "\n",
    "def flag(msg='unspecified'):\n",
    "    print('FLAG: {}'.format(msg))\n",
    "    return\n",
    "\n",
    "def loop_summary(titles:tuple, tensors:tuple):\n",
    "    for i in range(len(titles)):\n",
    "        flag(titles[i])\n",
    "        examine_tensor(tensors[i])\n",
    "    return\n",
    "\n",
    "def examine_dataloader(dataloader):\n",
    "    '''debugging function'''\n",
    "    print('DATALOADER OVERVIEW\\n'\n",
    "          'Number of Batches: {}\\n'\n",
    "          'Batch Size: {}\\n'.format(len(dataloader), dataloader.batch_size))\n",
    "\n",
    "    # Examine the first batch in the dataloader\n",
    "    first_batch = next(iter(dataloader))\n",
    "    print('First batch overview:')\n",
    "    print('Keys: ', first_batch.keys())\n",
    "    \n",
    "    for key in first_batch.keys():\n",
    "        print('Shape of {} tensor: {}'.format(key, first_batch[key].shape))\n",
    "    \n",
    "    # Examine the dataset\n",
    "    print('\\nDataset overview:')\n",
    "    print('Length of Dataset: ', len(dataloader.dataset))\n",
    "\n",
    "    # Try getting an item from the dataset\n",
    "    try:\n",
    "        print('Example item: ', dataloader.dataset[0])\n",
    "    except Exception as e:\n",
    "        print('Could not retrieve item from dataset: ', str(e))\n",
    "    return\n",
    "\n",
    "examine_data(train_texts)\n",
    "examine_dataloader(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
